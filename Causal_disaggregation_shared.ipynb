{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Exp. 1:\n",
        "Political Campaigning - Estimate how regional campaign spending affects subregional election outcomes when effectiveness depends on subregional wealth levels. Only aggregated regional outcomes are observed, and the goal is to recover the heterogeneous local causal effects."
      ],
      "metadata": {
        "id": "sPWzrHR_ICBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "IjF8IMYv-xp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE6n5HR-H-YR"
      },
      "outputs": [],
      "source": [
        "data_dir = 'data_exp1'\n",
        "num_regions = 100\n",
        "subregions_per_region = 100\n",
        "SPATIAL_VARIANCE = 5\n",
        "noise_variance = 0.01\n",
        "np.random.seed(42)\n",
        "\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# 1) Intervention matrix: 100x100, each row all 0s or all 1s with 50% probability\n",
        "interventions_region_flags = np.random.choice([0, 1], size=num_regions, p=[0.5, 0.5])\n",
        "interventions_sub = np.repeat(interventions_region_flags[:, np.newaxis], subregions_per_region, axis=1)\n",
        "pd.DataFrame(interventions_sub).to_csv(os.path.join(data_dir, 'interventions_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# Region-level interventions: average per row (which is 0 or 1 since all values same)\n",
        "interventions_reg = np.mean(interventions_sub, axis=1)\n",
        "pd.DataFrame(interventions_reg).to_csv(os.path.join(data_dir, 'interventions_region.csv'), index=False, header=False)\n",
        "\n",
        "# 2) Context matrix: 100x100, wealth levels 1,2,3\n",
        "context_sub = np.zeros((num_regions, subregions_per_region), dtype=int)\n",
        "for i in range(num_regions):\n",
        "    if i == 0:\n",
        "        # First row: all 2 (middle class)\n",
        "        context_sub[i, :] = 2\n",
        "    elif i == 1:\n",
        "        # Second row: exactly half 1 (poor) and half 3 (rich)\n",
        "        half = subregions_per_region // 2\n",
        "        arr = np.concatenate([np.ones(half, dtype=int), np.full(subregions_per_region - half, 3, dtype=int)])\n",
        "        np.random.shuffle(arr)\n",
        "        context_sub[i, :] = arr\n",
        "    else:\n",
        "        # For other rows: choose x3 middle class between 20 and 100\n",
        "        x3 = np.random.randint(20, 101)\n",
        "        remaining = subregions_per_region - x3\n",
        "        # Split remaining roughly 50:50 into poor (1) and rich (3), with wiggle room\n",
        "        deviation = np.random.randint(-SPATIAL_VARIANCE, SPATIAL_VARIANCE + 1)\n",
        "        num_poor = max(0, min(remaining, remaining // 2 + deviation))\n",
        "        num_rich = remaining - num_poor\n",
        "        # Create and shuffle array\n",
        "        arr = np.concatenate([np.ones(num_poor, dtype=int), np.full(x3, 2, dtype=int), np.full(num_rich, 3, dtype=int)])\n",
        "        np.random.shuffle(arr)\n",
        "        context_sub[i, :] = arr\n",
        "pd.DataFrame(context_sub).to_csv(os.path.join(data_dir, 'context_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# Region-level context: mean per row\n",
        "context_reg = np.mean(context_sub, axis=1)\n",
        "pd.DataFrame(context_reg).to_csv(os.path.join(data_dir, 'context_region.csv'), index=False, header=False)\n",
        "\n",
        "# 3) Noise matrix: 100x100, iid normal with given variance\n",
        "noise_sub = np.random.normal(0, np.sqrt(noise_variance), size=(num_regions, subregions_per_region))\n",
        "pd.DataFrame(noise_sub).to_csv(os.path.join(data_dir, 'noise_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# 4) Outcome matrix: based on intervention, context, and noise\n",
        "delta = np.zeros((num_regions, subregions_per_region), dtype=float)\n",
        "delta[context_sub == 1] = -0.1\n",
        "delta[context_sub == 2] = 0.0\n",
        "delta[context_sub == 3] = 0.3\n",
        "# Outcome = noise + intervention * delta\n",
        "outcome_sub = noise_sub + interventions_sub * delta\n",
        "pd.DataFrame(outcome_sub).to_csv(os.path.join(data_dir, 'outcome_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# Region-level outcome: mean per row\n",
        "outcome_reg = np.mean(outcome_sub, axis=1)\n",
        "pd.DataFrame(outcome_reg).to_csv(os.path.join(data_dir, 'outcome_region.csv'), index=False, header=False)\n",
        "\n",
        "# 5) Causal effect matrix\n",
        "causal_effect_sub = delta\n",
        "pd.DataFrame(causal_effect_sub).to_csv(os.path.join(data_dir, 'causal_effect_subregion.csv'), index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data_exp1'\n",
        "region_grid_size = 10\n",
        "sub_grid_size = 100\n",
        "sub_per_region_side = 10\n",
        "\n",
        "colors = sns.color_palette(\"deep\")\n",
        "red_cmap = sns.light_palette(colors[3], as_cmap=True)  # Red for interventions\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue for outcomes\n",
        "green_cmap = sns.light_palette(colors[2], as_cmap=True)  # Green for context\n",
        "purple_cmap = sns.light_palette(colors[4], as_cmap=True)  # Purple for causal effects\n",
        "orange_cmap = sns.light_palette(colors[1], as_cmap=True)  # Orange for noise\n",
        "\n",
        "# Function to reshape subregion data (100x100) to 100x100 grid\n",
        "def reshape_to_subgrid(data):\n",
        "    grid = np.zeros((sub_grid_size, sub_grid_size))\n",
        "    for ri in range(region_grid_size):\n",
        "        for rj in range(region_grid_size):\n",
        "            reg_idx = ri * region_grid_size + rj\n",
        "            sub_data = data[reg_idx].reshape(sub_per_region_side, sub_per_region_side)\n",
        "            grid[ri*sub_per_region_side:(ri+1)*sub_per_region_side, rj*sub_per_region_side:(rj+1)*sub_per_region_side] = sub_data\n",
        "    return grid\n",
        "\n",
        "# Function to reshape region data (100,) to 10x10 grid\n",
        "def reshape_to_reggrid(data):\n",
        "    return data.reshape(region_grid_size, region_grid_size)\n",
        "\n",
        "# Independent plotting functions\n",
        "\n",
        "def plot_interventions_subregion():\n",
        "    file_path = os.path.join(data_dir, 'interventions_subregion.csv')\n",
        "    data = pd.read_csv(file_path, header=None).values\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    sns.heatmap(grid, ax=ax, cmap=red_cmap, square=True, linewidths=0, cbar=True, cbar_kws={'location': 'right'})\n",
        "    ax.set_title('Interventions Subregion', fontsize=20)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=14)\n",
        "    fig.savefig(os.path.join(data_dir, 'interventions_subregion.jpg'), dpi=300, bbox_inches='tight')\n",
        "    fig.savefig(os.path.join(data_dir, 'interventions_subregion.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_interventions_region():\n",
        "    file_path = os.path.join(data_dir, 'interventions_region.csv')\n",
        "    data = pd.read_csv(file_path, header=None).values.squeeze()\n",
        "    grid = reshape_to_reggrid(data)\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    sns.heatmap(grid, ax=ax, cmap=red_cmap, square=True, linewidths=1, cbar=True, cbar_kws={'location': 'right'})\n",
        "    ax.set_title('Interventions Region', fontsize=20)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=14)\n",
        "    fig.savefig(os.path.join(data_dir, 'interventions_region.jpg'), dpi=300, bbox_inches='tight')\n",
        "    fig.savefig(os.path.join(data_dir, 'interventions_region.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_context_subregion():\n",
        "    file_path = os.path.join(data_dir, 'context_subregion.csv')\n",
        "    data = pd.read_csv(file_path, header=None).values\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    sns.heatmap(grid, ax=ax, cmap=green_cmap, square=True, linewidths=0, cbar=True, cbar_kws={'location': 'right'}, vmin=1, vmax=3)\n",
        "    ax.set_title('Context Subregion', fontsize=20)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=14)\n",
        "    fig.savefig(os.path.join(data_dir, 'context_subregion.jpg'), dpi=300, bbox_inches='tight')\n",
        "    fig.savefig(os.path.join(data_dir, 'context_subregion.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_context_region():\n",
        "    file_path = os.path.join(data_dir, 'context_region.csv')\n",
        "    data = pd.read_csv(file_path, header=None).values.squeeze()\n",
        "    grid = reshape_to_reggrid(data)\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    sns.heatmap(grid, ax=ax, cmap=green_cmap, square=True, linewidths=1, cbar=True, cbar_kws={'location': 'right'}, vmin=1.8, vmax=2.2)\n",
        "    ax.set_title('Context Region', fontsize=20)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=14)\n",
        "    fig.savefig(os.path.join(data_dir, 'context_region.jpg'), dpi=300, bbox_inches='tight')\n",
        "    fig.savefig(os.path.join(data_dir, 'context_region.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_noise_subregion():\n",
        "    file_path = os.path.join(data_dir, 'noise_subregion.csv')\n",
        "    data = pd.read_csv(file_path, header=None).values\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    sns.heatmap(grid, ax=ax, cmap=orange_cmap, square=True, linewidths=0, cbar=True, cbar_kws={'location': 'right'})\n",
        "    ax.set_title('Noise Subregion', fontsize=20)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=14)\n",
        "    fig.savefig(os.path.join(data_dir, 'noise_subregion.jpg'), dpi=300, bbox_inches='tight')\n",
        "    fig.savefig(os.path.join(data_dir, 'noise_subregion.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_outcome_subregion():\n",
        "    file_path = os.path.join(data_dir, 'outcome_subregion.csv')\n",
        "    data = pd.read_csv(file_path, header=None).values\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    sns.heatmap(grid, ax=ax, cmap=blue_cmap, square=True, linewidths=0, cbar=True, cbar_kws={'location': 'right'}, vmin=-0.1, vmax=0.5)\n",
        "    ax.set_title('Outcome Subregion', fontsize=20)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=14)\n",
        "    fig.savefig(os.path.join(data_dir, 'outcome_subregion.jpg'), dpi=300, bbox_inches='tight')\n",
        "    fig.savefig(os.path.join(data_dir, 'outcome_subregion.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_outcome_region():\n",
        "    file_path = os.path.join(data_dir, 'outcome_region.csv')\n",
        "    data = pd.read_csv(file_path, header=None).values.squeeze()\n",
        "    grid = reshape_to_reggrid(data)\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    sns.heatmap(grid, ax=ax, cmap=blue_cmap, square=True, linewidths=1, cbar=True, cbar_kws={'location': 'right'}, vmin=-0.05, vmax=0.1)\n",
        "    ax.set_title('Outcome Region', fontsize=20)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=14)\n",
        "    fig.savefig(os.path.join(data_dir, 'outcome_region.jpg'), dpi=300, bbox_inches='tight')\n",
        "    fig.savefig(os.path.join(data_dir, 'outcome_region.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_causal_effect_subregion():\n",
        "    file_path = os.path.join(data_dir, 'causal_effect_subregion.csv')\n",
        "    data = pd.read_csv(file_path, header=None).values\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    sns.heatmap(grid, ax=ax, cmap=purple_cmap, square=True, linewidths=0, cbar=True, cbar_kws={'location': 'right'}, vmin=-0.1, vmax=0.3)\n",
        "    ax.set_title('Causal Effect Subregion', fontsize=20)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=14)\n",
        "    fig.savefig(os.path.join(data_dir, 'causal_effect_subregion.jpg'), dpi=300, bbox_inches='tight')\n",
        "    fig.savefig(os.path.join(data_dir, 'causal_effect_subregion.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Call all plotting functions to generate and display all plots\n",
        "plot_interventions_subregion()\n",
        "plot_interventions_region()\n",
        "plot_context_subregion()\n",
        "plot_context_region()\n",
        "plot_noise_subregion()\n",
        "plot_outcome_subregion()\n",
        "plot_outcome_region()\n",
        "plot_causal_effect_subregion()"
      ],
      "metadata": {
        "id": "MZ3Rm604-1Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "data_dir = 'data_exp1'\n",
        "region_grid_size = 10\n",
        "sub_grid_size = 100\n",
        "sub_per_region_side = 10\n",
        "\n",
        "# Color definitions\n",
        "colors = sns.color_palette(\"deep\")\n",
        "red_cmap = sns.light_palette(colors[3], as_cmap=True)  # Red for interventions\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue for outcomes\n",
        "green_cmap = sns.light_palette(colors[2], as_cmap=True)  # Green for context\n",
        "\n",
        "# Function to reshape region data (100,) to 10x10 grid\n",
        "def reshape_to_reggrid(data):\n",
        "    return data.reshape(region_grid_size, region_grid_size)\n",
        "\n",
        "# Function to reshape subregion data (100x100) to 100x100 grid\n",
        "def reshape_to_subgrid(data):\n",
        "    grid = np.zeros((sub_grid_size, sub_grid_size))\n",
        "    for ri in range(region_grid_size):\n",
        "        for rj in range(region_grid_size):\n",
        "            reg_idx = ri * region_grid_size + rj\n",
        "            sub_data = data[reg_idx].reshape(sub_per_region_side, sub_per_region_side)\n",
        "            grid[ri*sub_per_region_side:(ri+1)*sub_per_region_side, rj*sub_per_region_side:(rj+1)*sub_per_region_side] = sub_data\n",
        "    return grid\n",
        "\n",
        "# Load data\n",
        "interv_data = pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.squeeze()\n",
        "outcome_data = pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.squeeze()\n",
        "context_data = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values\n",
        "context_data_norm = (context_data - 1) / 2.0  # Normalize 1-3 to 0-1\n",
        "\n",
        "# Parameter to control space between subplots\n",
        "space_between = 0.05\n",
        "\n",
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Left: Interventions in red\n",
        "sns.heatmap(reshape_to_reggrid(interv_data), ax=axs[0], cmap=red_cmap, cbar=False, square=True, linewidths=2, vmin=0.0, vmax=1.0)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "# Center: Outcomes in blue\n",
        "sns.heatmap(reshape_to_reggrid(outcome_data), ax=axs[1], cmap=blue_cmap, cbar=False, square=True, linewidths=2, vmin=-0.05, vmax=0.1)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "# Right: Context in green\n",
        "sns.heatmap(reshape_to_subgrid(context_data_norm), ax=axs[2], cmap=green_cmap, cbar=False, square=True, linewidths=0, vmin=0.0, vmax=1.0)\n",
        "axs[2].set_xticks([])\n",
        "axs[2].set_yticks([])\n",
        "\n",
        "# Adjust the space between subplots\n",
        "fig.subplots_adjust(wspace=space_between)\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(os.path.join(data_dir, 'combined_heatmaps.jpg'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(os.path.join(data_dir, 'combined_heatmaps.pdf'), bbox_inches='tight', pad_inches=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U3sgZgfn_Cyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "data_dir = 'data_exp1'\n",
        "\n",
        "# Color definitions\n",
        "colors = sns.color_palette(\"deep\")\n",
        "blue_color = colors[0]  # Blue\n",
        "red_color = colors[3]   # Red\n",
        "\n",
        "# Font size parameters\n",
        "label_fontsize = 20\n",
        "tick_fontsize = 18\n",
        "legend_fontsize = 18\n",
        "\n",
        "# Load region-level data\n",
        "context_reg = pd.read_csv(os.path.join(data_dir, 'context_region.csv'), header=None).values.squeeze()\n",
        "outcome_reg = pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.squeeze()\n",
        "interv_reg = pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.squeeze()\n",
        "\n",
        "# 1) Scatter plot: aggregated wealth vs. outcome, colored by intervention with different markers\n",
        "fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
        "sns.scatterplot(x=context_reg[interv_reg == 0], y=outcome_reg[interv_reg == 0], color=blue_color, alpha=0.6, s=150, label='Control', marker='o', ax=ax1)\n",
        "sns.scatterplot(x=context_reg[interv_reg == 1], y=outcome_reg[interv_reg == 1], color=red_color, alpha=0.6, s=150, label='Intervened', marker='^', ax=ax1)\n",
        "ax1.set_xlabel('Regional Wealth', fontsize=label_fontsize)\n",
        "ax1.set_ylabel('Outcome', fontsize=label_fontsize)\n",
        "ax1.tick_params(labelsize=tick_fontsize, length=0)\n",
        "ax1.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False, fontsize=legend_fontsize)\n",
        "plt.tight_layout()\n",
        "fig1.savefig(os.path.join(data_dir, 'scatter_wealth_outcome.jpg'), dpi=300, bbox_inches='tight')\n",
        "fig1.savefig(os.path.join(data_dir, 'scatter_wealth_outcome.pdf'), bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Load subregion-level data\n",
        "context_sub = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values.flatten()\n",
        "outcome_sub = pd.read_csv(os.path.join(data_dir, 'outcome_subregion.csv'), header=None).values.flatten()\n",
        "interv_sub = pd.read_csv(os.path.join(data_dir, 'interventions_subregion.csv'), header=None).values.flatten()\n",
        "\n",
        "# Create DataFrame for violin plot\n",
        "df = pd.DataFrame({\n",
        "    'Wealth': context_sub,\n",
        "    'Intervention': interv_sub,\n",
        "    'Outcome': outcome_sub\n",
        "})\n",
        "\n",
        "# 2) Violin plot: distribution of outcome by wealth and intervention\n",
        "fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
        "sns.violinplot(data=df, x='Wealth', y='Outcome', hue='Intervention', palette={0: blue_color, 1: red_color}, split=False, inner='quartile', ax=ax2)\n",
        "ax2.set_xlabel('Subregional Wealth', fontsize=label_fontsize)\n",
        "ax2.set_ylabel('Outcome', fontsize=label_fontsize)\n",
        "ax2.tick_params(labelsize=tick_fontsize, length=0)\n",
        "handles, labels = ax2.get_legend_handles_labels()\n",
        "ax2.legend(handles, ['Control', 'Intervened'], loc='upper left', bbox_to_anchor=(1, 1), frameon=False, fontsize=legend_fontsize)\n",
        "plt.tight_layout()\n",
        "fig2.savefig(os.path.join(data_dir, 'violin_wealth_intervention_outcome.jpg'), dpi=300, bbox_inches='tight')\n",
        "fig2.savefig(os.path.join(data_dir, 'violin_wealth_intervention_outcome.pdf'), bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZAhR6ulD_Gl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_to_subgrid(data):\n",
        "    grid = np.zeros((sub_grid_size, sub_grid_size))\n",
        "    for ri in range(region_grid_size):\n",
        "        for rj in range(region_grid_size):\n",
        "            reg_idx = ri * region_grid_size + rj\n",
        "            sub_data = data[reg_idx].reshape(sub_per_region_side, sub_per_region_side)\n",
        "            grid[ri * sub_per_region_side:(ri + 1) * sub_per_region_side, rj * sub_per_region_side:(rj + 1) * sub_per_region_side] = sub_data\n",
        "    return grid\n",
        "\n",
        "def estimate_current_subregionaloutcome(model, context_sub_t, interventions_reg_t):\n",
        "    _, outcome_sub_est_t = model(interventions_reg_t, context_sub_t)\n",
        "    return outcome_sub_est_t\n",
        "\n",
        "def estimate_current_causaleffectmatrix(model, context_sub_t):\n",
        "    causal_effect_sub_t = torch.zeros_like(context_sub_t, dtype=torch.float32)\n",
        "    causal_effect_sub_t[context_sub_t == 1] = model.theta[3] - model.theta[0]\n",
        "    causal_effect_sub_t[context_sub_t == 2] = model.theta[4] - model.theta[1]\n",
        "    causal_effect_sub_t[context_sub_t == 3] = model.theta[5] - model.theta[2]\n",
        "    return causal_effect_sub_t\n",
        "\n",
        "# Define important parameters\n",
        "data_dir = 'data_exp1'\n",
        "num_regions = 100\n",
        "subregions_per_region = 100\n",
        "n_epochs = 1000\n",
        "lr = 0.1\n",
        "seed = 42\n",
        "loglog_flag = True\n",
        "device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Load data\n",
        "interventions_reg = pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.squeeze() # (100,)\n",
        "context_sub = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values # (100, 100), wealth 1,2,3\n",
        "outcome_reg = pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.squeeze() # (100,)\n",
        "\n",
        "# Load ground truth causal effect\n",
        "causal_effect_true = pd.read_csv(os.path.join(data_dir, 'causal_effect_subregion.csv'), header=None).values  # (100, 100)\n",
        "\n",
        "# Convert to tensors\n",
        "interventions_reg_t = torch.tensor(interventions_reg, dtype=torch.long, device=device) # (100,), 0 or 1\n",
        "context_sub_t = torch.tensor(context_sub, dtype=torch.long, device=device) # (100, 100), 1,2,3\n",
        "outcome_reg_t = torch.tensor(outcome_reg, dtype=torch.float32, device=device) # (100,)\n",
        "\n",
        "# Define PyTorch module\n",
        "class OutcomeModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OutcomeModel, self).__init__()\n",
        "        self.theta = nn.Parameter(torch.ones(6))\n",
        "\n",
        "    def forward(self, interventions_reg_t, context_sub_t):\n",
        "        # Explicitly compute f_theta(t_{i,j}, c_{i,j})\n",
        "        pred_sub = torch.zeros_like(context_sub_t, dtype=torch.float32)\n",
        "        theta = self.theta\n",
        "        # if not self.eval():\n",
        "        # theta = theta + torch.randn_like(self.theta) * 0.01 # Add noise to theta\n",
        "        # Masks for intervention\n",
        "        mask_t0 = (interventions_reg_t[:, None] == 0)\n",
        "        mask_t1 = (interventions_reg_t[:, None] == 1)\n",
        "        # For t=0\n",
        "        pred_sub[mask_t0 & (context_sub_t == 1)] = theta[0] # theta1: t=0, c=1 (poor)\n",
        "        pred_sub[mask_t0 & (context_sub_t == 2)] = theta[1] # theta2: t=0, c=2 (middle)\n",
        "        pred_sub[mask_t0 & (context_sub_t == 3)] = theta[2] # theta3: t=0, c=3 (rich)\n",
        "        # For t=1\n",
        "        pred_sub[mask_t1 & (context_sub_t == 1)] = theta[3] # theta4: t=1, c=1 (poor)\n",
        "        pred_sub[mask_t1 & (context_sub_t == 2)] = theta[4] # theta5: t=1, c=2 (middle)\n",
        "        pred_sub[mask_t1 & (context_sub_t == 3)] = theta[5] # theta6: t=1, c=3 (rich)\n",
        "        # Aggregate mean per region: (100,)\n",
        "        pred_reg = pred_sub.mean(dim=1)\n",
        "        return pred_reg, pred_sub # Return pred_sub for later use\n",
        "\n",
        "# Instantiate model and optimizer\n",
        "model = OutcomeModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Loss history\n",
        "loss_hist = []\n",
        "ce_loss_hist = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    pred_reg, _ = model(interventions_reg_t, context_sub_t)\n",
        "    # MSE loss\n",
        "    loss = torch.mean((pred_reg - outcome_reg_t) ** 2)\n",
        "    # Backward and optimize\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_hist.append(loss.item())\n",
        "\n",
        "    # Compute MSE for causal effect\n",
        "    causal_effect_est_t = estimate_current_causaleffectmatrix(model, context_sub_t)\n",
        "    ce_mse = np.mean((causal_effect_est_t.detach().cpu().numpy() - causal_effect_true) ** 2)\n",
        "    ce_loss_hist.append(ce_mse)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Save and plot loss curves\n",
        "pd.DataFrame({'loss': loss_hist, 'ce_loss': ce_loss_hist}).to_csv(os.path.join(data_dir, 'estimation_loss.csv'), index=False)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
        "epochs = np.arange(1, len(loss_hist) + 1)\n",
        "\n",
        "if loglog_flag:\n",
        "    axs[0].loglog(epochs, loss_hist)\n",
        "    axs[1].loglog(epochs, ce_loss_hist)\n",
        "else:\n",
        "    axs[0].plot(epochs, loss_hist)\n",
        "    axs[1].plot(epochs, ce_loss_hist)\n",
        "\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('MSE Loss (Region Outcomes)')\n",
        "axs[0].set_title('Training Loss Curve')\n",
        "\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('MSE Loss (Causal Effects)')\n",
        "axs[1].set_title('Causal Effect Estimation Error')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(os.path.join(data_dir, 'estimation_losses.jpg'), dpi=300)\n",
        "fig.savefig(os.path.join(data_dir, 'estimation_losses.pdf'))\n",
        "plt.show()\n",
        "\n",
        "# Print final theta estimates\n",
        "print(\"Final theta estimates:\")\n",
        "print(\"theta1 (t=0, poor): \", model.theta[0].detach().cpu().numpy())\n",
        "print(\"theta2 (t=0, middle class): \", model.theta[1].detach().cpu().numpy())\n",
        "print(\"theta3 (t=0, rich): \", model.theta[2].detach().cpu().numpy())\n",
        "print(\"theta4 (t=1, poor): \", model.theta[3].detach().cpu().numpy())\n",
        "print(\"theta5 (t=1, middle class): \", model.theta[4].detach().cpu().numpy())\n",
        "print(\"theta6 (t=1, rich): \", model.theta[5].detach().cpu().numpy())\n",
        "\n",
        "# Save theta to text file\n",
        "with open(os.path.join(data_dir, 'parameter_estimate.txt'), 'w') as f:\n",
        "    f.write(\"theta1,theta2,theta3,theta4,theta5,theta6\\n\")\n",
        "    f.write(','.join(map(str, model.theta.detach().cpu().numpy().flatten())) + '\\n')\n",
        "\n",
        "# Compute deterministic subregion outcomes (no noise)\n",
        "with torch.no_grad():\n",
        "    outcome_sub_est_t = estimate_current_subregionaloutcome(model, context_sub_t, interventions_reg_t)\n",
        "outcome_sub_est = outcome_sub_est_t.cpu().numpy()\n",
        "pd.DataFrame(outcome_sub_est).to_csv(os.path.join(data_dir, 'outcome_subregion_estimated.csv'), index=False, header=False)\n",
        "\n",
        "# Visualize estimated subregion outcome\n",
        "region_grid_size = 10\n",
        "sub_grid_size = 100\n",
        "sub_per_region_side = 10\n",
        "\n",
        "# Color definitions\n",
        "colors = sns.color_palette(\"deep\")\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True) # Blue for outcomes\n",
        "\n",
        "grid = reshape_to_subgrid(outcome_sub_est)\n",
        "fig_vis, ax_vis = plt.subplots(figsize=(10, 10))\n",
        "sns.heatmap(grid, ax=ax_vis, cmap=blue_cmap, square=True, linewidths=0, cbar=True, cbar_kws={'location': 'right'}, vmin=-0.1, vmax=0.5)\n",
        "ax_vis.set_title('Estimated Outcome Subregion', fontsize=20)\n",
        "ax_vis.set_xticks([])\n",
        "ax_vis.set_yticks([])\n",
        "cbar = ax_vis.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=14)\n",
        "fig_vis.savefig(os.path.join(data_dir, 'outcome_subregion_estimated.jpg'), dpi=300, bbox_inches='tight')\n",
        "fig_vis.savefig(os.path.join(data_dir, 'outcome_subregion_estimated.pdf'), bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Compute causal effect subregion: f(1, c_ij) - f(0, c_ij)\n",
        "causal_effect_sub_t = estimate_current_causaleffectmatrix(model, context_sub_t)\n",
        "causal_effect_sub = causal_effect_sub_t.detach().cpu().numpy()\n",
        "pd.DataFrame(causal_effect_sub).to_csv(os.path.join(data_dir, 'causal_effect_subregion_estimated.csv'), index=False, header=False)\n",
        "\n",
        "# Visualize estimated causal effect subregion\n",
        "purple_cmap = sns.light_palette(colors[4], as_cmap=True) # Purple for causal effects\n",
        "grid_ce = reshape_to_subgrid(causal_effect_sub)\n",
        "fig_ce, ax_ce = plt.subplots(figsize=(10, 10))\n",
        "sns.heatmap(grid_ce, ax=ax_ce, cmap=purple_cmap, square=True, linewidths=0, cbar=True, cbar_kws={'location': 'right'}, vmin=-0.1, vmax=0.3)\n",
        "ax_ce.set_title('Estimated Causal Effect Subregion', fontsize=20)\n",
        "ax_ce.set_xticks([])\n",
        "ax_ce.set_yticks([])\n",
        "cbar_ce = ax_ce.collections[0].colorbar\n",
        "cbar_ce.ax.tick_params(labelsize=14)\n",
        "fig_ce.savefig(os.path.join(data_dir, 'causal_effect_subregion_estimated.jpg'), dpi=300, bbox_inches='tight')\n",
        "fig_ce.savefig(os.path.join(data_dir, 'causal_effect_subregion_estimated.pdf'), bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HOROUfnv_LZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and plot loss curve\n",
        "data_dir = 'data_exp1'\n",
        "loglog_flag = True\n",
        "\n",
        "# Load loss history from CSV\n",
        "loss_data = pd.read_csv(os.path.join(data_dir, 'estimation_loss.csv'))\n",
        "loss_hist = loss_data['loss'].values\n",
        "ce_loss_hist = loss_data['ce_loss'].values\n",
        "epochs = np.arange(1, len(loss_hist) + 1)\n",
        "\n",
        "# Plotting\n",
        "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "colors = sns.color_palette(\"deep\")\n",
        "\n",
        "ax1.set_xlabel('Epoch', fontsize=20)\n",
        "ax1.set_ylabel('Training Loss', fontsize=20, color=colors[0])\n",
        "if loglog_flag:\n",
        "    ax1.loglog(epochs, loss_hist, color=colors[0], alpha=0.9, lw=3)\n",
        "else:\n",
        "    ax1.plot(epochs, loss_hist, color=colors[0], alpha=0.7, lw=2)\n",
        "ax1.tick_params(axis='both', which='both', length=0, labelsize=18, colors=colors[0])\n",
        "ax1.set_title('MSE Curves', fontsize=22)\n",
        "ax1.spines['top'].set_visible(False)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('MSE Causal Effect', fontsize=20, color=colors[3])\n",
        "if loglog_flag:\n",
        "    ax2.loglog(epochs, ce_loss_hist, color=colors[3], alpha=0.9, ls='--', lw=5)\n",
        "else:\n",
        "    ax2.plot(epochs, ce_loss_hist, color=colors[3], alpha=0.7, ls='--', lw=2)\n",
        "ax2.tick_params(axis='y', which='both', length=0, labelsize=18, colors=colors[3])\n",
        "ax2.spines['top'].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(os.path.join(data_dir, 'estimation_loss.jpg'), dpi=300)\n",
        "fig.savefig(os.path.join(data_dir, 'estimation_loss.pdf'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "elJC0TWZ_jOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define important parameters\n",
        "data_dir = 'data_exp1'\n",
        "num_regions = 100\n",
        "region_grid_size = 10\n",
        "\n",
        "# Load original region-level interventions and invert second half\n",
        "interventions_reg = pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.squeeze()  # (100,)\n",
        "interventions_reg_cf = interventions_reg.copy()\n",
        "interventions_reg_cf[num_regions // 2:] = 1 - interventions_reg_cf[num_regions // 2:]\n",
        "pd.DataFrame(interventions_reg_cf).to_csv(os.path.join(data_dir, 'interventions_region_cf.csv'), index=False, header=False)\n",
        "\n",
        "# Load context, noise\n",
        "context_sub = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values  # (100, 100)\n",
        "noise_sub = pd.read_csv(os.path.join(data_dir, 'noise_subregion.csv'), header=None).values  # (100, 100)\n",
        "\n",
        "# Create subregion-level counterfactual interventions (all 0s or 1s per row)\n",
        "interventions_sub_cf = np.repeat(interventions_reg_cf[:, np.newaxis], context_sub.shape[1], axis=1)  # (100, 100)\n",
        "\n",
        "# Ground truth delta based on context: -0.1 for 1, 0 for 2, 0.3 for 3\n",
        "delta = np.zeros_like(context_sub, dtype=float)\n",
        "delta[context_sub == 1] = -0.1\n",
        "delta[context_sub == 2] = 0.0\n",
        "delta[context_sub == 3] = 0.3\n",
        "\n",
        "# Compute counterfactual outcome subregion\n",
        "outcome_sub_cf = noise_sub + interventions_sub_cf * delta\n",
        "pd.DataFrame(outcome_sub_cf).to_csv(os.path.join(data_dir, 'outcome_subregion_cf.csv'), index=False, header=False)\n",
        "\n",
        "# Aggregate mean per row for region-level\n",
        "outcome_reg_cf = np.mean(outcome_sub_cf, axis=1)\n",
        "pd.DataFrame(outcome_reg_cf).to_csv(os.path.join(data_dir, 'outcome_region_cf.csv'), index=False, header=False)\n",
        "\n",
        "# Visualize region-level counterfactual outcome\n",
        "colors = sns.color_palette(\"deep\")\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)\n",
        "\n",
        "def reshape_to_reggrid(data):\n",
        "    return data.reshape(region_grid_size, region_grid_size)\n",
        "\n",
        "grid_cf = reshape_to_reggrid(outcome_reg_cf)\n",
        "fig_cf, ax_cf = plt.subplots(figsize=(5, 5))\n",
        "sns.heatmap(grid_cf, ax=ax_cf, cmap=blue_cmap, square=True, linewidths=1, cbar=True, cbar_kws={'location': 'right'}, vmin=-0.05, vmax=0.1)\n",
        "ax_cf.set_title('CF Outcome GT', fontsize=20)\n",
        "ax_cf.set_xticks([])\n",
        "ax_cf.set_yticks([])\n",
        "cbar = ax_cf.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=14)\n",
        "fig_cf.savefig(os.path.join(data_dir, 'outcome_region_cf.jpg'), dpi=300, bbox_inches='tight')\n",
        "fig_cf.savefig(os.path.join(data_dir, 'outcome_region_cf.pdf'), bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HihqWfdJ_oGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_interventions_region_cf():\n",
        "    file_path = os.path.join(data_dir, 'interventions_region_cf.csv')\n",
        "    data = pd.read_csv(file_path, header=None).values.squeeze()\n",
        "    grid = reshape_to_reggrid(data)\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    sns.heatmap(grid, ax=ax, cmap=red_cmap, square=True, linewidths=1, cbar=True, cbar_kws={'location': 'right'})\n",
        "    ax.set_title('CF Interventions', fontsize=20)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=14)\n",
        "    fig.savefig(os.path.join(data_dir, 'interventions_region_cf.jpg'), dpi=300, bbox_inches='tight')\n",
        "    fig.savefig(os.path.join(data_dir, 'interventions_region_cf.pdf'), bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_interventions_region_cf()"
      ],
      "metadata": {
        "id": "ntVC9746_pxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define important parameters\n",
        "data_dir = 'data_exp1'\n",
        "num_regions = 100\n",
        "subregions_per_region = 100\n",
        "region_grid_size = 10\n",
        "sub_grid_size = 100\n",
        "sub_per_region_side = 10\n",
        "device = 'cpu'\n",
        "\n",
        "# Load data\n",
        "interventions_reg_cf = pd.read_csv(os.path.join(data_dir, 'interventions_region_cf.csv'), header=None).values.squeeze()  # (100,)\n",
        "interventions_reg = pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.squeeze()  # (100,)\n",
        "context_sub = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values  # (100, 100)\n",
        "outcome_reg = pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.squeeze()  # (100,)\n",
        "\n",
        "# Load inferred theta\n",
        "with open(os.path.join(data_dir, 'parameter_estimate.txt'), 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    theta_values = list(map(float, lines[1].strip().split(',')))\n",
        "theta = np.array(theta_values)  # [theta1, theta2, theta3, theta4, theta5, theta6]\n",
        "\n",
        "# Convert to tensors\n",
        "interventions_reg_t = torch.tensor(interventions_reg, dtype=torch.long, device=device)\n",
        "interventions_reg_cf_t = torch.tensor(interventions_reg_cf, dtype=torch.long, device=device)\n",
        "context_sub_t = torch.tensor(context_sub, dtype=torch.long, device=device)\n",
        "\n",
        "# Function to compute subregional predictions with theta\n",
        "def compute_pred_sub(interventions_t, context_t, theta):\n",
        "    pred_sub = torch.zeros_like(context_t, dtype=torch.float32)\n",
        "    mask_t0 = (interventions_t[:, None] == 0)\n",
        "    mask_t1 = (interventions_t[:, None] == 1)\n",
        "    pred_sub[mask_t0 & (context_t == 1)] = theta[0]\n",
        "    pred_sub[mask_t0 & (context_t == 2)] = theta[1]\n",
        "    pred_sub[mask_t0 & (context_t == 3)] = theta[2]\n",
        "    pred_sub[mask_t1 & (context_t == 1)] = theta[3]\n",
        "    pred_sub[mask_t1 & (context_t == 2)] = theta[4]\n",
        "    pred_sub[mask_t1 & (context_t == 3)] = theta[5]\n",
        "    return pred_sub\n",
        "\n",
        "# 1) Estimate regional noise: residual = actual_reg - pred_reg_original (deterministic mean)\n",
        "pred_sub_original = compute_pred_sub(interventions_reg_t, context_sub_t, theta)\n",
        "pred_reg_original = pred_sub_original.mean(dim=1).numpy()\n",
        "regional_noise = outcome_reg - pred_reg_original  # (100,)\n",
        "\n",
        "# 2) Compute CF subregional deterministic\n",
        "pred_sub_cf = compute_pred_sub(interventions_reg_cf_t, context_sub_t, theta)\n",
        "pd.DataFrame(pred_sub_cf.numpy()).to_csv(os.path.join(data_dir, 'outcome_subregion_cf_estimated.csv'), index=False, header=False)\n",
        "\n",
        "# Aggregate to regional deterministic, add noise\n",
        "pred_reg_cf_deterministic = pred_sub_cf.mean(dim=1).numpy()\n",
        "outcome_reg_cf = pred_reg_cf_deterministic + regional_noise\n",
        "pd.DataFrame(outcome_reg_cf).to_csv(os.path.join(data_dir, 'outcome_region_cf_estimated.csv'), index=False, header=False)\n",
        "\n",
        "# 3) Visualize CF regional outcome\n",
        "colors = sns.color_palette(\"deep\")\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)\n",
        "\n",
        "def reshape_to_reggrid(data):\n",
        "    return data.reshape(region_grid_size, region_grid_size)\n",
        "\n",
        "grid_cf = reshape_to_reggrid(outcome_reg_cf)\n",
        "fig_cf, ax_cf = plt.subplots(figsize=(5, 5))\n",
        "sns.heatmap(grid_cf, ax=ax_cf, cmap=blue_cmap, square=True, linewidths=1, cbar=True, cbar_kws={'location': 'right'}, vmin=-0.05, vmax=0.1)\n",
        "ax_cf.set_title('CF Outcome Estimate', fontsize=20)\n",
        "ax_cf.set_xticks([])\n",
        "ax_cf.set_yticks([])\n",
        "cbar = ax_cf.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=14)\n",
        "fig_cf.savefig(os.path.join(data_dir, 'outcome_region_cf_estimated.jpg'), dpi=300, bbox_inches='tight')\n",
        "fig_cf.savefig(os.path.join(data_dir, 'outcome_region_cf_estimated.pdf'), bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jEh3c2TB_r-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation for Exp 1.\n",
        "Testing how regional (mean) context variation and subregional context variation after recovery of the underlying causal mechanism."
      ],
      "metadata": {
        "id": "GRfSSq6BYOJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "OIAioh6RYVqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CONFIG ====\n",
        "GRID = 10              # number of regions per side\n",
        "SUB = 10               # subregions per region per side\n",
        "NUM_REGIONS = GRID * GRID\n",
        "NUM_SUBREGIONS = SUB * SUB\n",
        "OUTPUT_DIR = \"data_ablation_exp1\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Full Simulation Generator ---\n",
        "def generate_full_simulation(regional_std, inter_std, seed=42, outfolder=\"data_ablation_exp1\"):\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Generate a unique folder for each setting\n",
        "    setting_folder = os.path.join(outfolder, f\"inter{inter_std:.2f}_regional{regional_std:.2f}_sample{seed}\")\n",
        "    os.makedirs(setting_folder, exist_ok=True)\n",
        "\n",
        "    # --- Fixed parameters ---\n",
        "    GRID_SIZE, SUBREGION_SIZE = 10, 10\n",
        "    RICH_VOTE, POOR_VOTE, INTER_VOTE, BASELINE_VOTE = 0.80, 0.40, 0.50, 0.50\n",
        "\n",
        "    # Map std hyperparams to data gen control\n",
        "    spatial_variance = int(np.ceil(regional_std * 10))   # controls wiggle in counts\n",
        "    outcome_noise = inter_std                            # subregion-level noise\n",
        "\n",
        "    # --- 1) interventions ---\n",
        "    interventions = rng.integers(0, 2, size=(GRID_SIZE, GRID_SIZE))\n",
        "    interventions[0, 0] = 1\n",
        "    interventions[0, 1] = 1\n",
        "    pd.DataFrame(interventions).to_csv(os.path.join(setting_folder, \"interventions.csv\"), header=False, index=False)\n",
        "\n",
        "    # --- 2) wealth grid generation ---\n",
        "    wealth_hi = np.zeros((GRID_SIZE*SUBREGION_SIZE, GRID_SIZE*SUBREGION_SIZE), dtype=int)\n",
        "    for i in range(GRID_SIZE):\n",
        "        for j in range(GRID_SIZE):\n",
        "            n_inter = rng.integers(20, 101)\n",
        "            remaining = 100 - n_inter\n",
        "            wiggle = min(spatial_variance, remaining // 2)\n",
        "            n_rich = np.clip(remaining // 2 + rng.integers(-wiggle, wiggle+1), 0, remaining)\n",
        "            n_poor = remaining - n_rich\n",
        "\n",
        "            if (i, j) == (0, 0):\n",
        "                block_vals = np.full(100, 2)  # all intermediate\n",
        "            elif (i, j) == (0, 1):\n",
        "                half = 50\n",
        "                block_vals = np.concatenate([np.ones(half, int), np.full(half, 3, int)])\n",
        "                rng.shuffle(block_vals)\n",
        "            else:\n",
        "                block_vals = np.concatenate([\n",
        "                    np.ones(n_poor, int),\n",
        "                    np.full(n_inter, 2, int),\n",
        "                    np.full(n_rich, 3, int)\n",
        "                ])\n",
        "                rng.shuffle(block_vals)\n",
        "\n",
        "            r0, c0 = i*SUBREGION_SIZE, j*SUBREGION_SIZE\n",
        "            wealth_hi[r0:r0+SUBREGION_SIZE, c0:c0+SUBREGION_SIZE] = \\\n",
        "                block_vals.reshape(SUBREGION_SIZE, SUBREGION_SIZE)\n",
        "\n",
        "    pd.DataFrame(wealth_hi).to_csv(os.path.join(setting_folder, \"wealth_high_res.csv\"), header=False, index=False)\n",
        "\n",
        "    # --- 3) outcome generation ---\n",
        "    subregion_noise = np.zeros_like(wealth_hi, dtype=float)\n",
        "    outcome = np.zeros_like(interventions, dtype=float)\n",
        "\n",
        "    def vote(w_class, treated, rr, cc):\n",
        "        noise = rng.normal(0, outcome_noise)\n",
        "        subregion_noise[rr, cc] = noise\n",
        "        if not treated:\n",
        "            return np.clip(BASELINE_VOTE + noise, 0, 1)\n",
        "        base = RICH_VOTE if w_class == 3 else POOR_VOTE if w_class == 1 else INTER_VOTE\n",
        "        return np.clip(base + noise, 0, 1)\n",
        "\n",
        "    for i in range(GRID_SIZE):\n",
        "        for j in range(GRID_SIZE):\n",
        "            r0, c0 = i*SUBREGION_SIZE, j*SUBREGION_SIZE\n",
        "            block_wealth = wealth_hi[r0:r0+SUBREGION_SIZE, c0:c0+SUBREGION_SIZE]\n",
        "            treated = bool(interventions[i, j])\n",
        "            votes = [[vote(block_wealth[r, c], treated, r0 + r, c0 + c)\n",
        "                      for c in range(SUBREGION_SIZE)]\n",
        "                      for r in range(SUBREGION_SIZE)]\n",
        "            outcome[i, j] = np.mean(votes) * 100\n",
        "\n",
        "    pd.DataFrame(np.round(subregion_noise, 4)).to_csv(os.path.join(setting_folder, \"subregion_noise_gt.csv\"), header=False, index=False)\n",
        "    region_noise = subregion_noise.reshape(GRID_SIZE, SUBREGION_SIZE, GRID_SIZE, SUBREGION_SIZE).sum(axis=(1, 3))\n",
        "    pd.DataFrame(np.round(region_noise, 4)).to_csv(os.path.join(setting_folder, \"region_noise_gt.csv\"), header=False, index=False)\n",
        "    pd.DataFrame(np.round(outcome, 2)).to_csv(os.path.join(setting_folder, \"outcome.csv\"), header=False, index=False)\n",
        "\n",
        "    # --- 4) counterfactual ---\n",
        "    interventions_cf = interventions.copy()\n",
        "    interventions_cf[:GRID_SIZE//2, :] = 1 - interventions_cf[:GRID_SIZE//2, :]\n",
        "    pd.DataFrame(interventions_cf).to_csv(os.path.join(setting_folder, \"interventions_cf_gt.csv\"), header=False, index=False)\n",
        "\n",
        "    outcome_cf = np.zeros_like(interventions_cf, dtype=float)\n",
        "    for i in range(GRID_SIZE):\n",
        "        for j in range(GRID_SIZE):\n",
        "            r0, c0 = i*SUBREGION_SIZE, j*SUBREGION_SIZE\n",
        "            block_wealth = wealth_hi[r0:r0+SUBREGION_SIZE, c0:c0+SUBREGION_SIZE]\n",
        "            treated = bool(interventions_cf[i, j])\n",
        "            votes = [[\n",
        "                np.clip(\n",
        "                    (RICH_VOTE if w == 3 else POOR_VOTE if w == 1 else INTER_VOTE) + subregion_noise[r0 + r, c0 + c]\n",
        "                    if treated else BASELINE_VOTE + subregion_noise[r0 + r, c0 + c],\n",
        "                    0, 1\n",
        "                )\n",
        "                for c, w in enumerate(block_wealth[r])]\n",
        "                for r in range(SUBREGION_SIZE)]\n",
        "            outcome_cf[i, j] = np.mean(votes) * 100\n",
        "\n",
        "    pd.DataFrame(np.round(outcome_cf, 2)).to_csv(os.path.join(setting_folder, \"outcome_cf_gt.csv\"), header=False, index=False)\n",
        "\n",
        "    print(f\" Simulation saved for inter_std={inter_std}, regional_std={regional_std}, sample={seed}\")\n"
      ],
      "metadata": {
        "id": "QFsWjzQiE4gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"data_ablation_exp1\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "inter_std_values = [0.01, 0.1, 0.3, 0.5, 0.7]\n",
        "regional_std_values = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
        "num_samples = 5\n",
        "base_seed = 42\n",
        "\n",
        "for inter_std in inter_std_values:\n",
        "    for regional_std in regional_std_values:\n",
        "        for sample_idx in range(num_samples):\n",
        "            seed = base_seed + sample_idx\n",
        "            generate_full_simulation(\n",
        "                inter_std=inter_std,\n",
        "                regional_std=regional_std,\n",
        "                seed=seed\n",
        "            )\n",
        "\n",
        "            print(f\" Simulation saved for inter_std={inter_std:.2f}, \"\n",
        "                  f\"regional_std={regional_std:.2f}, sample={sample_idx:02d}\")"
      ],
      "metadata": {
        "id": "jFjBlCRnFaR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATA_ROOT = \"data_ablation_exp1\"\n",
        "OUTFOLDER = \"exp1_ablation_results\"\n",
        "GRID, SUB = 10, 10\n",
        "N_EPOCHS, LR = 2000, 0.1\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "TRUE_PARAMS = np.array([\n",
        "    0.50, 0.50, 0.50,\n",
        "    0.40, 0.50, 0.80\n",
        "])\n",
        "\n",
        "# Ablation ranges\n",
        "inter_std_values = [0.01, 0.1, 0.3, 0.5, 0.7]\n",
        "regional_std_values = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
        "num_samples = 5\n",
        "base_seed = 42\n",
        "\n",
        "pathlib.Path(OUTFOLDER).mkdir(exist_ok=True)\n",
        "\n",
        "for inter_std in inter_std_values:\n",
        "    for regional_std in regional_std_values:\n",
        "        for sample_idx in range(num_samples):\n",
        "\n",
        "            seed = base_seed + sample_idx\n",
        "            setting_dir = os.path.join(\n",
        "                DATA_ROOT, f\"inter{inter_std:.2f}_regional{regional_std:.2f}_sample{seed}\"\n",
        "            )\n",
        "\n",
        "            if not os.path.exists(setting_dir):\n",
        "                print(f\" Missing data for {setting_dir}, skipping.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"--- Training for inter_std={inter_std:.2f}, \"\n",
        "                  f\"regional_std={regional_std:.2f}, sample={sample_idx:02d} ---\")\n",
        "\n",
        "\n",
        "            wealth_hi = pd.read_csv(os.path.join(setting_dir, \"wealth_high_res.csv\"), header=None).values\n",
        "            interv_np = pd.read_csv(os.path.join(setting_dir, \"interventions.csv\"), header=None).values\n",
        "            outcome_np = pd.read_csv(os.path.join(setting_dir, \"outcome.csv\"), header=None).values\n",
        "\n",
        "\n",
        "            counts_np = np.zeros((GRID, GRID, 3), dtype=int)\n",
        "            for i in range(GRID):\n",
        "                for j in range(GRID):\n",
        "                    blk = wealth_hi[i*SUB:(i+1)*SUB, j*SUB:(j+1)*SUB]\n",
        "                    counts_np[i, j, 0] = (blk == 1).sum()\n",
        "                    counts_np[i, j, 1] = (blk == 2).sum()\n",
        "                    counts_np[i, j, 2] = (blk == 3).sum()\n",
        "\n",
        "            interv_t  = torch.tensor(interv_np,  dtype=torch.float32, device=DEVICE)\n",
        "            counts_t  = torch.tensor(counts_np,  dtype=torch.float32, device=DEVICE)\n",
        "            outcome_t = torch.tensor(outcome_np, dtype=torch.float32, device=DEVICE)\n",
        "            treated_mask = interv_t == 1\n",
        "\n",
        "\n",
        "            params = torch.nn.Parameter(torch.tensor(\n",
        "                [0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=torch.float32, device=DEVICE\n",
        "            ))\n",
        "            opt = torch.optim.Adam([params], lr=LR)\n",
        "\n",
        "            loss_hist = []\n",
        "            param_mse_hist = []\n",
        "\n",
        "            for epoch in range(N_EPOCHS):\n",
        "                opt.zero_grad()\n",
        "\n",
        "                mean_ctrl  = (counts_t[:, :, 0]*params[0] +\n",
        "                              counts_t[:, :, 1]*params[1] +\n",
        "                              counts_t[:, :, 2]*params[2]) / 100.0\n",
        "                mean_trt   = (counts_t[:, :, 0]*params[3] +\n",
        "                              counts_t[:, :, 1]*params[4] +\n",
        "                              counts_t[:, :, 2]*params[5]) / 100.0\n",
        "\n",
        "                mean_pred  = torch.where(treated_mask, mean_trt, mean_ctrl)\n",
        "                loss = torch.mean((mean_pred*100 - outcome_t) ** 2)\n",
        "\n",
        "                param_mse = torch.mean((params - torch.tensor(TRUE_PARAMS, device=DEVICE))**2)\n",
        "\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                loss_hist.append(loss.item())\n",
        "                param_mse_hist.append(param_mse.item())\n",
        "\n",
        "            loss_df = pd.DataFrame({\n",
        "                \"mse_loss\": loss_hist,\n",
        "                \"param_mse\": param_mse_hist\n",
        "            })\n",
        "            loss_filename = f\"loss_inter{inter_std:.2f}_regional{regional_std:.2f}_sample{sample_idx:02d}.csv\"\n",
        "            loss_df.to_csv(os.path.join(OUTFOLDER, loss_filename), index_label=\"epoch\")\n",
        "\n",
        "            param_names = [\"mu_poor_0\",\"mu_inter_0\",\"mu_rich_0\",\n",
        "                           \"mu_poor_1\",\"mu_inter_1\",\"mu_rich_1\"]\n",
        "            results_df = pd.DataFrame({\"parameter\": param_names,\n",
        "                                       \"value\": params.detach().cpu().numpy()})\n",
        "            results_filename = f\"results_inter{inter_std:.2f}_regional{regional_std:.2f}_sample{sample_idx:02d}.csv\"\n",
        "            results_df.to_csv(os.path.join(OUTFOLDER, results_filename), index=False)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                mean_ctrl_final  = (counts_t[:, :, 0]*params[0] +\n",
        "                                    counts_t[:, :, 1]*params[1] +\n",
        "                                    counts_t[:, :, 2]*params[2]) / 100.0\n",
        "                mean_trt_final   = (counts_t[:, :, 0]*params[3] +\n",
        "                                    counts_t[:, :, 1]*params[4] +\n",
        "                                    counts_t[:, :, 2]*params[5]) / 100.0\n",
        "                mean_factual = torch.where(treated_mask, mean_trt_final, mean_ctrl_final) * 100\n",
        "\n",
        "            region_noise = (outcome_t - mean_factual).cpu().numpy()\n",
        "            noise_filename = f\"region_noise_inter{inter_std:.2f}_regional{regional_std:.2f}_sample{sample_idx:02d}.csv\"\n",
        "            pd.DataFrame(np.round(region_noise,3)).to_csv(os.path.join(OUTFOLDER, noise_filename),\n",
        "                                                          header=False, index=False)\n",
        "\n",
        "print(\" Ablation study training complete.\")\n"
      ],
      "metadata": {
        "id": "wGQMGqm7HL6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "OUTFOLDER = \"exp1_ablation_results\"\n",
        "inter_std_values = [0.01, 0.1, 0.3, 0.5, 0.7]\n",
        "regional_std_values = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
        "num_samples = 5\n",
        "\n",
        "sns.set_context(\"paper\", font_scale=1.4)\n",
        "plt.rcParams.update({\n",
        "    \"axes.edgecolor\": \"black\",\n",
        "    \"axes.linewidth\": 1.5,\n",
        "    \"axes.spines.top\": False,\n",
        "    \"axes.spines.right\": False,\n",
        "    \"xtick.direction\": \"out\",\n",
        "    \"ytick.direction\": \"out\",\n",
        "    \"xtick.major.size\": 5,\n",
        "    \"ytick.major.size\": 5,\n",
        "    \"xtick.labelsize\": 13,\n",
        "    \"ytick.labelsize\": 13,\n",
        "    \"legend.frameon\": False,\n",
        "})\n",
        "colors = sns.color_palette(\"colorblind\")\n",
        "\n",
        "loss_records = []\n",
        "\n",
        "for inter_std in inter_std_values:\n",
        "    for regional_std in regional_std_values:\n",
        "        for sample_idx in range(num_samples):\n",
        "            fname = f\"loss_inter{inter_std:.2f}_regional{regional_std:.2f}_sample{sample_idx:02d}.csv\"\n",
        "            fpath = os.path.join(OUTFOLDER, fname)\n",
        "            if not os.path.exists(fpath):\n",
        "                print(f\"[Warning] Missing: {fname}\")\n",
        "                continue\n",
        "            df = pd.read_csv(fpath)\n",
        "            loss_records.append({\n",
        "                \"inter_std\": inter_std,\n",
        "                \"regional_std\": regional_std,\n",
        "                \"sample\": sample_idx,\n",
        "                \"final_mse_loss\": df[\"mse_loss\"].iloc[-1],\n",
        "                \"final_param_mse\": df[\"param_mse\"].iloc[-1]\n",
        "            })\n",
        "\n",
        "loss_df = pd.DataFrame(loss_records)\n",
        "\n",
        "loss_summary = (\n",
        "    loss_df.groupby([\"inter_std\", \"regional_std\"])\n",
        "    .agg(final_mse_loss_mean=(\"final_mse_loss\", \"mean\"),\n",
        "         final_mse_loss_std=(\"final_mse_loss\", \"std\"),\n",
        "         final_param_mse_mean=(\"final_param_mse\", \"mean\"),\n",
        "         final_param_mse_std=(\"final_param_mse\", \"std\"))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "for i, reg_std in enumerate(regional_std_values):\n",
        "    subset = loss_summary[loss_summary[\"regional_std\"] == reg_std]\n",
        "    ax.errorbar(\n",
        "        subset[\"inter_std\"], subset[\"final_mse_loss_mean\"],\n",
        "        yerr=subset[\"final_mse_loss_std\"],\n",
        "        label=f\"{reg_std:.2f}\",\n",
        "        marker=\"o\", markersize=6,\n",
        "        lw=2.5, capsize=4,\n",
        "        color=colors[i % len(colors)]\n",
        "    )\n",
        "\n",
        "ax.set_title(\"Final Region-Level MSE\", fontsize=16, pad=12)\n",
        "ax.set_xlabel(\"Within-Region Standard Deviation\", fontsize=14)\n",
        "ax.set_ylabel(\"Region-Level MSE\", fontsize=14)\n",
        "ax.legend(title=\"Across-Region Std\", fontsize=11, title_fontsize=12)\n",
        "ax.grid(False)\n",
        "ax.tick_params(axis='both', which='both', length=0) # <--- REMOVE TICKS\n",
        "plt.tight_layout()\n",
        "fig.savefig(os.path.join(OUTFOLDER, \"final_mse_loss_with_uncertainty.jpg\"), dpi=300)\n",
        "fig.savefig(os.path.join(OUTFOLDER, \"final_mse_loss_with_uncertainty.pdf\"))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "for i, reg_std in enumerate(regional_std_values):\n",
        "    subset = loss_summary[loss_summary[\"regional_std\"] == reg_std]\n",
        "    ax.errorbar(\n",
        "        subset[\"inter_std\"], subset[\"final_param_mse_mean\"],\n",
        "        yerr=subset[\"final_param_mse_std\"],\n",
        "        label=f\"{reg_std:.2f}\",\n",
        "        marker=\"o\", markersize=6,\n",
        "        lw=2.5, capsize=4,\n",
        "        color=colors[i % len(colors)]\n",
        "    )\n",
        "\n",
        "ax.set_title(\"Final Parameter MSE\", fontsize=16, pad=12)\n",
        "ax.set_xlabel(\"Within-Region Standard Deviation\", fontsize=14)\n",
        "ax.set_ylabel(\"Final Parameter-Level MSE\", fontsize=14)\n",
        "ax.legend(title=\"Across-Region Std\", fontsize=11, title_fontsize=12)\n",
        "ax.grid(False)\n",
        "ax.tick_params(axis='both', which='both', length=0) # <--- REMOVE TICKS\n",
        "plt.tight_layout()\n",
        "fig.savefig(os.path.join(OUTFOLDER, \"final_param_mse_with_uncertainty.jpg\"), dpi=300)\n",
        "fig.savefig(os.path.join(OUTFOLDER, \"final_param_mse_with_uncertainty.pdf\"))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "pivot_loss = loss_summary.pivot(index=\"inter_std\", columns=\"regional_std\", values=\"final_mse_loss_mean\")\n",
        "fig, ax = plt.subplots(figsize=(7.5, 5.5))\n",
        "sns.heatmap(\n",
        "    pivot_loss, annot=True, fmt=\".3f\", cmap=\"viridis\", linewidths=0.5,\n",
        "    cbar_kws={'label': 'Final Region MSE'}, ax=ax\n",
        ")\n",
        "\n",
        "ax.set_title(\"Mean Final Region MSE (Heatmap)\", fontsize=16, pad=12)\n",
        "ax.set_xlabel(\"Across-Region Std\", fontsize=14)\n",
        "ax.set_ylabel(\"Within-Region Std\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "fig.savefig(os.path.join(OUTFOLDER, \"final_mse_loss_heatmap_mean.jpg\"), dpi=300)\n",
        "fig.savefig(os.path.join(OUTFOLDER, \"final_mse_loss_heatmap_mean.pdf\"))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "npdJYEacI-yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exp. 2\n",
        "Unknown Intervention Locations : Examine how regional public school funding affects educational outcomes when the exact subregion receiving the funding is unknown. Each region contains multiple school districts, but only one district receives additional resources, and at the regional level only the total spending is observed. The goal is to identify the hidden intervention location within each region and recover the corresponding local causal effects."
      ],
      "metadata": {
        "id": "rzY_kl58II1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "shift_param = 1.23\n",
        "scale_param = 4.2\n",
        "noise_var = 0.005\n",
        "data_dir = 'data_exp2'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1) Interventions subregion\n",
        "interventions_sub = np.zeros((num_regions, num_subregions))\n",
        "for i in range(1, num_regions):  # Skip row 0\n",
        "    col_idx = np.random.randint(0, num_subregions)\n",
        "    interventions_sub[i, col_idx] = np.random.uniform(0.2, 1)\n",
        "pd.DataFrame(interventions_sub).to_csv(os.path.join(data_dir, 'interventions_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# Interventions region: sum rows\n",
        "interventions_reg = np.sum(interventions_sub, axis=1)\n",
        "pd.DataFrame(interventions_reg).to_csv(os.path.join(data_dir, 'interventions_region.csv'), index=False, header=False)\n",
        "\n",
        "# 2) Context (wealth) subregion\n",
        "context_sub = np.random.uniform(0, 1, (num_regions, num_subregions))\n",
        "pd.DataFrame(context_sub).to_csv(os.path.join(data_dir, 'context_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# 3) Noise subregion\n",
        "noise_sub = np.random.normal(0, np.sqrt(noise_var), (num_regions, num_subregions))\n",
        "pd.DataFrame(noise_sub).to_csv(os.path.join(data_dir, 'noise_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# 4) Outcome subregion\n",
        "outcome_sub = (shift_param - context_sub) * scale_param * interventions_sub + noise_sub\n",
        "pd.DataFrame(outcome_sub).to_csv(os.path.join(data_dir, 'outcome_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# Outcome region: average rows\n",
        "outcome_reg = np.mean(outcome_sub, axis=1)\n",
        "pd.DataFrame(outcome_reg).to_csv(os.path.join(data_dir, 'outcome_region.csv'), index=False, header=False)"
      ],
      "metadata": {
        "id": "OQj6uDBMILcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp2'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "grid_size_reg = 10  # 10x10 for regions\n",
        "grid_size_sub = 20  # 20x20 for subregions (2x2 per region)\n",
        "sns.set_context('paper', font_scale=1.5)\n",
        "reds = sns.color_palette(\"deep\")[3]\n",
        "blues = sns.color_palette(\"deep\")[0]\n",
        "\n",
        "def visualize_subregion_csv(file_name, cmap, title, is_wealth=False):\n",
        "    data = pd.read_csv(os.path.join(data_dir, file_name), header=None).values\n",
        "    # Reshape to 20x20: each region row becomes 2x2 block\n",
        "    grid = np.zeros((grid_size_sub, grid_size_sub))\n",
        "    for i in range(grid_size_reg):\n",
        "        for j in range(grid_size_reg):\n",
        "            region_idx = i * grid_size_reg + j\n",
        "            sub_data = data[region_idx, :].reshape(2, 2)\n",
        "            grid[2*i:2*(i+1), 2*j:2*(j+1)] = sub_data\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    if is_wealth:\n",
        "        heatmap_cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
        "    else:\n",
        "        if isinstance(cmap, tuple):\n",
        "            heatmap_cmap = sns.light_palette(cmap, as_cmap=True)\n",
        "        else:\n",
        "            heatmap_cmap = cmap\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=True, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title(title, fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)  # No legend box\n",
        "    plt.tight_layout()\n",
        "    base_name = file_name.replace('.csv', '')\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def visualize_region_csv(file_name, cmap, title, is_wealth=False):\n",
        "    data = pd.read_csv(os.path.join(data_dir, file_name), header=None).values.flatten()\n",
        "    # Reshape to 10x10\n",
        "    grid = data.reshape(grid_size_reg, grid_size_reg)\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    if is_wealth:\n",
        "        heatmap_cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
        "    else:\n",
        "        if isinstance(cmap, tuple):\n",
        "            heatmap_cmap = sns.light_palette(cmap, as_cmap=True)\n",
        "        else:\n",
        "            heatmap_cmap = cmap\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=True, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title(title, fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)\n",
        "    plt.tight_layout()\n",
        "    base_name = file_name.replace('.csv', '')\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Visualize each\n",
        "visualize_subregion_csv('interventions_subregion.csv', reds, 'Interventions Subregion', is_wealth=False)\n",
        "visualize_region_csv('interventions_region.csv', reds, 'Interventions Region', is_wealth=False)\n",
        "visualize_subregion_csv('context_subregion.csv', None, 'Wealth Subregion', is_wealth=True)\n",
        "visualize_subregion_csv('noise_subregion.csv', 'coolwarm', 'Noise Subregion', is_wealth=False)  # Using coolwarm for noise as not specified\n",
        "visualize_subregion_csv('outcome_subregion.csv', blues, 'Outcome Subregion', is_wealth=False)\n",
        "visualize_region_csv('outcome_region.csv', blues, 'Outcome Region', is_wealth=False)\n",
        "\n",
        "# Wealth low-res: average context_sub to get wealth_reg\n",
        "context_sub = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values\n",
        "wealth_reg = np.mean(context_sub, axis=1)\n",
        "pd.DataFrame(wealth_reg).to_csv(os.path.join(data_dir, 'context_region.csv'), index=False, header=False)  # Temp save for viz\n",
        "visualize_region_csv('context_region.csv', None, 'Wealth Region', is_wealth=True)"
      ],
      "metadata": {
        "id": "0n4OFPVAKray"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp2'\n",
        "grid_size_reg = 10  # 10x10 for regions\n",
        "grid_size_sub = 20  # 20x20 for subregions (2x2 per region)\n",
        "\n",
        "# Color maps from seaborn deep palette\n",
        "colors = sns.color_palette(\"deep\")\n",
        "red_cmap = sns.light_palette(colors[3], as_cmap=True)  # Red\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue\n",
        "green_cmap = sns.light_palette(colors[2], as_cmap=True)  # Green\n",
        "\n",
        "# Parameter to control space between subplots (as fraction of average axis width)\n",
        "space_between = 0.05  # Adjust this value as needed (e.g., 0.0 for no space, 0.5 for more space)\n",
        "\n",
        "# Load data for combined\n",
        "interv_data = pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.flatten().reshape(grid_size_reg, grid_size_reg).astype(float)  # Assuming 0/1, float for heatmap\n",
        "outcome_data = pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.flatten().reshape(grid_size_reg, grid_size_reg)# / 100.0  # Assuming 0-100, normalize to 0-1\n",
        "context_data = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values\n",
        "context_grid = np.zeros((grid_size_sub, grid_size_sub))\n",
        "for i in range(grid_size_reg):\n",
        "    for j in range(grid_size_reg):\n",
        "        region_idx = i * grid_size_reg + j\n",
        "        sub_data = context_data[region_idx, :].reshape(2, 2)\n",
        "        context_grid[2*i:2*(i+1), 2*j:2*(j+1)] = sub_data\n",
        "context_norm = (context_grid - 1) / 2.0  # Normalize assuming values 1,2,3 to 0-1\n",
        "\n",
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Left: Intervention in red\n",
        "sns.heatmap(interv_data, ax=axs[0], cmap=red_cmap, cbar=False, square=True, linewidths=2, vmin=0.0, vmax=1.0)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "sns.heatmap(outcome_data, ax=axs[1], cmap=blue_cmap, cbar=False, square=True, linewidths=2, vmin=0.0, vmax=1.0)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "\n",
        "# Right: Context in green\n",
        "sns.heatmap(context_norm, ax=axs[2], cmap=green_cmap, cbar=False, square=True, linewidths=1)#, vmin=0.0)#, vmax=1.0)\n",
        "axs[2].set_xticks([])\n",
        "axs[2].set_yticks([])\n",
        "\n",
        "# Adjust the space between subplots\n",
        "fig.subplots_adjust(wspace=space_between)\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(os.path.join(data_dir, 'combined_heatmaps.jpg'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(os.path.join(data_dir, 'combined_heatmaps.pdf'), bbox_inches='tight', pad_inches=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d2YsbyU3KxmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp2'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "epochs = 10000\n",
        "lr = 0.001\n",
        "torch.manual_seed(42)\n",
        "grid_size_sub = 20\n",
        "sns.set_context('paper', font_scale=1.5)\n",
        "reds = sns.color_palette(\"deep\")[3]\n",
        "\n",
        "# True parameters (from data generation)\n",
        "true_shift_param = 1.23\n",
        "true_scale_param = 4.2\n",
        "\n",
        "\n",
        "# Read all CSVs\n",
        "interventions_reg = torch.tensor(pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.flatten(), dtype=torch.float32)\n",
        "outcome_reg = torch.tensor(pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.flatten(), dtype=torch.float32)\n",
        "context_sub = torch.tensor(pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values, dtype=torch.float32)\n",
        "outcome_sub = torch.tensor(pd.read_csv(os.path.join(data_dir, 'outcome_subregion.csv'), header=None).values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Model\n",
        "class CausalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.inter_est = nn.Parameter(torch.randn(num_regions, num_subregions))\n",
        "        self.shift = nn.Parameter(torch.tensor(1.0))\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "        self.temp = torch.tensor(10.0)\n",
        "        self.noise_scale = 0.1\n",
        "\n",
        "    def preprocess_inter(self):\n",
        "        inter_pos = (self.inter_est + torch.randn_like(self.inter_est)*self.noise_scale) ** 2  # Ensure positivity\n",
        "        inter_sparse = torch.softmax(inter_pos / self.temp, dim=1)  # Softmax for differentiable near-one-hot sparsity\n",
        "        inter_constrained = inter_sparse * interventions_reg.unsqueeze(1)  # Multiply by known region sum\n",
        "        return inter_constrained\n",
        "\n",
        "    def forward(self):\n",
        "        inter_sub = self.preprocess_inter()\n",
        "        outcome_sub_pred = (self.shift - context_sub) * self.scale * inter_sub# + noise_sub\n",
        "        outcome_reg_pred = outcome_sub_pred.mean(dim=1)\n",
        "        return outcome_reg_pred, outcome_sub_pred\n",
        "\n",
        "model = CausalModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Train\n",
        "losses = []\n",
        "param_losses = []\n",
        "subregion_losses = []\n",
        "parameter_logs = {'shift': [], 'scale': []}\n",
        "start_temp = 10.0\n",
        "end_temp = 2.0\n",
        "for epoch in range(epochs):\n",
        "    model.temp = torch.tensor(start_temp + (end_temp - start_temp) * (epoch / (epochs - 1)))\n",
        "    optimizer.zero_grad()\n",
        "    outcome_reg_pred, outcome_sub_pred = model.forward()\n",
        "    loss = criterion(outcome_reg_pred, outcome_reg)\n",
        "\n",
        "    # Calculate subregion loss\n",
        "    subregion_loss = criterion(outcome_sub_pred, outcome_sub)\n",
        "    subregion_losses.append(subregion_loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Log parameter values\n",
        "    parameter_logs['shift'].append(model.shift.item())\n",
        "    parameter_logs['scale'].append(model.scale.item())\n",
        "\n",
        "    # Calculate loss with respect to true parameters\n",
        "    param_loss = criterion(torch.tensor([model.shift.item(), model.scale.item()]), torch.tensor([true_shift_param, true_scale_param]))\n",
        "    param_losses.append(param_loss.item())\n",
        "\n",
        "\n",
        "# Save losses to CSV\n",
        "pd.DataFrame(losses).to_csv(os.path.join(data_dir, 'outcome_region_loss.csv'), index=False, header=['loss'])\n",
        "pd.DataFrame(param_losses).to_csv(os.path.join(data_dir, 'parameter_loss.csv'), index=False, header=['param_loss'])\n",
        "pd.DataFrame(subregion_losses).to_csv(os.path.join(data_dir, 'outcome_subregion_loss.csv'), index=False, header=['subregion_loss'])\n",
        "\n",
        "\n",
        "# Save loss curve (Region)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses)\n",
        "plt.title('Loss Curve (Outcome Region)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curve_region.jpg'))\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curve_region.pdf'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Save loss curve (Subregion)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(subregion_losses)\n",
        "plt.title('Loss Curve (Outcome Subregion)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curve_subregion.jpg'))\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curve_subregion.pdf'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# Plot parameter logs\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(parameter_logs['shift'], label='Shift Parameter')\n",
        "plt.plot(parameter_logs['scale'], label='Scale Parameter')\n",
        "plt.title('Parameter Values During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Parameter Value')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(data_dir, 'parameter_values.jpg'))\n",
        "plt.savefig(os.path.join(data_dir, 'parameter_values.pdf'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Plot parameter loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(param_losses)\n",
        "plt.title('Loss w.r.t True Parameters')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.savefig(os.path.join(data_dir, 'parameter_loss_curve.jpg'))\n",
        "plt.savefig(os.path.join(data_dir, 'parameter_loss_curve.pdf'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# Print final estimates\n",
        "print(f\"Final shift_param: {model.shift.item()}\")\n",
        "print(f\"Final scale_param: {model.scale.item()}\")\n",
        "\n",
        "# Save params to txt\n",
        "with open(os.path.join(data_dir, 'parameter_estimate.txt'), 'w') as f:\n",
        "    f.write(f\"shift_param: {model.shift.item()}\\n\")\n",
        "    f.write(f\"scale_param: {model.scale.item()}\\n\")\n",
        "\n",
        "# Save estimated interventions sub (processed)\n",
        "inter_est_final = model.preprocess_inter().detach().numpy()\n",
        "pd.DataFrame(inter_est_final).to_csv(os.path.join(data_dir, 'interventions_subregion_estimated.csv'), index=False, header=False)\n",
        "\n",
        "# Visualize estimated subregion\n",
        "def visualize_estimated_subregion():\n",
        "    data = inter_est_final\n",
        "    grid = np.zeros((grid_size_sub, grid_size_sub))\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            region_idx = i * 10 + j\n",
        "            sub_data = data[region_idx, :].reshape(2, 2)\n",
        "            grid[2*i:2*(i+1), 2*j:2*(j+1)] = sub_data\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    heatmap_cmap = sns.light_palette(reds, as_cmap=True)\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=True, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title('Estimated Interventions Subregion', fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, 'interventions_subregion_estimated.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, 'interventions_subregion_estimated.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_estimated_subregion()"
      ],
      "metadata": {
        "id": "40x0GcbZbD1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data_exp2'\n",
        "loglog_flag = False\n",
        "\n",
        "# Load loss history from CSV\n",
        "loss_hist = pd.read_csv(\n",
        "    os.path.join(data_dir, 'outcome_region_loss.csv'),\n",
        "    header=0  # use first row as header, then drop column names\n",
        ").iloc[:, 0].values\n",
        "\n",
        "ce_loss_hist = pd.read_csv(\n",
        "    os.path.join(data_dir, 'outcome_subregion_loss.csv'),\n",
        "    header=0\n",
        ").iloc[:, 0].values\n",
        "epochs = np.arange(1, len(loss_hist) + 1)\n",
        "\n",
        "# Plotting\n",
        "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "colors = sns.color_palette(\"deep\")\n",
        "\n",
        "ax1.set_xlabel('Epoch', fontsize=40)\n",
        "ax1.set_ylabel('Training Loss', fontsize=30, color=colors[0])\n",
        "if loglog_flag:\n",
        "    ax1.loglog(epochs, loss_hist, color=colors[0], alpha=0.9, lw=3)\n",
        "else:\n",
        "    ax1.plot(epochs, loss_hist, color=colors[0], alpha=0.7, lw=2)\n",
        "ax1.tick_params(axis='both', which='both', length=0, labelsize=18, colors=colors[0])\n",
        "# ax1.set_title('MSE Curves', fontsize=32)\n",
        "ax1.spines['top'].set_visible(False)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('MSE Local Effect', fontsize=30, color=colors[3])\n",
        "if loglog_flag:\n",
        "    ax2.loglog(epochs, ce_loss_hist, color=colors[3], alpha=0.9, ls='--', lw=5)\n",
        "else:\n",
        "    ax2.plot(epochs, ce_loss_hist, color=colors[3], alpha=0.7, ls='--', lw=2)\n",
        "ax2.tick_params(axis='y', which='both', length=0, labelsize=18, colors=colors[3])\n",
        "ax2.spines['top'].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(os.path.join(data_dir, 'estimation_loss.jpg'), dpi=300)\n",
        "fig.savefig(os.path.join(data_dir, 'exp2_estimation_loss.pdf'))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dUb0-STrbVQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_sub_np = context_sub.numpy() if isinstance(context_sub, torch.Tensor) else context_sub\n",
        "interventions_sub_np = interventions_sub.numpy() if isinstance(interventions_sub, torch.Tensor) else interventions_sub\n",
        "true_outcome_sub = (shift_param - context_sub_np) * scale_param * interventions_sub_np\n",
        "\n",
        "\n",
        "# Load estimated interventions\n",
        "inter_est_final = pd.read_csv(os.path.join(data_dir, 'interventions_subregion_estimated.csv'), header=None).values\n",
        "\n",
        "# Check the shape of the loaded estimated interventions\n",
        "if inter_est_final.shape != (num_regions, num_subregions):\n",
        "    print(f\"Warning: Loaded estimated interventions have shape {inter_est_final.shape}, expected ({num_regions}, {num_subregions})\")\n",
        "    pass\n",
        "\n",
        "\n",
        "# Calculate predicted outcome at subregion level using estimated interventions and learned parameters\n",
        "with open(os.path.join(data_dir, 'parameter_estimate.txt'), 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    estimated_shift = float(lines[0].split(': ')[1])\n",
        "    estimated_scale = float(lines[1].split(': ')[1])\n",
        "\n",
        "# Ensure context_sub is a numpy array for the calculation\n",
        "context_sub_np = context_sub.numpy() if isinstance(context_sub, torch.Tensor) else context_sub\n",
        "\n",
        "predicted_outcome_sub = (estimated_shift - context_sub_np) * estimated_scale * inter_est_final\n",
        "\n",
        "\n",
        "# Visualize true vs predicted outcome at subregion level\n",
        "def visualize_true_vs_predicted_subregion(true_data, predicted_data, num_regions=100, num_subregions=4):\n",
        "    grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "    subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "\n",
        "    true_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    predicted_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    difference_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "\n",
        "    for i in range(int(np.sqrt(num_regions))):\n",
        "        for j in range(int(np.sqrt(num_regions))):\n",
        "            region_idx = i * int(np.sqrt(num_regions)) + j\n",
        "            # Ensure sub_data is a numpy array before reshaping\n",
        "            true_sub_data = np.asarray(true_data[region_idx, :]).reshape(subregions_per_dim, subregions_per_dim)\n",
        "            predicted_sub_data = np.asarray(predicted_data[region_idx, :]).reshape(subregions_per_dim, subregions_per_dim)\n",
        "            true_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = true_sub_data\n",
        "            predicted_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = predicted_sub_data\n",
        "            difference_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = true_sub_data - predicted_sub_data\n",
        "\n",
        "    # Color maps from seaborn deep palette (assuming these are available in the notebook's global scope)\n",
        "    colors = sns.color_palette(\"deep\")\n",
        "    blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue for Outcome\n",
        "    coolwarm_cmap = 'coolwarm' # Using coolwarm for difference as not specified\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5)) # Adjusted figure size\n",
        "\n",
        "\n",
        "    # True Outcome (Blue)\n",
        "    sns.heatmap(true_grid, ax=axs[0], cmap=blue_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[0].set_xticks([])\n",
        "    axs[0].set_yticks([])\n",
        "\n",
        "    # Predicted Outcome (Blue)\n",
        "    sns.heatmap(predicted_grid, ax=axs[1], cmap=blue_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[1].set_xticks([])\n",
        "    axs[1].set_yticks([])\n",
        "\n",
        "    # Difference (Coolwarm)\n",
        "    sns.heatmap(difference_grid, ax=axs[2], cmap=coolwarm_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[2].set_xticks([])\n",
        "    axs[2].set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, 'exp2_true_vs_predicted_outcome_subregion_styled.pdf'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, 'exp2_true_vs_predicted_outcome_subregion_styled.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_true_vs_predicted_subregion(true_outcome_sub, predicted_outcome_sub, num_regions, num_subregions)\n",
        "\n",
        "# Calculate MSE for true vs predicted subregion outcome\n",
        "mse_subregion = np.mean((np.asarray(true_outcome_sub) - np.asarray(predicted_outcome_sub)) ** 2)\n",
        "print(f\"MSE between true and predicted subregion outcome: {mse_subregion}\")"
      ],
      "metadata": {
        "id": "_XDE9Jh8bcH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp. 3\n",
        "Hidden Spatial Confounding: Estimate the effect of heatwaves (regional interventions over time) on school outcomes when both parental education level (observed covariate) and vegetation coverage (unobserved confounder) modulate the effect. The goal is to recover local treatment effects and reconstruct the hidden confounder field from aggregated outcomes."
      ],
      "metadata": {
        "id": "FW6EH3pDIRF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "num_months = 48\n",
        "num_regions = 100\n",
        "num_subregions_per_region = 9\n",
        "data_folder = 'data_exp3'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(f'{data_folder}/interventions_subregion', exist_ok=True)\n",
        "os.makedirs(f'{data_folder}/interventions_region', exist_ok=True)\n",
        "os.makedirs(f'{data_folder}/context_subregion', exist_ok=True)\n",
        "os.makedirs(f'{data_folder}/noise_subregion', exist_ok=True)\n",
        "os.makedirs(f'{data_folder}/outcome_subregion', exist_ok=True)\n",
        "os.makedirs(f'{data_folder}/outcome_region', exist_ok=True)\n",
        "\n",
        "# 1) Interventions\n",
        "for month in range(1, num_months + 1):\n",
        "    interventions_sub = np.zeros((num_regions, num_subregions_per_region))\n",
        "    for r in range(num_regions):\n",
        "        val = 1 if np.random.rand() < 0.4 else 0\n",
        "        interventions_sub[r, :] = val\n",
        "    pd.DataFrame(interventions_sub).to_csv(\n",
        "        f'{data_folder}/interventions_subregion/interventions_subregion_{month:02d}.csv', index=False, header=False\n",
        "    )\n",
        "    interventions_reg = interventions_sub.mean(axis=1).reshape(-1, 1)\n",
        "    pd.DataFrame(interventions_reg).to_csv(\n",
        "        f'{data_folder}/interventions_region/interventions_region_{month:02d}.csv', index=False, header=False\n",
        "    )\n",
        "\n",
        "# 2) Context (parents education)\n",
        "prev_context = None\n",
        "for month in range(1, num_months + 1):\n",
        "    if month == 1:\n",
        "        context = np.random.randint(1, 4, size=(num_regions, num_subregions_per_region))\n",
        "    else:\n",
        "        context = prev_context.copy()\n",
        "        flips = np.random.rand(num_regions, num_subregions_per_region) < 0.05\n",
        "        context[flips] = np.random.randint(1, 4, size=flips.sum())\n",
        "    pd.DataFrame(context).to_csv(\n",
        "        f'{data_folder}/context_subregion/context_subregion_{month:02d}.csv', index=False, header=False\n",
        "    )\n",
        "    prev_context = context\n",
        "\n",
        "# 3) Unobserved confounder (vegetation)\n",
        "vegetation = np.random.uniform(0, 1, size=(num_regions, num_subregions_per_region))\n",
        "pd.DataFrame(vegetation).to_csv(\n",
        "    f'{data_folder}/unobserved_confounder_context_subregion.csv', index=False, header=False\n",
        ")\n",
        "\n",
        "# 4) Noise\n",
        "for month in range(1, num_months + 1):\n",
        "    noise = np.random.normal(0, np.sqrt(0.02), size=(num_regions, num_subregions_per_region))\n",
        "    pd.DataFrame(noise).to_csv(\n",
        "        f'{data_folder}/noise_subregion/noise_subregion_{month:02d}.csv', index=False, header=False\n",
        "    )\n",
        "\n",
        "# 5) Outcomes\n",
        "for month in range(1, num_months + 1):\n",
        "    interv_sub = pd.read_csv(\n",
        "        f'{data_folder}/interventions_subregion/interventions_subregion_{month:02d}.csv', header=None\n",
        "    ).values\n",
        "    context = pd.read_csv(\n",
        "        f'{data_folder}/context_subregion/context_subregion_{month:02d}.csv', header=None\n",
        "    ).values\n",
        "    poor_indicator = (context == 1).astype(float)\n",
        "    intermediate_indicator = (context == 2).astype(float)\n",
        "    rich_indicator = (context == 3).astype(float)\n",
        "    outcome_sub = ((10 * poor_indicator * interv_sub) + (5 * intermediate_indicator * interv_sub) + (rich_indicator * interv_sub)) * (1 - vegetation)\n",
        "    pd.DataFrame(outcome_sub).to_csv(\n",
        "        f'{data_folder}/outcome_subregion/outcome_subregion_{month:02d}.csv', index=False, header=False\n",
        "    )\n",
        "    outcome_reg = outcome_sub.mean(axis=1).reshape(-1, 1)\n",
        "    pd.DataFrame(outcome_reg).to_csv(\n",
        "        f'{data_folder}/outcome_region/outcome_region_{month:02d}.csv', index=False, header=False\n",
        "    )"
      ],
      "metadata": {
        "id": "FcP8dK_OISXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Parameters\n",
        "data_folder = 'data_exp3'\n",
        "num_regions = 100\n",
        "region_grid_size = 10  # 10x10\n",
        "sub_grid_size = 30  # 30x30\n",
        "sub_per_region_side = 3  # 3x3 subregions per region\n",
        "\n",
        "# Color definitions\n",
        "colors = sns.color_palette(\"deep\")\n",
        "red = colors[3]\n",
        "blue = colors[0]\n",
        "green = colors[2]\n",
        "\n",
        "# Function to reshape subregion data (100x9) to 30x30 grid\n",
        "def reshape_to_subgrid(data):\n",
        "    grid = np.zeros((sub_grid_size, sub_grid_size))\n",
        "    for ri in range(region_grid_size):\n",
        "        for rj in range(region_grid_size):\n",
        "            reg_idx = ri * region_grid_size + rj\n",
        "            sub_data = data[reg_idx].reshape(sub_per_region_side, sub_per_region_side)\n",
        "            grid[ri*sub_per_region_side:(ri+1)*sub_per_region_side, rj*sub_per_region_side:(rj+1)*sub_per_region_side] = sub_data\n",
        "    return grid\n",
        "\n",
        "# Function to reshape region data (100x1) to 10x10 grid\n",
        "def reshape_to_reggrid(data):\n",
        "    return data.reshape(region_grid_size, region_grid_size)\n",
        "\n",
        "# Base plot function\n",
        "def base_heatmap(grid, cmap, title, vmin=None, vmax=None, cbar_ticks=None, cbar_labels=None, figsize=(10,10)):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    sns.heatmap(grid, ax=ax, cmap=cmap, vmin=vmin, vmax=vmax, square=True, linewidths=0, cbar_kws={'shrink': 0.5})\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_title(title, fontsize=20)\n",
        "    ax.tick_params(labelsize=14)\n",
        "    if cbar_ticks is not None and cbar_labels is not None:\n",
        "        cbar = ax.collections[0].colorbar\n",
        "        cbar.set_ticks(cbar_ticks)\n",
        "        cbar.set_ticklabels(cbar_labels)\n",
        "        cbar.ax.tick_params(labelsize=14)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Specific plot functions (one per data type)\n",
        "def plot_interventions_sub(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values  # 100x9\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    cmap = sns.light_palette(red, as_cmap=True)\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path), vmin=0, vmax=1)\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_interventions_reg(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values.squeeze()  # 100\n",
        "    grid = reshape_to_reggrid(data)\n",
        "    cmap = sns.light_palette(red, as_cmap=True)\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path), vmin=0, vmax=1)\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_context_sub(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values  # 100x9\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    cmap = ListedColormap(sns.color_palette(\"Spectral\", 3))\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path), vmin=1, vmax=3,\n",
        "                       cbar_ticks=[1,2,3], cbar_labels=['poor', 'intermediate', 'rich'])\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_confounder(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values  # 100x9\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    cmap = sns.light_palette(green, as_cmap=True)\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path), vmin=0, vmax=1)\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_outcome_sub(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values  # 100x9\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    cmap = sns.light_palette(blue, as_cmap=True)\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path), vmin=0, vmax=10)\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_outcome_reg(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values.squeeze()  # 100\n",
        "    grid = reshape_to_reggrid(data)\n",
        "    cmap = sns.light_palette(blue, as_cmap=True)\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path), vmin=0, vmax=10)\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_noise_sub(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values  # 100x9\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    cmap = sns.light_palette(blue, as_cmap=True)\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path))\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "# Unobserved confounder (single file)\n",
        "confounder_file = f'{data_folder}/unobserved_confounder_context_subregion.csv'\n",
        "if os.path.exists(confounder_file):\n",
        "    plot_confounder(confounder_file)\n",
        "\n",
        "# Independent plotting blocks (one per data type, no shared loop or conditions)\n",
        "# Interventions subregion\n",
        "for file in sorted(glob.glob(f'{data_folder}/interventions_subregion/*.csv')):\n",
        "    plot_interventions_sub(file)\n",
        "\n",
        "# Interventions region\n",
        "for file in sorted(glob.glob(f'{data_folder}/interventions_region/*.csv')):\n",
        "    plot_interventions_reg(file)\n",
        "\n",
        "# Context subregion\n",
        "for file in sorted(glob.glob(f'{data_folder}/context_subregion/*.csv')):\n",
        "    plot_context_sub(file)\n",
        "\n",
        "# Noise subregion\n",
        "for file in sorted(glob.glob(f'{data_folder}/noise_subregion/*.csv')):\n",
        "    plot_noise_sub(file)\n",
        "\n",
        "# Outcome subregion\n",
        "for file in sorted(glob.glob(f'{data_folder}/outcome_subregion/*.csv')):\n",
        "    plot_outcome_sub(file)\n",
        "\n",
        "# Outcome region\n",
        "for file in sorted(glob.glob(f'{data_folder}/outcome_region/*.csv')):\n",
        "    plot_outcome_reg(file)"
      ],
      "metadata": {
        "id": "yAgj--3Xb_8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Parameters\n",
        "data_folder = 'data_exp3'\n",
        "region_grid_size = 10  # 10x10\n",
        "sub_grid_size = 30  # 30x30\n",
        "sub_per_region_side = 3  # 3x3 subregions per region\n",
        "\n",
        "# Color definitions\n",
        "colors = sns.color_palette(\"deep\")\n",
        "red_cmap = sns.light_palette(colors[3], as_cmap=True)  # Red\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue\n",
        "green_cmap = sns.light_palette(colors[2], as_cmap=True)  # Green\n",
        "\n",
        "# Function to reshape subregion data (100x9) to 30x30 grid\n",
        "def reshape_to_subgrid(data):\n",
        "    grid = np.zeros((sub_grid_size, sub_grid_size))\n",
        "    for ri in range(region_grid_size):\n",
        "        for rj in range(region_grid_size):\n",
        "            reg_idx = ri * region_grid_size + rj\n",
        "            sub_data = data[reg_idx].reshape(sub_per_region_side, sub_per_region_side)\n",
        "            grid[ri*sub_per_region_side:(ri+1)*sub_per_region_side, rj*sub_per_region_side:(rj+1)*sub_per_region_side] = sub_data\n",
        "    return grid\n",
        "\n",
        "# Function to reshape region data (100x1) to 10x10 grid\n",
        "def reshape_to_reggrid(data):\n",
        "    return data.reshape(region_grid_size, region_grid_size)\n",
        "\n",
        "# Parameter to control space between subplots (as fraction of average axis width)\n",
        "space_between = 0.05\n",
        "# Day parameter\n",
        "day = 1\n",
        "\n",
        "# Load data for combined\n",
        "interv_file = os.path.join(data_folder, 'interventions_region', f'interventions_region_{day:02d}.csv')\n",
        "interv_data = pd.read_csv(interv_file, header=None).values.squeeze()\n",
        "interv_grid = reshape_to_reggrid(interv_data.astype(float))  # Assuming 0/1, float for heatmap\n",
        "\n",
        "outcome_file = os.path.join(data_folder, 'outcome_region', f'outcome_region_{day:02d}.csv')\n",
        "outcome_data = pd.read_csv(outcome_file, header=None).values.squeeze()\n",
        "outcome_grid = reshape_to_reggrid(outcome_data) #/ 10.0  # Assuming 0-10, normalize to 0-1\n",
        "\n",
        "context_file = os.path.join(data_folder, 'context_subregion', f'context_subregion_{day:02d}.csv')\n",
        "context_data = pd.read_csv(context_file, header=None).values\n",
        "context_grid = reshape_to_subgrid(context_data)\n",
        "context_norm = (context_grid - 1) / 2.0  # Normalize assuming values 1,2,3 to 0-1\n",
        "context_norm = (context_norm + 0.2) / 1.2\n",
        "\n",
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Left: Interventions in red\n",
        "sns.heatmap(interv_grid, ax=axs[0], cmap=red_cmap, cbar=False, square=True, linewidths=2, vmin=0.0, vmax=1.0)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "\n",
        "# Center: Outcomes in blue\n",
        "sns.heatmap(outcome_grid, ax=axs[1], cmap=blue_cmap, cbar=False, square=True, linewidths=2)#, vmin=0.0, vmax=1.0)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "\n",
        "# Right: Context in green\n",
        "sns.heatmap(context_norm, ax=axs[2], cmap=green_cmap, cbar=False, square=True, linewidths=1.0, vmin=0.0, vmax=1.0)\n",
        "axs[2].set_xticks([])\n",
        "axs[2].set_yticks([])\n",
        "\n",
        "\n",
        "# Adjust the space between subplots\n",
        "fig.subplots_adjust(wspace=space_between)\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(os.path.join(data_folder, f'combined_heatmaps_day{day}.jpg'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(os.path.join(data_folder, f'combined_heatmaps_day{day}.pdf'), bbox_inches='tight', pad_inches=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7H9EfaE-cUW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Parameters\n",
        "data_folder = 'data_exp3'\n",
        "num_months = 48\n",
        "num_regions = 100\n",
        "num_subregions_per_region = 9\n",
        "sub_grid_size = 30\n",
        "region_grid_size = 10\n",
        "sub_per_region_side = 3\n",
        "learning_rate = 0.0001\n",
        "epochs = 10000\n",
        "\n",
        "# Load data\n",
        "interv_reg_list = [\n",
        "    pd.read_csv(f'{data_folder}/interventions_region/interventions_region_{month:02d}.csv', header=None).values.squeeze()\n",
        "    for month in range(1, num_months + 1)\n",
        "]\n",
        "context_list = [\n",
        "    pd.read_csv(f'{data_folder}/context_subregion/context_subregion_{month:02d}.csv', header=None).values\n",
        "    for month in range(1, num_months + 1)\n",
        "]\n",
        "outcome_reg_list = [\n",
        "    pd.read_csv(f'{data_folder}/outcome_region/outcome_region_{month:02d}.csv', header=None).values.squeeze()\n",
        "    for month in range(1, num_months + 1)\n",
        "]\n",
        "\n",
        "gt_veg = pd.read_csv(f'{data_folder}/unobserved_confounder_context_subregion.csv', header=None).values\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(5, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1)\n",
        "        )\n",
        "        self.veg = nn.Parameter(torch.randn(num_regions, num_subregions_per_region))\n",
        "\n",
        "# Training\n",
        "model = Model()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "losses = []\n",
        "veg_mses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    perm = np.random.permutation(num_months)\n",
        "    epoch_loss = 0.0\n",
        "    for i in perm:\n",
        "        parent_edu = torch.tensor(context_list[i], dtype=torch.long)\n",
        "        interv = torch.tensor(interv_reg_list[i], dtype=torch.float32)\n",
        "        gt = torch.tensor(outcome_reg_list[i], dtype=torch.float32)\n",
        "\n",
        "        onehot = F.one_hot(parent_edu - 1, num_classes=3).float()  # 100x9x3\n",
        "        interv_sub = interv.unsqueeze(1).repeat(1, num_subregions_per_region)  # 100x9\n",
        "        inputs = torch.cat([onehot, interv_sub.unsqueeze(-1), torch.sigmoid(model.veg).unsqueeze(-1)], dim=-1)  # 100x9x5\n",
        "        flat = inputs.view(-1, 5)  # 900x5\n",
        "        pred_sub = model.mlp(flat).view(num_regions, num_subregions_per_region)  # 100x9\n",
        "        pred_reg = pred_sub.mean(dim=1)  # 100\n",
        "\n",
        "        loss = F.mse_loss(pred_reg, gt)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    losses.append(epoch_loss / num_months)\n",
        "    veg_est = torch.sigmoid(model.veg).detach().numpy()\n",
        "    veg_mse = np.mean((gt_veg - veg_est)**2)\n",
        "    veg_mses.append(veg_mse)\n",
        "    if (epoch + 1) % (epochs // 100) == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}: Avg Loss = {losses[-1]:.6f}, Veg MSE = {veg_mse:.6f}\")\n",
        "    row = pd.DataFrame({'epoch': [epoch+1], 'loss': [losses[-1]], 'veg_mse': [veg_mses[-1]]})\n",
        "    if epoch == 0:\n",
        "        row.to_csv(f'{data_folder}/training_curves.csv', index=False)\n",
        "    else:\n",
        "        row.to_csv(f'{data_folder}/training_curves.csv', mode='a', header=False, index=False)\n",
        "\n",
        "# Save loss curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses)\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.savefig(f'{data_folder}/loss_curve.jpg')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(veg_mses)\n",
        "plt.title('Vegetation MSE Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.savefig(f'{data_folder}/veg_mse_curve.jpg')\n",
        "plt.show()\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), f'{data_folder}/model.pth')\n",
        "\n",
        "# Compute and save estimated subregion outcomes (no noise)\n",
        "os.makedirs(f'{data_folder}/outcome_subregion_estimated', exist_ok=True)\n",
        "estimated_sub_outcomes = []\n",
        "with torch.no_grad():\n",
        "    for month in range(num_months):\n",
        "        parent_edu = torch.tensor(context_list[month], dtype=torch.long)\n",
        "        interv = torch.tensor(interv_reg_list[month], dtype=torch.float32)\n",
        "        onehot = F.one_hot(parent_edu - 1, num_classes=3).float()\n",
        "        interv_sub = interv.unsqueeze(1).repeat(1, num_subregions_per_region)\n",
        "        inputs = torch.cat([onehot, interv_sub.unsqueeze(-1), torch.sigmoid(model.veg).unsqueeze(-1)], dim=-1)\n",
        "        flat = inputs.view(-1, 5)\n",
        "        pred_sub = model.mlp(flat).view(num_regions, num_subregions_per_region).numpy()\n",
        "        estimated_sub_outcomes.append(pred_sub)\n",
        "        pd.DataFrame(pred_sub).to_csv(\n",
        "            f'{data_folder}/outcome_subregion_estimated/outcome_subregion_estimated_{month+1:02d}.csv',\n",
        "            index=False, header=False\n",
        "        )\n",
        "\n",
        "# Save estimated vegetation\n",
        "estimated_veg = torch.sigmoid(model.veg).detach().numpy()\n",
        "pd.DataFrame(estimated_veg).to_csv(\n",
        "    f'{data_folder}/unobserved_confounder_context_subregion_estimated.csv', index=False, header=False\n",
        ")\n",
        "\n",
        "# Visualization functions (repeated for independence)\n",
        "colors = sns.color_palette(\"deep\")\n",
        "blue = colors[0]\n",
        "green = colors[2]\n",
        "\n",
        "def reshape_to_subgrid(data):\n",
        "    grid = np.zeros((sub_grid_size, sub_grid_size))\n",
        "    for ri in range(region_grid_size):\n",
        "        for rj in range(region_grid_size):\n",
        "            reg_idx = ri * region_grid_size + rj\n",
        "            sub_data = data[reg_idx].reshape(sub_per_region_side, sub_per_region_side)\n",
        "            grid[ri*sub_per_region_side:(ri+1)*sub_per_region_side, rj*sub_per_region_side:(rj+1)*sub_per_region_side] = sub_data\n",
        "    return grid\n",
        "\n",
        "def base_heatmap(grid, cmap, title, vmin=None, vmax=None, figsize=(10,10)):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    sns.heatmap(grid, ax=ax, cmap=cmap, vmin=vmin, vmax=vmax, square=True, linewidths=0, cbar_kws={'shrink': 0.5})\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_title(title, fontsize=20)\n",
        "    ax.tick_params(labelsize=14)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_outcome_sub_est(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values  # 100x9\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    cmap = sns.light_palette(blue, as_cmap=True)\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path), vmin=0, vmax=10)\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_confounder_est(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values  # 100x9\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    cmap = sns.light_palette(green, as_cmap=True)\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path), vmin=0, vmax=1)\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "# Visualize estimated files\n",
        "num_months_new = 3\n",
        "est_sub_files = [f'{data_folder}/outcome_subregion_estimated/outcome_subregion_estimated_{month:02d}.csv' for month in range(1, num_months_new + 1)]\n",
        "for file in est_sub_files:\n",
        "    plot_outcome_sub_est(file)\n",
        "plot_confounder_est(f'{data_folder}/unobserved_confounder_context_subregion_estimated.csv')"
      ],
      "metadata": {
        "id": "36i3kOKCBGCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Parameters\n",
        "data_folder = 'data_exp3'\n",
        "num_months = 48\n",
        "num_regions = 100\n",
        "num_subregions_per_region = 9\n",
        "sub_grid_size = 30\n",
        "region_grid_size = 10\n",
        "sub_per_region_side = 3\n",
        "learning_rate = 0.0001\n",
        "epochs = 10000\n",
        "\n",
        "# Load data\n",
        "interv_reg_list = [\n",
        "    pd.read_csv(f'{data_folder}/interventions_region/interventions_region_{month:02d}.csv', header=None).values.squeeze()\n",
        "    for month in range(1, num_months + 1)\n",
        "]\n",
        "context_list = [\n",
        "    pd.read_csv(f'{data_folder}/context_subregion/context_subregion_{month:02d}.csv', header=None).values\n",
        "    for month in range(1, num_months + 1)\n",
        "]\n",
        "outcome_reg_list = [\n",
        "    pd.read_csv(f'{data_folder}/outcome_region/outcome_region_{month:02d}.csv', header=None).values.squeeze()\n",
        "    for month in range(1, num_months + 1)\n",
        "]\n",
        "\n",
        "gt_veg = pd.read_csv(f'{data_folder}/unobserved_confounder_context_subregion.csv', header=None).values\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(4, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1)\n",
        "        )\n",
        "        self.veg = nn.Parameter(torch.randn(num_regions, num_subregions_per_region))\n",
        "\n",
        "# Training\n",
        "model = Model()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "losses = []\n",
        "veg_mses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    perm = np.random.permutation(num_months)\n",
        "    epoch_loss = 0.0\n",
        "    for i in perm:\n",
        "        parent_edu = torch.tensor(context_list[i], dtype=torch.long)\n",
        "        interv = torch.tensor(interv_reg_list[i], dtype=torch.float32)\n",
        "        gt = torch.tensor(outcome_reg_list[i], dtype=torch.float32)\n",
        "\n",
        "        onehot = F.one_hot(parent_edu - 1, num_classes=3).float()  # 100x9x3\n",
        "        interv_sub = interv.unsqueeze(1).repeat(1, num_subregions_per_region)  # 100x9\n",
        "        inputs = torch.cat([onehot, interv_sub.unsqueeze(-1)], dim=-1)  # 100x9x4\n",
        "        flat = inputs.view(-1, 4)  # 900x4\n",
        "        pred_sub_base = model.mlp(flat).view(num_regions, num_subregions_per_region)  # 100x9\n",
        "        pred_sub = pred_sub_base * (1 - torch.sigmoid(model.veg))\n",
        "        pred_reg = pred_sub.mean(dim=1)  # 100\n",
        "\n",
        "        loss = F.mse_loss(pred_reg, gt)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    losses.append(epoch_loss / num_months)\n",
        "    veg_est = torch.sigmoid(model.veg).detach().numpy()\n",
        "    veg_mse = np.mean((gt_veg - veg_est)**2)\n",
        "    veg_mses.append(veg_mse)\n",
        "    if (epoch + 1) % (epochs // 100) == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}: Avg Loss = {losses[-1]:.6f}, Veg MSE = {veg_mse:.6f}\")\n",
        "    row = pd.DataFrame({'epoch': [epoch+1], 'loss': [losses[-1]], 'veg_mse': [veg_mses[-1]]})\n",
        "    if epoch == 0:\n",
        "        row.to_csv(f'{data_folder}/training_curves_alt.csv', index=False)\n",
        "    else:\n",
        "        row.to_csv(f'{data_folder}/training_curves_alt.csv', mode='a', header=False, index=False)\n",
        "\n",
        "print(\"last epoch loss:\", epoch_loss)\n",
        "# Save loss curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses)\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.savefig(f'{data_folder}/loss_curve_alt.jpg')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(veg_mses)\n",
        "plt.title('Vegetation MSE Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.savefig(f'{data_folder}/veg_mse_curve_alt.jpg')\n",
        "plt.show()\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), f'{data_folder}/model_alt.pth')\n",
        "\n",
        "# Compute and save estimated subregion outcomes (no noise)\n",
        "os.makedirs(f'{data_folder}/outcome_subregion_estimated_alt', exist_ok=True)\n",
        "estimated_sub_outcomes = []\n",
        "with torch.no_grad():\n",
        "    for month in range(num_months):\n",
        "        parent_edu = torch.tensor(context_list[month], dtype=torch.long)\n",
        "        interv = torch.tensor(interv_reg_list[month], dtype=torch.float32)\n",
        "        onehot = F.one_hot(parent_edu - 1, num_classes=3).float()\n",
        "        interv_sub = interv.unsqueeze(1).repeat(1, num_subregions_per_region)\n",
        "        inputs = torch.cat([onehot, interv_sub.unsqueeze(-1)], dim=-1)\n",
        "        flat = inputs.view(-1, 4)\n",
        "        pred_sub_base = model.mlp(flat).view(num_regions, num_subregions_per_region)\n",
        "        pred_sub = pred_sub_base * (1 - torch.sigmoid(model.veg))\n",
        "        pred_sub = pred_sub.numpy()\n",
        "        estimated_sub_outcomes.append(pred_sub)\n",
        "        pd.DataFrame(pred_sub).to_csv(\n",
        "            f'{data_folder}/outcome_subregion_estimated_alt/outcome_subregion_estimated_{month+1:02d}.csv',\n",
        "            index=False, header=False\n",
        "        )\n",
        "\n",
        "# Save estimated vegetation\n",
        "estimated_veg = torch.sigmoid(model.veg).detach().numpy()\n",
        "pd.DataFrame(estimated_veg).to_csv(\n",
        "    f'{data_folder}/unobserved_confounder_context_subregion_estimated_alt.csv', index=False, header=False\n",
        ")\n",
        "\n",
        "# Visualization functions (repeated for independence)\n",
        "colors = sns.color_palette(\"deep\")\n",
        "blue = colors[0]\n",
        "green = colors[2]\n",
        "\n",
        "def reshape_to_subgrid(data):\n",
        "    grid = np.zeros((sub_grid_size, sub_grid_size))\n",
        "    for ri in range(region_grid_size):\n",
        "        for rj in range(region_grid_size):\n",
        "            reg_idx = ri * region_grid_size + rj\n",
        "            sub_data = data[reg_idx].reshape(sub_per_region_side, sub_per_region_side)\n",
        "            grid[ri*sub_per_region_side:(ri+1)*sub_per_region_side, rj*sub_per_region_side:(rj+1)*sub_per_region_side] = sub_data\n",
        "    return grid\n",
        "\n",
        "def base_heatmap(grid, cmap, title, vmin=None, vmax=None, figsize=(10,10)):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    sns.heatmap(grid, ax=ax, cmap=cmap, vmin=vmin, vmax=vmax, square=True, linewidths=0, cbar_kws={'shrink': 0.5})\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_title(title, fontsize=20)\n",
        "    ax.tick_params(labelsize=14)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_outcome_sub_est(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values  # 100x9\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    cmap = sns.light_palette(blue, as_cmap=True)\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path), vmin=0, vmax=10)\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_confounder_est(file_path):\n",
        "    data = pd.read_csv(file_path, header=None).values  # 100x9\n",
        "    grid = reshape_to_subgrid(data)\n",
        "    cmap = sns.light_palette(green, as_cmap=True)\n",
        "    fig = base_heatmap(grid, cmap, os.path.basename(file_path), vmin=0, vmax=1)\n",
        "    fig.savefig(f'{file_path[:-4]}.jpg')\n",
        "    fig.savefig(f'{file_path[:-4]}.pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "# Visualize estimated files\n",
        "num_months_new = 3\n",
        "est_sub_files = [f'{data_folder}/outcome_subregion_estimated_alt/outcome_subregion_estimated_{month:02d}.csv' for week in range(1, num_months_new + 1)]\n",
        "for file in est_sub_files:\n",
        "    plot_outcome_sub_est(file)\n",
        "plot_confounder_est(f'{data_folder}/unobserved_confounder_context_subregion_estimated_alt.csv')"
      ],
      "metadata": {
        "id": "yBbgunTwFRr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Toggles for log scales\n",
        "log_x = True\n",
        "log_y = True\n",
        "\n",
        "xlim = None\n",
        "ylim = None\n",
        "\n",
        "smoothing = 0\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'font.size': 24,  # Very large text\n",
        "    'axes.titlesize': 28,\n",
        "    'axes.labelsize': 26,\n",
        "    'legend.fontsize': 24,\n",
        "    'xtick.labelsize': 22,\n",
        "    'ytick.labelsize': 22,\n",
        "    'lines.linewidth': 3,\n",
        "    'figure.figsize': (12, 8),\n",
        "    'axes.grid': False,  # No grid clutter\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "})\n",
        "\n",
        "# Load data\n",
        "df_regular = pd.read_csv('data_exp3/training_curves.csv')\n",
        "\n",
        "# Apply smoothing to losses if applicable\n",
        "if smoothing > 0:\n",
        "    regular_loss = df_regular['loss'].rolling(smoothing, min_periods=1).mean()\n",
        "else:\n",
        "    regular_loss = df_regular['loss']\n",
        "\n",
        "# Colors from sns deep palette\n",
        "colors = sns.color_palette(\"deep\")\n",
        "blue = colors[0]\n",
        "red = colors[3]\n",
        "\n",
        "\n",
        "# Linear case plot with twin axes\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.plot(df_regular['epoch'], regular_loss, color=blue, linestyle='-', label='Loss', alpha=0.9, lw=3)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Training Loss', color=blue)\n",
        "ax1.tick_params(axis='both', which='both', length=0, labelsize=22, colors=blue)\n",
        "ax1.spines['top'].set_visible(False)\n",
        "ax1.spines['right'].set_visible(False)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(df_regular['epoch'], df_regular['veg_mse'], color=red, linestyle='--', label='Veg MSE', lw=5, alpha=0.9)\n",
        "ax2.set_ylabel('MSE Hidden Confounder', color=red)\n",
        "ax2.tick_params(axis='y', which='both', length=0, labelsize=22, colors=red)\n",
        "ax2.spines['top'].set_visible(False)\n",
        "ax2.spines['right'].set_visible(True)\n",
        "ax2.yaxis.set_major_locator(plt.MaxNLocator(nbins=5))\n",
        "\n",
        "if log_x:\n",
        "    ax1.set_xscale('log')\n",
        "if log_y:\n",
        "    ax1.set_yscale('log')\n",
        "    ax2.set_yscale('log')\n",
        "if xlim is not None:\n",
        "    ax1.set_xlim(xlim)\n",
        "if ylim is not None:\n",
        "    ax1.set_ylim(ylim)\n",
        "    ax2.set_ylim(ylim)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('data_exp3/linear_plot.jpg', dpi=300)\n",
        "fig.savefig('data_exp3/linear_plot.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qF6blLzmJYaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Toggles for log scales\n",
        "log_x = True\n",
        "log_y = True\n",
        "\n",
        "# Custom ranges (set to None for automatic)\n",
        "xlim = None  # Example: (1, 10000)\n",
        "ylim = None  # Example: (1e-5, 1)\n",
        "\n",
        "# Smoothing parameter for loss (window size, 0 = no smoothing)\n",
        "smoothing = 0\n",
        "\n",
        "# Set publication-ready style\n",
        "plt.rcParams.update({\n",
        "    'font.size': 24,  # Very large text\n",
        "    'axes.titlesize': 28,\n",
        "    'axes.labelsize': 26,\n",
        "    'legend.fontsize': 24,\n",
        "    'xtick.labelsize': 22,\n",
        "    'ytick.labelsize': 22,\n",
        "    'lines.linewidth': 3,\n",
        "    'figure.figsize': (12, 8),\n",
        "    'axes.grid': False,  # No grid clutter\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "})\n",
        "\n",
        "# Load data\n",
        "df_regular = pd.read_csv('data_exp3/training_curves_alt.csv')\n",
        "\n",
        "# Apply smoothing to losses if applicable\n",
        "if smoothing > 0:\n",
        "    regular_loss = df_regular['loss'].rolling(smoothing, min_periods=1).mean()\n",
        "else:\n",
        "    regular_loss = df_regular['loss']\n",
        "\n",
        "# Colors from sns deep palette\n",
        "colors = sns.color_palette(\"deep\")\n",
        "blue = colors[0]\n",
        "red = colors[3]\n",
        "\n",
        "#  #alpha=0.9, ls='--', lw=5), alpha=0.7, ls='-', lw=2)\n",
        "\n",
        "# Linear case plot with twin axes\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.plot(df_regular['epoch'], regular_loss, color=blue, linestyle='-', label='Loss', alpha=0.9, lw=3)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Training Loss', color=blue)\n",
        "ax1.tick_params(axis='both', which='both', length=0, labelsize=22, colors=blue)\n",
        "ax1.spines['top'].set_visible(False)\n",
        "ax1.spines['right'].set_visible(False)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(df_regular['epoch'], df_regular['veg_mse'], color=red, linestyle='--', label='Veg MSE', lw=5, alpha=0.9)\n",
        "ax2.set_ylabel('MSE Hidden Confounder', color=red)\n",
        "ax2.tick_params(axis='y', which='both', length=0, labelsize=22, colors=red)\n",
        "ax2.spines['top'].set_visible(False)\n",
        "ax2.spines['right'].set_visible(True)\n",
        "ax2.yaxis.set_major_locator(plt.MaxNLocator(nbins=5))\n",
        "\n",
        "if log_x:\n",
        "    ax1.set_xscale('log')\n",
        "if log_y:\n",
        "    ax1.set_yscale('log')\n",
        "    ax2.set_yscale('log')\n",
        "if xlim is not None:\n",
        "    ax1.set_xlim(xlim)\n",
        "if ylim is not None:\n",
        "    ax1.set_ylim(ylim)\n",
        "    ax2.set_ylim(ylim)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('data_exp3/alt_plot.jpg', dpi=300)\n",
        "fig.savefig('data_exp3/alt_plot.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZRHy_ONoJh19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "\n",
        "# Load true and estimated confounder data\n",
        "gt_veg = pd.read_csv(f'{data_folder}/unobserved_confounder_context_subregion.csv', header=None).values\n",
        "estimated_veg = pd.read_csv(f'{data_folder}/unobserved_confounder_context_subregion_estimated_alt.csv', header=None).values\n",
        "\n",
        "# Reshape data for plotting\n",
        "gt_veg_grid = reshape_to_subgrid(gt_veg)\n",
        "estimated_veg_grid = reshape_to_subgrid(estimated_veg)\n",
        "\n",
        "# Define shared colormap and range for confounder plots\n",
        "green_cmap = sns.light_palette(colors[2], as_cmap=True)\n",
        "vmin_veg = np.min([gt_veg_grid, estimated_veg_grid])\n",
        "vmax_veg = np.max([gt_veg_grid, estimated_veg_grid])\n",
        "\n",
        "# Create figure and axes with space for the colorbar for confounder plots\n",
        "fig_veg, axs_veg = plt.subplots(1, 2, figsize=(12, 6), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.05})\n",
        "\n",
        "# Plot true confounder\n",
        "sns.heatmap(gt_veg_grid, ax=axs_veg[0], cmap=green_cmap, cbar=False, square=True,\n",
        "            vmin=vmin_veg, vmax=vmax_veg, linewidths=0)\n",
        "# axs_veg[0].set_title(\"True Hidden Confounder (Vegetation)\")\n",
        "axs_veg[0].set_xticks([])\n",
        "axs_veg[0].set_yticks([])\n",
        "\n",
        "# Plot estimated confounder\n",
        "sns.heatmap(estimated_veg_grid, ax=axs_veg[1], cmap=green_cmap, cbar=False, square=True,\n",
        "            vmin=vmin_veg, vmax=vmax_veg, linewidths=0)\n",
        "# axs_veg[1].set_title(\"Estimated Hidden Confounder (Vegetation)\")\n",
        "axs_veg[1].set_xticks([])\n",
        "axs_veg[1].set_yticks([])\n",
        "\n",
        "# Add a single shared vertical colorbar to the right for confounder plots\n",
        "norm_veg = mpl.colors.Normalize(vmin=vmin_veg, vmax=vmax_veg)\n",
        "sm_veg = mpl.cm.ScalarMappable(cmap=green_cmap, norm=norm_veg)\n",
        "sm_veg.set_array([])\n",
        "\n",
        "cbar_veg = fig_veg.colorbar(sm_veg, ax=axs_veg, orientation='vertical', fraction=0.046, pad=0.04)\n",
        "cbar_veg.set_label(\"Vegetation Value\")\n",
        "\n",
        "# Save and show confounder plots\n",
        "fig_veg.savefig(f'{data_folder}/vegetation_heatmaps.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig_veg.savefig(f'{data_folder}/vegetation_heatmaps_alt.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "print(\" Hidden confounder heatmaps generated.\")\n",
        "\n",
        "# Calculate and visualize the error between true and estimated confounder\n",
        "vegetation_error = np.abs(gt_veg_grid - estimated_veg_grid)\n",
        "fig_veg_error, ax_veg_error = plt.subplots(figsize=(6, 6))\n",
        "error_cmap = sns.light_palette(\"red\", as_cmap=True) # Use a different color for error\n",
        "sns.heatmap(vegetation_error, ax=ax_veg_error, cmap=error_cmap, square=True, linewidths=0)\n",
        "# ax_veg_error.set_title(\"Absolute Error in Vegetation Estimation\")\n",
        "ax_veg_error.set_xticks([])\n",
        "ax_veg_error.set_yticks([])\n",
        "plt.tight_layout()\n",
        "fig_veg_error.savefig(f'{data_folder}/vegetation_error_heatmap.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig_veg_error.savefig(f'{data_folder}/vegetation_error_heatmap_alt.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "print(\" Vegetation error heatmap generated.\")\n",
        "\n",
        "\n",
        "# Calculate true causal effect\n",
        "context_week1 = pd.read_csv(f'{data_folder}/context_subregion/context_subregion_01.csv', header=None).values\n",
        "poor_indicator_week1 = (context_week1 == 1).astype(float)\n",
        "intermediate_indicator_week1 = (context_week1 == 2).astype(float)\n",
        "rich_indicator_week1 = (context_week1 == 3).astype(float)\n",
        "true_causal_effect = ((10 * poor_indicator_week1) + (5 * intermediate_indicator_week1) + (rich_indicator_week1)) * (1 - gt_veg)\n",
        "\n",
        "\n",
        "# Calculate estimated causal effect using the trained model\n",
        "# We need to load the trained model weights\n",
        "model_alt = Model() # Assuming the alternative model is used for causal effect estimation based on previous plots\n",
        "model_alt.load_state_dict(torch.load(f'{data_folder}/model_alt.pth'))\n",
        "model_alt.eval() # Set model to evaluation mode\n",
        "\n",
        "estimated_causal_effect = np.zeros((num_regions, num_subregions_per_region))\n",
        "with torch.no_grad():\n",
        "    # Assuming we use the context from the first week (or an average/representative week)\n",
        "    wealth_week1 = torch.tensor(context_week1, dtype=torch.long)\n",
        "    onehot_week1 = F.one_hot(wealth_week1 - 1, num_classes=3).float() # 100x9x3\n",
        "    estimated_veg_tensor = torch.sigmoid(model_alt.veg) # 100x9\n",
        "\n",
        "    # Predict outcome when intervention is 1\n",
        "    interv_sub_1 = torch.ones(num_regions, num_subregions_per_region) # 100x9\n",
        "    inputs_1 = torch.cat([onehot_week1, interv_sub_1.unsqueeze(-1)], dim=-1) # 100x9x4\n",
        "    flat_1 = inputs_1.view(-1, 4) # 900x4\n",
        "    pred_sub_base_1 = model_alt.mlp(flat_1).view(num_regions, num_subregions_per_region) # 100x9\n",
        "    pred_sub_1 = pred_sub_base_1 * (1 - estimated_veg_tensor)\n",
        "\n",
        "    # Predict outcome when intervention is 0\n",
        "    interv_sub_0 = torch.zeros(num_regions, num_subregions_per_region) # 100x9\n",
        "    inputs_0 = torch.cat([onehot_week1, interv_sub_0.unsqueeze(-1)], dim=-1) # 100x9x4\n",
        "    flat_0 = inputs_0.view(-1, 4) # 900x4\n",
        "    pred_sub_base_0 = model_alt.mlp(flat_0).view(num_regions, num_subregions_per_region) # 100x9\n",
        "    pred_sub_0 = pred_sub_base_0 * (1 - estimated_veg_tensor)\n",
        "\n",
        "    # Estimated causal effect = Predicted outcome with intervention 1 - Predicted outcome with intervention 0\n",
        "    estimated_causal_effect = (pred_sub_1 - pred_sub_0).numpy()\n",
        "\n",
        "\n",
        "# Reshape causal effect data for plotting\n",
        "causal_effect_true_grid = reshape_to_subgrid(true_causal_effect)\n",
        "estimated_causal_effect_grid = reshape_to_subgrid(estimated_causal_effect)\n",
        "\n",
        "\n",
        "# Define shared colormap and range for causal effect\n",
        "colors = sns.color_palette(\"deep\")\n",
        "pink_cmap = sns.light_palette(colors[4], as_cmap=True)\n",
        "vmin = np.min([causal_effect_true_grid, estimated_causal_effect_grid])\n",
        "vmax = np.max([causal_effect_true_grid, estimated_causal_effect_grid])\n",
        "\n",
        "# Create figure and axes with space for the colorbar\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.05})\n",
        "\n",
        "# Plot true causal effect\n",
        "sns.heatmap(causal_effect_true_grid, ax=axs[0], cmap=pink_cmap, cbar=False, square=True,\n",
        "            vmin=vmin, vmax=vmax, linewidths=0)\n",
        "axs[0].set_title(\"True Causal Effect (Subregion)\")\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "# Plot estimated causal effect\n",
        "sns.heatmap(estimated_causal_effect_grid, ax=axs[1], cmap=pink_cmap, cbar=False, square=True,\n",
        "            vmin=vmin, vmax=vmax, linewidths=0)\n",
        "axs[1].set_title(\"Estimated Causal Effect (Subregion)\")\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "# Add a single shared vertical colorbar to the right\n",
        "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
        "sm = mpl.cm.ScalarMappable(cmap=pink_cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "\n",
        "# Add colorbar to the right of both plots\n",
        "cbar = fig.colorbar(sm, ax=axs, orientation='vertical', fraction=0.046, pad=0.04)\n",
        "cbar.set_label(\"Causal Effect\") # Removed (%) as it's not normalized to percentage\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(f'{data_folder}/subregion_causal_effects_heatmaps.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(f'{data_folder}/subregion_causal_effects_heatmaps.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "print(\" Subregion causal effect plots generated.\")"
      ],
      "metadata": {
        "id": "0pgpre-UTxF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "\n",
        "# Load true and estimated confounder data\n",
        "gt_veg = pd.read_csv(f'{data_folder}/unobserved_confounder_context_subregion.csv', header=None).values\n",
        "estimated_veg = pd.read_csv(f'{data_folder}/unobserved_confounder_context_subregion_estimated.csv', header=None).values\n",
        "\n",
        "# Reshape data for plotting\n",
        "gt_veg_grid = reshape_to_subgrid(gt_veg)\n",
        "estimated_veg_grid = reshape_to_subgrid(estimated_veg)\n",
        "\n",
        "# Define shared colormap and range for confounder plots\n",
        "green_cmap = sns.light_palette(colors[2], as_cmap=True)\n",
        "vmin_veg = np.min([gt_veg_grid, estimated_veg_grid])\n",
        "vmax_veg = np.max([gt_veg_grid, estimated_veg_grid])\n",
        "\n",
        "# Create figure and axes with space for the colorbar for confounder plots\n",
        "fig_veg, axs_veg = plt.subplots(1, 2, figsize=(12, 6), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.05})\n",
        "\n",
        "# Plot true confounder\n",
        "sns.heatmap(gt_veg_grid, ax=axs_veg[0], cmap=green_cmap, cbar=False, square=True,\n",
        "            vmin=vmin_veg, vmax=vmax_veg, linewidths=0)\n",
        "# axs_veg[0].set_title(\"True Hidden Confounder (Vegetation)\")\n",
        "axs_veg[0].set_xticks([])\n",
        "axs_veg[0].set_yticks([])\n",
        "\n",
        "# Plot estimated confounder\n",
        "sns.heatmap(estimated_veg_grid, ax=axs_veg[1], cmap=green_cmap, cbar=False, square=True,\n",
        "            vmin=vmin_veg, vmax=vmax_veg, linewidths=0)\n",
        "# axs_veg[1].set_title(\"Estimated Hidden Confounder (Vegetation)\")\n",
        "axs_veg[1].set_xticks([])\n",
        "axs_veg[1].set_yticks([])\n",
        "\n",
        "# Add a single shared vertical colorbar to the right for confounder plots\n",
        "norm_veg = mpl.colors.Normalize(vmin=vmin_veg, vmax=vmax_veg)\n",
        "sm_veg = mpl.cm.ScalarMappable(cmap=green_cmap, norm=norm_veg)\n",
        "sm_veg.set_array([])\n",
        "\n",
        "cbar_veg = fig_veg.colorbar(sm_veg, ax=axs_veg, orientation='vertical', fraction=0.046, pad=0.04)\n",
        "cbar_veg.set_label(\"Vegetation Value\")\n",
        "\n",
        "# Save and show confounder plots\n",
        "fig_veg.savefig(f'{data_folder}/vegetation_heatmaps.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig_veg.savefig(f'{data_folder}/vegetation_heatmaps.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "print(\" Hidden confounder heatmaps generated.\")\n",
        "\n",
        "# Calculate and visualize the error between true and estimated confounder\n",
        "vegetation_error = np.abs(gt_veg_grid - estimated_veg_grid)\n",
        "fig_veg_error, ax_veg_error = plt.subplots(figsize=(6, 6))\n",
        "error_cmap = sns.light_palette(\"red\", as_cmap=True) # Use a different color for error\n",
        "sns.heatmap(vegetation_error, ax=ax_veg_error, cmap=error_cmap, square=True, linewidths=0)\n",
        "# ax_veg_error.set_title(\"Absolute Error in Vegetation Estimation\")\n",
        "ax_veg_error.set_xticks([])\n",
        "ax_veg_error.set_yticks([])\n",
        "plt.tight_layout()\n",
        "fig_veg_error.savefig(f'{data_folder}/vegetation_error_heatmap.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig_veg_error.savefig(f'{data_folder}/vegetation_error_heatmap.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "print(\" Vegetation error heatmap generated.\")\n",
        "\n",
        "\n",
        "# Calculate true causal effect\n",
        "context_week1 = pd.read_csv(f'{data_folder}/context_subregion/context_subregion_01.csv', header=None).values\n",
        "poor_indicator_week1 = (context_week1 == 1).astype(float)\n",
        "intermediate_indicator_week1 = (context_week1 == 2).astype(float)\n",
        "rich_indicator_week1 = (context_week1 == 3).astype(float)\n",
        "true_causal_effect = ((10 * poor_indicator_week1) + (5 * intermediate_indicator_week1) + (rich_indicator_week1)) * (1 - gt_veg)\n",
        "\n",
        "\n",
        "# Calculate estimated causal effect using the trained model\n",
        "model_alt = Model() # Assuming the alternative model is used for causal effect estimation based on previous plots\n",
        "model_alt.load_state_dict(torch.load(f'{data_folder}/model_alt.pth'))\n",
        "model_alt.eval() # Set model to evaluation mode\n",
        "\n",
        "estimated_causal_effect = np.zeros((num_regions, num_subregions_per_region))\n",
        "with torch.no_grad():\n",
        "    # Assuming we use the context from the first week (or an average/representative week)\n",
        "    wealth_week1 = torch.tensor(context_week1, dtype=torch.long)\n",
        "    onehot_week1 = F.one_hot(wealth_week1 - 1, num_classes=3).float() # 100x9x3\n",
        "    estimated_veg_tensor = torch.sigmoid(model_alt.veg) # 100x9\n",
        "\n",
        "    # Predict outcome when intervention is 1\n",
        "    interv_sub_1 = torch.ones(num_regions, num_subregions_per_region) # 100x9\n",
        "    inputs_1 = torch.cat([onehot_week1, interv_sub_1.unsqueeze(-1)], dim=-1) # 100x9x4\n",
        "    flat_1 = inputs_1.view(-1, 4) # 900x4\n",
        "    pred_sub_base_1 = model_alt.mlp(flat_1).view(num_regions, num_subregions_per_region) # 100x9\n",
        "    pred_sub_1 = pred_sub_base_1 * (1 - estimated_veg_tensor)\n",
        "\n",
        "    # Predict outcome when intervention is 0\n",
        "    interv_sub_0 = torch.zeros(num_regions, num_subregions_per_region) # 100x9\n",
        "    inputs_0 = torch.cat([onehot_week1, interv_sub_0.unsqueeze(-1)], dim=-1) # 100x9x4\n",
        "    flat_0 = inputs_0.view(-1, 4) # 900x4\n",
        "    pred_sub_base_0 = model_alt.mlp(flat_0).view(num_regions, num_subregions_per_region) # 100x9\n",
        "    pred_sub_0 = pred_sub_base_0 * (1 - estimated_veg_tensor)\n",
        "\n",
        "    # Estimated causal effect = Predicted outcome with intervention 1 - Predicted outcome with intervention 0\n",
        "    estimated_causal_effect = (pred_sub_1 - pred_sub_0).numpy()\n",
        "\n",
        "\n",
        "# Reshape causal effect data for plotting\n",
        "causal_effect_true_grid = reshape_to_subgrid(true_causal_effect)\n",
        "estimated_causal_effect_grid = reshape_to_subgrid(estimated_causal_effect)\n",
        "\n",
        "\n",
        "# Define shared colormap and range for causal effect\n",
        "colors = sns.color_palette(\"deep\")\n",
        "pink_cmap = sns.light_palette(colors[4], as_cmap=True)\n",
        "vmin = np.min([causal_effect_true_grid, estimated_causal_effect_grid])\n",
        "vmax = np.max([causal_effect_true_grid, estimated_causal_effect_grid])\n",
        "\n",
        "# Create figure and axes with space for the colorbar\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.05})\n",
        "\n",
        "# Plot true causal effect\n",
        "sns.heatmap(causal_effect_true_grid, ax=axs[0], cmap=pink_cmap, cbar=False, square=True,\n",
        "            vmin=vmin, vmax=vmax, linewidths=0)\n",
        "axs[0].set_title(\"True Causal Effect (Subregion)\")\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "# Plot estimated causal effect\n",
        "sns.heatmap(estimated_causal_effect_grid, ax=axs[1], cmap=pink_cmap, cbar=False, square=True,\n",
        "            vmin=vmin, vmax=vmax, linewidths=0)\n",
        "axs[1].set_title(\"Estimated Causal Effect (Subregion)\")\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "# Add a single shared vertical colorbar to the right\n",
        "# Use a ScalarMappable for correct scale\n",
        "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
        "sm = mpl.cm.ScalarMappable(cmap=pink_cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "\n",
        "# Add colorbar to the right of both plots\n",
        "cbar = fig.colorbar(sm, ax=axs, orientation='vertical', fraction=0.046, pad=0.04)\n",
        "cbar.set_label(\"Causal Effect\") # Removed (%) as it's not normalized to percentage\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(f'{data_folder}/subregion_causal_effects_heatmaps.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(f'{data_folder}/subregion_causal_effects_heatmaps.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "print(\" Subregion causal effect plots generated.\")"
      ],
      "metadata": {
        "id": "pzLZWoEpT-LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp. 4\n",
        "Confounded Treatment Allocation: Estimating treatment locations and causal effect when treatment assignment depends on contextual covariates."
      ],
      "metadata": {
        "id": "GtY6iHiFIUcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exp. 4 Low Confounding"
      ],
      "metadata": {
        "id": "Jpwk0hqPU0Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#with wealth based intervention allocation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "shift_param = 1.23\n",
        "scale_param = -4.2\n",
        "noise_var = 0.01\n",
        "data_dir = 'data_exp4_lc'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1) Context (wealth) subregion\n",
        "context_sub = np.random.uniform(0, 1, (num_regions, num_subregions))\n",
        "pd.DataFrame(context_sub).to_csv(os.path.join(data_dir, 'context_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# 2) Interventions subregion (only one per region, using softmax over context)\n",
        "def softmax(x, tau=1):\n",
        "    x = x / tau  # temperature scaling\n",
        "    e_x = np.exp(x - np.max(x))  # numerical stability\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "interventions_sub = np.zeros((num_regions, num_subregions))\n",
        "for i in range(num_regions):\n",
        "    probs = softmax(context_sub[i], tau=1)  # smaller tau = sharper preferences for high context\n",
        "    col_idx = np.random.choice(np.arange(num_subregions), p=probs)\n",
        "    interventions_sub[i, col_idx] = np.random.uniform(0.2, 1)\n",
        "pd.DataFrame(interventions_sub).to_csv(os.path.join(data_dir, 'interventions_subregion.csv'), index=False, header=False)\n",
        "\n",
        "\n",
        "# Interventions region: sum of subregion interventions\n",
        "interventions_reg = interventions_sub.sum(axis=1)\n",
        "pd.DataFrame(interventions_reg).to_csv(os.path.join(data_dir, 'interventions_region.csv'), index=False, header=False)\n",
        "\n",
        "# 3) Noise subregion\n",
        "noise_sub = np.random.normal(0, np.sqrt(noise_var), size=(num_regions, num_subregions))\n",
        "pd.DataFrame(noise_sub).to_csv(os.path.join(data_dir, 'noise_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# 4) Outcome subregion: context-dependent + noise\n",
        "outcome_sub = (shift_param - context_sub) * scale_param * interventions_sub + noise_sub\n",
        "pd.DataFrame(outcome_sub).to_csv(os.path.join(data_dir, 'outcome_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# Outcome region: average outcome across subregions\n",
        "outcome_reg = outcome_sub.mean(axis=1)\n",
        "pd.DataFrame(outcome_reg).to_csv(os.path.join(data_dir, 'outcome_region.csv'), index=False, header=False)"
      ],
      "metadata": {
        "id": "kIRzkA7dIbT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp4_lc'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "grid_size_reg = 10  # 10x10 for regions\n",
        "grid_size_sub = 20  # 20x20 for subregions (2x2 per region)\n",
        "sns.set_context('paper', font_scale=1.5)\n",
        "colors = sns.color_palette(\"deep\")\n",
        "reds = colors[3]\n",
        "blues = colors[0]\n",
        "\n",
        "def visualize_subregion_csv(file_name, cmap, title, is_wealth=False, num_regions=100, num_subregions=4):\n",
        "    data = pd.read_csv(os.path.join(data_dir, file_name), header=None).values\n",
        "    # Reshape to subregion grid size: each region row becomes block of subregions\n",
        "    grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "    subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "    grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    for i in range(int(np.sqrt(num_regions))):\n",
        "        for j in range(int(np.sqrt(num_regions))):\n",
        "            region_idx = i * int(np.sqrt(num_regions)) + j\n",
        "            sub_data = data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = sub_data\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    if is_wealth:\n",
        "        heatmap_cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
        "    else:\n",
        "        if isinstance(cmap, tuple):\n",
        "            heatmap_cmap = sns.light_palette(cmap, as_cmap=True)\n",
        "        else:\n",
        "            heatmap_cmap = cmap\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=False, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title(title, fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)  # No legend box\n",
        "    plt.tight_layout()\n",
        "    base_name = file_name.replace('.csv', '')\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def visualize_region_csv(file_name, cmap, title, is_wealth=False, num_regions=100):\n",
        "    data = pd.read_csv(os.path.join(data_dir, file_name), header=None).values.flatten()\n",
        "    # Reshape to region grid size\n",
        "    grid_size_reg_calc = int(np.sqrt(num_regions))\n",
        "    grid = data.reshape(grid_size_reg_calc, grid_size_reg_calc)\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    if is_wealth:\n",
        "        heatmap_cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
        "    else:\n",
        "        if isinstance(cmap, tuple):\n",
        "            heatmap_cmap = sns.light_palette(cmap, as_cmap=True)\n",
        "        else:\n",
        "            heatmap_cmap = cmap\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=False, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title(title, fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)\n",
        "    plt.tight_layout()\n",
        "    base_name = file_name.replace('.csv', '')\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Visualize each\n",
        "visualize_subregion_csv('interventions_subregion.csv', reds, 'Interventions Subregion', is_wealth=False, num_regions=num_regions, num_subregions=num_subregions)\n",
        "visualize_region_csv('interventions_region.csv', reds, 'Interventions Region', is_wealth=False, num_regions=num_regions)\n",
        "visualize_subregion_csv('context_subregion.csv', None, 'Wealth Subregion', is_wealth=True, num_regions=num_regions, num_subregions=num_subregions)\n",
        "visualize_subregion_csv('noise_subregion.csv', 'coolwarm', 'Noise Subregion', is_wealth=False, num_regions=num_regions, num_subregions=num_subregions)  # Using coolwarm for noise as not specified\n",
        "visualize_subregion_csv('outcome_subregion.csv', blues, 'Outcome Subregion', is_wealth=False, num_regions=num_regions, num_subregions=num_subregions)\n",
        "visualize_region_csv('outcome_region.csv', blues, 'Outcome Region', is_wealth=False, num_regions=num_regions)\n",
        "\n",
        "# Wealth low-res: average context_sub to get wealth_reg\n",
        "context_sub = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values\n",
        "wealth_reg = np.mean(context_sub, axis=1)\n",
        "pd.DataFrame(wealth_reg).to_csv(os.path.join(data_dir, 'context_region.csv'), index=False, header=False)  # Temp save for viz\n",
        "visualize_region_csv('context_region.csv', None, 'Wealth Region', is_wealth=True, num_regions=num_regions)"
      ],
      "metadata": {
        "id": "_DlcYulUdhcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp4_lc'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "grid_size_reg = int(np.sqrt(num_regions))  # 10x10 for regions\n",
        "grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))  # 20x20 for subregions (2x2 per region)\n",
        "subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "\n",
        "\n",
        "# Color maps from seaborn deep palette\n",
        "colors = sns.color_palette(\"deep\")\n",
        "red_cmap = sns.light_palette(colors[3], as_cmap=True)  # Red\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue\n",
        "green_cmap = sns.light_palette(colors[2], as_cmap=True)  # Green\n",
        "\n",
        "# Parameter to control space between subplots (as fraction of average axis width)\n",
        "space_between = 0.05  # Adjust this value as needed (e.g., 0.0 for no space, 0.5 for more space)\n",
        "\n",
        "# Load data for combined\n",
        "interv_data = pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.flatten().reshape(grid_size_reg, grid_size_reg).astype(float)  # Assuming 0/1, float for heatmap\n",
        "outcome_data = pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.flatten().reshape(grid_size_reg, grid_size_reg)# / 100.0  # Assuming 0-100, normalize to 0-1\n",
        "context_data = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values\n",
        "context_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "for i in range(grid_size_reg):\n",
        "    for j in range(grid_size_reg):\n",
        "        region_idx = i * grid_size_reg + j\n",
        "        sub_data = context_data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "        context_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = sub_data\n",
        "context_norm = (context_grid - 1) / 2.0  # Normalize assuming values 1,2,3 to 0-1\n",
        "\n",
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Left: Intervention in red\n",
        "sns.heatmap(interv_data, ax=axs[0], cmap=red_cmap, cbar=False, square=True, linewidths=2)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "# Center: Outcome in blue\n",
        "sns.heatmap(outcome_data, ax=axs[1], cmap=blue_cmap, cbar=False, square=True, linewidths=2)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "# Right: Context in green\n",
        "sns.heatmap(context_norm, ax=axs[2], cmap=green_cmap, cbar=False, square=True, linewidths=1)#, vmin=0.0)#, vmax=1.0)\n",
        "axs[2].set_xticks([])\n",
        "axs[2].set_yticks([])#\n",
        "\n",
        "\n",
        "# Adjust the space between subplots\n",
        "fig.subplots_adjust(wspace=space_between)\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(os.path.join(data_dir, 'combined_heatmaps.jpg'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(os.path.join(data_dir, 'exp5_combined_heatmaps.pdf'), bbox_inches='tight', pad_inches=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FazcOnP-dkgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp4_lc'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "epochs = 10000\n",
        "lr = 0.001\n",
        "torch.manual_seed(42)\n",
        "grid_size_sub = 20\n",
        "sns.set_context('paper', font_scale=1.5)\n",
        "reds = sns.color_palette(\"deep\")[3]\n",
        "\n",
        "# Read all CSVs\n",
        "interventions_reg = torch.tensor(pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.flatten(), dtype=torch.float32)\n",
        "outcome_reg = torch.tensor(pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.flatten(), dtype=torch.float32)\n",
        "context_sub = torch.tensor(pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values, dtype=torch.float32)\n",
        "\n",
        "# Model\n",
        "class CausalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.inter_est = nn.Parameter(torch.randn(num_regions, num_subregions))\n",
        "        self.shift = nn.Parameter(torch.tensor(1.0))\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "        self.temp = torch.tensor(10.0)\n",
        "        self.noise_scale = 0.1\n",
        "\n",
        "    def preprocess_inter(self):\n",
        "        inter_pos = (self.inter_est + torch.randn_like(self.inter_est)*self.noise_scale) ** 2  # Ensure positivity\n",
        "        inter_sparse = torch.softmax(inter_pos / self.temp, dim=1)  # Softmax for differentiable near-one-hot sparsity\n",
        "        inter_constrained = inter_sparse * interventions_reg.unsqueeze(1)  # Multiply by known region sum\n",
        "        return inter_constrained\n",
        "\n",
        "    def forward(self):\n",
        "        inter_sub = self.preprocess_inter()\n",
        "        outcome_sub_pred = (self.shift - context_sub) * self.scale * inter_sub# + noise_sub\n",
        "        outcome_reg_pred = outcome_sub_pred.mean(dim=1)\n",
        "        return outcome_reg_pred\n",
        "\n",
        "model = CausalModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Train\n",
        "losses = []\n",
        "start_temp = 10.0\n",
        "end_temp = 2.0\n",
        "for epoch in range(epochs):\n",
        "    model.temp = torch.tensor(start_temp + (end_temp - start_temp) * (epoch / (epochs - 1)))\n",
        "    optimizer.zero_grad()\n",
        "    pred = model.forward()\n",
        "    loss = criterion(pred, outcome_reg)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "model.temp = 0.0001\n",
        "model.noise_scale = 0.0\n",
        "\n",
        "# Save loss curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses)\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curve.jpg'))\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curve.pdf'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Print final estimates\n",
        "print(f\"Final shift_param: {model.shift.item()}\")\n",
        "print(f\"Final scale_param: {model.scale.item()}\")\n",
        "\n",
        "# Save params to txt\n",
        "with open(os.path.join(data_dir, 'parameter_estimate.txt'), 'w') as f:\n",
        "    f.write(f\"shift_param: {model.shift.item()}\\n\")\n",
        "    f.write(f\"scale_param: {model.scale.item()}\\n\")\n",
        "\n",
        "# Save estimated interventions sub (processed)\n",
        "inter_est_final = model.preprocess_inter().detach().numpy()\n",
        "pd.DataFrame(inter_est_final).to_csv(os.path.join(data_dir, 'interventions_subregion_estimated.csv'), index=False, header=False)\n",
        "\n",
        "# Visualize estimated subregion\n",
        "def visualize_estimated_subregion():\n",
        "    data = inter_est_final\n",
        "    grid = np.zeros((grid_size_sub, grid_size_sub))\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            region_idx = i * 10 + j\n",
        "            sub_data = data[region_idx, :].reshape(2, 2)\n",
        "            grid[2*i:2*(i+1), 2*j:2*(j+1)] = sub_data\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    heatmap_cmap = sns.light_palette(reds, as_cmap=True)\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=False, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title('Estimated Interventions Subregion', fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, 'interventions_subregion_estimated.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, 'interventions_subregion_estimated_standard.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_estimated_subregion()"
      ],
      "metadata": {
        "id": "73FOU4EmdsLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate true causal effect at subregion level\n",
        "true_outcome_sub = (shift_param - context_sub) * scale_param * interventions_sub\n",
        "\n",
        "# Load estimated interventions\n",
        "inter_est_final = pd.read_csv(os.path.join(data_dir, 'interventions_subregion_estimated.csv'), header=None).values\n",
        "\n",
        "# Calculate predicted outcome at subregion level using estimated interventions and learned parameters\n",
        "# Need to load learned parameters from the text file\n",
        "with open(os.path.join(data_dir, 'parameter_estimate.txt'), 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    estimated_shift = float(lines[0].split(': ')[1])\n",
        "    estimated_scale = float(lines[1].split(': ')[1])\n",
        "\n",
        "# Ensure context_sub is a numpy array for the calculation\n",
        "if isinstance(context_sub, torch.Tensor):\n",
        "    context_sub_np = context_sub.numpy()\n",
        "else:\n",
        "    context_sub_np = context_sub\n",
        "\n",
        "\n",
        "predicted_outcome_sub = (estimated_shift - context_sub_np) * estimated_scale * inter_est_final\n",
        "\n",
        "# Visualize true vs predicted outcome at subregion level\n",
        "def visualize_true_vs_predicted_subregion(true_data, predicted_data, num_regions=100, num_subregions=4):\n",
        "    grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "    subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "\n",
        "    true_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    predicted_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    difference_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "\n",
        "    for i in range(int(np.sqrt(num_regions))):\n",
        "        for j in range(int(np.sqrt(num_regions))):\n",
        "            region_idx = i * int(np.sqrt(num_regions)) + j\n",
        "            true_sub_data = true_data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            predicted_sub_data = predicted_data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            true_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = true_sub_data\n",
        "            predicted_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = predicted_sub_data\n",
        "            difference_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = true_sub_data - predicted_sub_data\n",
        "\n",
        "    # Color maps from seaborn deep palette (assuming these are available in the notebook's global scope)\n",
        "    colors = sns.color_palette(\"deep\")\n",
        "    blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue for Outcome\n",
        "    coolwarm_cmap = 'coolwarm' # Using coolwarm for difference as it was used before\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5)) # Adjusted figure size\n",
        "\n",
        "\n",
        "    # True Outcome (Blue)\n",
        "    sns.heatmap(true_grid, ax=axs[0], cmap=blue_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[0].set_title('True Outcome Subregion')\n",
        "    axs[0].set_xticks([])\n",
        "    axs[0].set_yticks([])\n",
        "\n",
        "    # Predicted Outcome (Blue)\n",
        "    sns.heatmap(predicted_grid, ax=axs[1], cmap=blue_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[1].set_title('Predicted Outcome Subregion')\n",
        "    axs[1].set_xticks([])\n",
        "    axs[1].set_yticks([])\n",
        "\n",
        "    # Difference (Coolwarm)\n",
        "    sns.heatmap(difference_grid, ax=axs[2], cmap=coolwarm_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[2].set_title('Difference (True - Predicted)')\n",
        "    axs[2].set_xticks([])\n",
        "    axs[2].set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, 'true_vs_predicted_outcome_subregion_styled.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, 'standard_true_vs_predicted_outcome_subregion_styled.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_true_vs_predicted_subregion(true_outcome_sub, predicted_outcome_sub, num_regions, num_subregions)\n",
        "\n",
        "# Calculate MSE for true vs predicted subregion outcome\n",
        "mse_subregion = np.mean((np.asarray(true_outcome_sub) - np.asarray(predicted_outcome_sub)) ** 2)\n",
        "print(f\"MSE between true and predicted subregion outcome: {mse_subregion}\")"
      ],
      "metadata": {
        "id": "5h8_cYDWdy6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp4_lc'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "epochs = 10000\n",
        "lr = 0.01\n",
        "torch.manual_seed(42)\n",
        "grid_size_sub = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "sns.set_context('paper', font_scale=1.5)\n",
        "reds = sns.color_palette(\"deep\")[3]\n",
        "\n",
        "# Read all CSVs\n",
        "interventions_reg = torch.tensor(pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.flatten(), dtype=torch.float32)\n",
        "outcome_reg = torch.tensor(pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.flatten(), dtype=torch.float32)\n",
        "context_sub = torch.tensor(pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values, dtype=torch.float32)\n",
        "# context_reg is not directly used in the preprocess_inter in this version, but kept for consistency if needed elsewhere\n",
        "context_reg = torch.mean(context_sub, dim=1)\n",
        "\n",
        "# Model\n",
        "class CausalModelCombined(nn.Module):\n",
        "    def __init__(self, num_regions, num_subregions):\n",
        "        super().__init__()\n",
        "        self.shift = nn.Parameter(torch.tensor(1.0))\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "        # Make tau a learnable parameter\n",
        "        self.log_tau = nn.Parameter(torch.tensor(np.log(1.0))) # Initialize log_tau to 0 (tau=1)\n",
        "        self.num_regions = num_regions\n",
        "        self.num_subregions = num_subregions\n",
        "\n",
        "\n",
        "    def preprocess_inter(self, context_sub, interventions_reg):\n",
        "        tau = torch.exp(self.log_tau)\n",
        "        # Apply softmax over subregion contexts using the learnable tau\n",
        "        inter_pred_raw = context_sub / tau\n",
        "        inter_pred_softmax = torch.softmax(inter_pred_raw, dim=1)\n",
        "\n",
        "        # Find the subregion with the highest predicted probability for each region\n",
        "        # and set its value to 1, and others to 0.\n",
        "        max_prob_indices = torch.argmax(inter_pred_softmax, dim=1)\n",
        "        inter_discrete = torch.zeros_like(inter_pred_softmax)\n",
        "        inter_discrete[torch.arange(self.num_regions), max_prob_indices] = 1.0\n",
        "\n",
        "        # Scale the discrete intervention by the total regional intervention amount\n",
        "        inter_constrained = inter_discrete * interventions_reg.unsqueeze(1)\n",
        "\n",
        "        return inter_constrained, inter_pred_softmax # Also return softmax probabilities for loss\n",
        "\n",
        "    def forward(self, context_sub, interventions_reg):\n",
        "        inter_sub, inter_probs = self.preprocess_inter(context_sub, interventions_reg)\n",
        "        outcome_sub_pred = (self.shift - context_sub) * self.scale * inter_sub# + noise_sub\n",
        "        outcome_reg_pred = outcome_sub_pred.mean(dim=1)\n",
        "        return outcome_reg_pred, inter_sub, inter_probs\n",
        "\n",
        "model_combined = CausalModelCombined(num_regions, num_subregions)\n",
        "optimizer_combined = optim.Adam(model_combined.parameters(), lr=lr)\n",
        "criterion_outcome = nn.MSELoss()\n",
        "criterion_inter = nn.NLLLoss()\n",
        "\n",
        "# Train\n",
        "losses_combined = []\n",
        "outcome_losses_combined = []\n",
        "inter_losses_combined = []\n",
        "# Load true subregion interventions for loss calculation\n",
        "true_interventions_sub = torch.tensor(pd.read_csv(os.path.join(data_dir, 'interventions_subregion.csv'), header=None).values, dtype=torch.float32)\n",
        "# Create target indices for NLLLoss\n",
        "true_intervention_indices = torch.argmax(true_interventions_sub, dim=1)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer_combined.zero_grad()\n",
        "    outcome_reg_pred, inter_sub_pred, inter_probs = model_combined.forward(context_sub, interventions_reg)\n",
        "\n",
        "    loss_outcome = criterion_outcome(outcome_reg_pred, outcome_reg)\n",
        "    # Use log_softmax for NLLLoss\n",
        "    loss_inter = criterion_inter(torch.log(inter_probs + 1e-9), true_intervention_indices)\n",
        "    loss = loss_outcome + loss_inter # Combine losses\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer_combined.step()\n",
        "    losses_combined.append(loss.item())\n",
        "    outcome_losses_combined.append(loss_outcome.item())\n",
        "    inter_losses_combined.append(loss_inter.item())\n",
        "\n",
        "\n",
        "# Save loss curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses_combined, label='Total Loss')\n",
        "plt.plot(outcome_losses_combined, label='Outcome Loss')\n",
        "plt.plot(inter_losses_combined, label='Intervention Prediction Loss (NLL)')\n",
        "plt.title('Combined Model Loss Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curves_combined_new.jpg'))\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curves_combined_new.pdf'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# Print final estimates\n",
        "print(f\"Final shift_param (Combined): {model_combined.shift.item()}\")\n",
        "print(f\"Final scale_param (Combined): {model_combined.scale.item()}\")\n",
        "print(f\"Final estimated tau (Combined): {torch.exp(model_combined.log_tau).item()}\")\n",
        "\n",
        "# Save params to txt\n",
        "with open(os.path.join(data_dir, 'parameter_estimate_combined_new.txt'), 'w') as f:\n",
        "    f.write(f\"shift_param: {model_combined.shift.item()}\\n\")\n",
        "    f.write(f\"scale_param: {model_combined.scale.item()}\\n\")\n",
        "    f.write(f\"estimated_tau: {torch.exp(model_combined.log_tau).item()}\\n\")\n",
        "\n",
        "# Save estimated interventions sub (processed)\n",
        "inter_est_final_combined, _ = model_combined.preprocess_inter(context_sub, interventions_reg)\n",
        "inter_est_final_combined = inter_est_final_combined.detach().numpy()\n",
        "pd.DataFrame(inter_est_final_combined).to_csv(os.path.join(data_dir, 'interventions_subregion_estimated_combined_new.csv'), index=False, header=False)\n",
        "\n",
        "# Visualize estimated subregion\n",
        "def visualize_estimated_subregion(data, title, file_suffix):\n",
        "    grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "    subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "    grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    for i in range(int(np.sqrt(num_regions))):\n",
        "        for j in range(int(np.sqrt(num_regions))):\n",
        "            region_idx = i * int(np.sqrt(num_regions)) + j\n",
        "            sub_data = data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = sub_data\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    heatmap_cmap = sns.light_palette(reds, as_cmap=True)\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=False, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title(title, fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, f'interventions_subregion_estimated_{file_suffix}.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, f'interventions_subregion_estimated_{file_suffix}.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_estimated_subregion(inter_est_final_combined, 'Estimated Interventions Subregion (Combined Model)', 'combined_new')"
      ],
      "metadata": {
        "id": "c4wvFfLGeJaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# --- Parameters ---\n",
        "data_dir = 'data_exp4_lc'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "epochs = 10000\n",
        "lr = 0.01\n",
        "torch.manual_seed(42)\n",
        "grid_size_sub = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "sns.set_context('paper', font_scale=1.5)\n",
        "reds = sns.color_palette(\"deep\")[3]\n",
        "\n",
        "# --- Load Data ---\n",
        "interventions_reg = torch.tensor(\n",
        "    pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.flatten(),\n",
        "    dtype=torch.float32\n",
        ")\n",
        "outcome_reg = torch.tensor(\n",
        "    pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.flatten(),\n",
        "    dtype=torch.float32\n",
        ")\n",
        "context_sub = torch.tensor(\n",
        "    pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values,\n",
        "    dtype=torch.float32\n",
        ")\n",
        "context_reg = torch.mean(context_sub, dim=1)\n",
        "\n",
        "# --- Model ---\n",
        "class CausalModelCombined(nn.Module):\n",
        "    def __init__(self, num_regions, num_subregions):\n",
        "        super().__init__()\n",
        "        self.shift = nn.Parameter(torch.tensor(1.0))\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "        self.log_tau = nn.Parameter(torch.tensor(np.log(1.0)))  # learnable tau\n",
        "        self.num_regions = num_regions\n",
        "        self.num_subregions = num_subregions\n",
        "\n",
        "    def preprocess_inter(self, context_sub, interventions_reg):\n",
        "        tau = torch.exp(self.log_tau)\n",
        "        inter_pred_raw = context_sub / tau\n",
        "        inter_pred_softmax = torch.softmax(inter_pred_raw, dim=1)\n",
        "\n",
        "        # Always sample from probabilities\n",
        "        sampled_indices = torch.multinomial(inter_pred_softmax, num_samples=1).squeeze(1)\n",
        "\n",
        "        inter_discrete = torch.zeros_like(inter_pred_softmax)\n",
        "        inter_discrete[torch.arange(self.num_regions), sampled_indices] = 1.0\n",
        "        inter_constrained = inter_discrete * interventions_reg.unsqueeze(1)\n",
        "\n",
        "        return inter_constrained, inter_pred_softmax\n",
        "\n",
        "    def forward(self, context_sub, interventions_reg):\n",
        "        inter_sub, inter_probs = self.preprocess_inter(context_sub, interventions_reg)\n",
        "        outcome_sub_pred = (self.shift - context_sub) * self.scale * inter_sub\n",
        "        outcome_reg_pred = outcome_sub_pred.mean(dim=1)\n",
        "        return outcome_reg_pred, inter_sub, inter_probs\n",
        "\n",
        "# --- Setup ---\n",
        "model_combined = CausalModelCombined(num_regions, num_subregions)\n",
        "optimizer_combined = optim.Adam(model_combined.parameters(), lr=lr)\n",
        "criterion_outcome = nn.MSELoss()\n",
        "criterion_inter = nn.NLLLoss()\n",
        "\n",
        "true_interventions_sub = torch.tensor(\n",
        "    pd.read_csv(os.path.join(data_dir, 'interventions_subregion.csv'), header=None).values,\n",
        "    dtype=torch.float32\n",
        ")\n",
        "true_intervention_indices = torch.argmax(true_interventions_sub, dim=1)\n",
        "\n",
        "# --- Training ---\n",
        "losses_combined = []\n",
        "outcome_losses_combined = []\n",
        "inter_losses_combined = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer_combined.zero_grad()\n",
        "    outcome_reg_pred, inter_sub_pred, inter_probs = model_combined.forward(context_sub, interventions_reg)\n",
        "\n",
        "    loss_outcome = criterion_outcome(outcome_reg_pred, outcome_reg)\n",
        "    loss_inter = criterion_inter(torch.log(inter_probs + 1e-9), true_intervention_indices)\n",
        "    loss = loss_outcome + loss_inter\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer_combined.step()\n",
        "\n",
        "    losses_combined.append(loss.item())\n",
        "    outcome_losses_combined.append(loss_outcome.item())\n",
        "    inter_losses_combined.append(loss_inter.item())\n",
        "\n",
        "# --- Save Loss Curves ---\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses_combined, label='Total Loss')\n",
        "plt.plot(outcome_losses_combined, label='Outcome Loss')\n",
        "plt.plot(inter_losses_combined, label='Intervention Prediction Loss (NLL)')\n",
        "plt.title('Combined Model Loss Curves (Sampling Always)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curves_combined_sampling_always.jpg'))\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curves_combined_sampling_always.pdf'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# --- Save Parameters ---\n",
        "print(f\"Final shift_param (Combined): {model_combined.shift.item()}\")\n",
        "print(f\"Final scale_param (Combined): {model_combined.scale.item()}\")\n",
        "print(f\"Final estimated tau (Combined): {torch.exp(model_combined.log_tau).item()}\")\n",
        "\n",
        "with open(os.path.join(data_dir, 'parameter_estimate_combined_sampling_always.txt'), 'w') as f:\n",
        "    f.write(f\"shift_param: {model_combined.shift.item()}\\n\")\n",
        "    f.write(f\"scale_param: {model_combined.scale.item()}\\n\")\n",
        "    f.write(f\"estimated_tau: {torch.exp(model_combined.log_tau).item()}\\n\")\n",
        "\n",
        "# --- Save Estimated Interventions (sampled) ---\n",
        "inter_est_final_combined, _ = model_combined.preprocess_inter(context_sub, interventions_reg)\n",
        "pd.DataFrame(inter_est_final_combined.detach().numpy()).to_csv(\n",
        "    os.path.join(data_dir, 'interventions_subregion_estimated_combined_sampling_always.csv'),\n",
        "    index=False, header=False\n",
        ")\n",
        "\n",
        "# --- Visualization ---\n",
        "def visualize_estimated_subregion(data, title, file_suffix):\n",
        "    grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "    subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "    grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    for i in range(int(np.sqrt(num_regions))):\n",
        "        for j in range(int(np.sqrt(num_regions))):\n",
        "            region_idx = i * int(np.sqrt(num_regions)) + j\n",
        "            sub_data = data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            grid[i*subregions_per_dim:(i+1)*subregions_per_dim,\n",
        "                 j*subregions_per_dim:(j+1)*subregions_per_dim] = sub_data\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    heatmap_cmap = sns.light_palette(reds, as_cmap=True)\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=False,\n",
        "                cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title(title, fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, f'interventions_subregion_estimated_{file_suffix}.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, f'interventions_subregion_estimated_{file_suffix}.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_estimated_subregion(\n",
        "    inter_est_final_combined.detach().numpy(),\n",
        "    'Estimated Interventions Subregion (Combined Model)',\n",
        "    'combined_sampling_always'\n",
        ")\n"
      ],
      "metadata": {
        "id": "M5LrF-b8eRdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate true causal effect at subregion level\n",
        "# This is based on the original data generation formula before adding noise\n",
        "true_outcome_sub = (shift_param - context_sub) * scale_param * interventions_sub\n",
        "\n",
        "# Load estimated interventions\n",
        "inter_est_final = pd.read_csv(os.path.join(data_dir, 'interventions_subregion_estimated_combined_new.csv'), header=None).values\n",
        "\n",
        "# Calculate predicted outcome at subregion level using estimated interventions and learned parameters\n",
        "# Need to load learned parameters from the text file\n",
        "with open(os.path.join(data_dir, 'parameter_estimate_combined_new.txt'), 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    estimated_shift = float(lines[0].split(': ')[1])\n",
        "    estimated_scale = float(lines[1].split(': ')[1])\n",
        "\n",
        "# Ensure context_sub is a numpy array for the calculation\n",
        "if isinstance(context_sub, torch.Tensor):\n",
        "    context_sub_np = context_sub.numpy()\n",
        "else:\n",
        "    context_sub_np = context_sub\n",
        "\n",
        "\n",
        "predicted_outcome_sub = (estimated_shift - context_sub_np) * estimated_scale * inter_est_final\n",
        "\n",
        "# Visualize true vs predicted outcome at subregion level\n",
        "def visualize_true_vs_predicted_subregion(true_data, predicted_data, num_regions=100, num_subregions=4):\n",
        "    grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "    subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "\n",
        "    true_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    predicted_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    difference_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "\n",
        "    for i in range(int(np.sqrt(num_regions))):\n",
        "        for j in range(int(np.sqrt(num_regions))):\n",
        "            region_idx = i * int(np.sqrt(num_regions)) + j\n",
        "            true_sub_data = true_data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            predicted_sub_data = predicted_data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            true_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = true_sub_data\n",
        "            predicted_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = predicted_sub_data\n",
        "            difference_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = true_sub_data - predicted_sub_data\n",
        "\n",
        "    # Color maps from seaborn deep palette (assuming these are available in the notebook's global scope)\n",
        "    colors = sns.color_palette(\"deep\")\n",
        "    blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue for Outcome\n",
        "    coolwarm_cmap = 'coolwarm' # Using coolwarm for difference as it was used before\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5)) # Adjusted figure size\n",
        "\n",
        "\n",
        "    # True Outcome (Blue)\n",
        "    sns.heatmap(true_grid, ax=axs[0], cmap=blue_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[0].set_title('True Outcome Subregion')\n",
        "    axs[0].set_xticks([])\n",
        "    axs[0].set_yticks([])\n",
        "\n",
        "    # Predicted Outcome (Blue)\n",
        "    sns.heatmap(predicted_grid, ax=axs[1], cmap=blue_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[1].set_title('Predicted Outcome Subregion')\n",
        "    axs[1].set_xticks([])\n",
        "    axs[1].set_yticks([])\n",
        "\n",
        "    # Difference (Coolwarm)\n",
        "    sns.heatmap(difference_grid, ax=axs[2], cmap=coolwarm_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[2].set_title('Difference (True - Predicted)')\n",
        "    axs[2].set_xticks([])\n",
        "    axs[2].set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, 'true_vs_predicted_outcome_subregion_styled.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, 'exp5_tau_true_vs_predicted_outcome_subregion_styled.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_true_vs_predicted_subregion(true_outcome_sub, predicted_outcome_sub, num_regions, num_subregions)\n",
        "\n",
        "# Calculate MSE for true vs predicted subregion outcome\n",
        "mse_subregion = np.mean((np.asarray(true_outcome_sub) - np.asarray(predicted_outcome_sub)) ** 2)\n",
        "print(f\"MSE between true and predicted subregion outcome: {mse_subregion}\")"
      ],
      "metadata": {
        "id": "i77RHTEfegDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exp 4. High Confounding"
      ],
      "metadata": {
        "id": "gwscb4IgUn93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "shift_param = 1.23\n",
        "scale_param = -4.2\n",
        "noise_var = 0.01\n",
        "data_dir = 'data_exp4_hc'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1) Context (wealth) subregion\n",
        "context_sub = np.random.uniform(0, 1, (num_regions, num_subregions))\n",
        "pd.DataFrame(context_sub).to_csv(os.path.join(data_dir, 'context_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# 2) Interventions subregion (only one per region, using softmax over context)\n",
        "def softmax(x, tau=1):\n",
        "    x = x / tau  # temperature scaling\n",
        "    e_x = np.exp(x - np.max(x))  # numerical stability\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "interventions_sub = np.zeros((num_regions, num_subregions))\n",
        "for i in range(num_regions):\n",
        "    probs = softmax(context_sub[i], tau=0.1)  # smaller tau = sharper preferences for high context\n",
        "    col_idx = np.random.choice(np.arange(num_subregions), p=probs)\n",
        "    interventions_sub[i, col_idx] = np.random.uniform(0.2, 1)\n",
        "pd.DataFrame(interventions_sub).to_csv(os.path.join(data_dir, 'interventions_subregion.csv'), index=False, header=False)\n",
        "\n",
        "\n",
        "# Interventions region: sum of subregion interventions\n",
        "interventions_reg = interventions_sub.sum(axis=1)\n",
        "pd.DataFrame(interventions_reg).to_csv(os.path.join(data_dir, 'interventions_region.csv'), index=False, header=False)\n",
        "\n",
        "# 3) Noise subregion\n",
        "noise_sub = np.random.normal(0, np.sqrt(noise_var), size=(num_regions, num_subregions))\n",
        "pd.DataFrame(noise_sub).to_csv(os.path.join(data_dir, 'noise_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# 4) Outcome subregion: context-dependent + noise\n",
        "outcome_sub = (shift_param - context_sub) * scale_param * interventions_sub + noise_sub\n",
        "pd.DataFrame(outcome_sub).to_csv(os.path.join(data_dir, 'outcome_subregion.csv'), index=False, header=False)\n",
        "\n",
        "# Outcome region: average outcome across subregions\n",
        "outcome_reg = outcome_sub.mean(axis=1)\n",
        "pd.DataFrame(outcome_reg).to_csv(os.path.join(data_dir, 'outcome_region.csv'), index=False, header=False)"
      ],
      "metadata": {
        "id": "OP3AO1ksesNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp4_hc'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "grid_size_reg = 10  # 10x10 for regions\n",
        "grid_size_sub = 20  # 20x20 for subregions (2x2 per region)\n",
        "sns.set_context('paper', font_scale=1.5)\n",
        "colors = sns.color_palette(\"deep\")\n",
        "reds = colors[3]\n",
        "blues = colors[0]\n",
        "\n",
        "def visualize_subregion_csv(file_name, cmap, title, is_wealth=False, num_regions=100, num_subregions=4):\n",
        "    data = pd.read_csv(os.path.join(data_dir, file_name), header=None).values\n",
        "    # Reshape to subregion grid size: each region row becomes block of subregions\n",
        "    grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "    subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "    grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    for i in range(int(np.sqrt(num_regions))):\n",
        "        for j in range(int(np.sqrt(num_regions))):\n",
        "            region_idx = i * int(np.sqrt(num_regions)) + j\n",
        "            sub_data = data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = sub_data\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    if is_wealth:\n",
        "        heatmap_cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
        "    else:\n",
        "        if isinstance(cmap, tuple):\n",
        "            heatmap_cmap = sns.light_palette(cmap, as_cmap=True)\n",
        "        else:\n",
        "            heatmap_cmap = cmap\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=False, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title(title, fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)  # No legend box\n",
        "    plt.tight_layout()\n",
        "    base_name = file_name.replace('.csv', '')\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def visualize_region_csv(file_name, cmap, title, is_wealth=False, num_regions=100):\n",
        "    data = pd.read_csv(os.path.join(data_dir, file_name), header=None).values.flatten()\n",
        "    # Reshape to region grid size\n",
        "    grid_size_reg_calc = int(np.sqrt(num_regions))\n",
        "    grid = data.reshape(grid_size_reg_calc, grid_size_reg_calc)\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    if is_wealth:\n",
        "        heatmap_cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
        "    else:\n",
        "        if isinstance(cmap, tuple):\n",
        "            heatmap_cmap = sns.light_palette(cmap, as_cmap=True)\n",
        "        else:\n",
        "            heatmap_cmap = cmap\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=False, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title(title, fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)\n",
        "    plt.tight_layout()\n",
        "    base_name = file_name.replace('.csv', '')\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, f'{base_name}.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Visualize each\n",
        "visualize_subregion_csv('interventions_subregion.csv', reds, 'Interventions Subregion', is_wealth=False, num_regions=num_regions, num_subregions=num_subregions)\n",
        "visualize_region_csv('interventions_region.csv', reds, 'Interventions Region', is_wealth=False, num_regions=num_regions)\n",
        "visualize_subregion_csv('context_subregion.csv', None, 'Wealth Subregion', is_wealth=True, num_regions=num_regions, num_subregions=num_subregions)\n",
        "visualize_subregion_csv('noise_subregion.csv', 'coolwarm', 'Noise Subregion', is_wealth=False, num_regions=num_regions, num_subregions=num_subregions)  # Using coolwarm for noise as not specified\n",
        "visualize_subregion_csv('outcome_subregion.csv', blues, 'Outcome Subregion', is_wealth=False, num_regions=num_regions, num_subregions=num_subregions)\n",
        "visualize_region_csv('outcome_region.csv', blues, 'Outcome Region', is_wealth=False, num_regions=num_regions)\n",
        "\n",
        "# Wealth low-res: average context_sub to get wealth_reg\n",
        "context_sub = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values\n",
        "wealth_reg = np.mean(context_sub, axis=1)\n",
        "pd.DataFrame(wealth_reg).to_csv(os.path.join(data_dir, 'context_region.csv'), index=False, header=False)  # Temp save for viz\n",
        "visualize_region_csv('context_region.csv', None, 'Wealth Region', is_wealth=True, num_regions=num_regions)"
      ],
      "metadata": {
        "id": "B69Xi5iLe6y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp4_hc'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "grid_size_reg = int(np.sqrt(num_regions))  # 10x10 for regions\n",
        "grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))  # 20x20 for subregions (2x2 per region)\n",
        "subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "\n",
        "\n",
        "# Color maps from seaborn deep palette\n",
        "colors = sns.color_palette(\"deep\")\n",
        "red_cmap = sns.light_palette(colors[3], as_cmap=True)  # Red\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue\n",
        "green_cmap = sns.light_palette(colors[2], as_cmap=True)  # Green\n",
        "\n",
        "# Parameter to control space between subplots (as fraction of average axis width)\n",
        "space_between = 0.05  # Adjust this value as needed (e.g., 0.0 for no space, 0.5 for more space)\n",
        "\n",
        "# Load data for combined\n",
        "interv_data = pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.flatten().reshape(grid_size_reg, grid_size_reg).astype(float)  # Assuming 0/1, float for heatmap\n",
        "outcome_data = pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.flatten().reshape(grid_size_reg, grid_size_reg)# / 100.0  # Assuming 0-100, normalize to 0-1\n",
        "context_data = pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values\n",
        "context_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "for i in range(grid_size_reg):\n",
        "    for j in range(grid_size_reg):\n",
        "        region_idx = i * grid_size_reg + j\n",
        "        sub_data = context_data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "        context_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = sub_data\n",
        "context_norm = (context_grid - 1) / 2.0  # Normalize assuming values 1,2,3 to 0-1\n",
        "\n",
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Left: Intervention in red\n",
        "sns.heatmap(interv_data, ax=axs[0], cmap=red_cmap, cbar=False, square=True, linewidths=2)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "\n",
        "# Center: Outcome in blue\n",
        "sns.heatmap(outcome_data, ax=axs[1], cmap=blue_cmap, cbar=False, square=True, linewidths=2)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "# Right: Context in green\n",
        "sns.heatmap(context_norm, ax=axs[2], cmap=green_cmap, cbar=False, square=True, linewidths=1)#, vmin=0.0)#, vmax=1.0)\n",
        "axs[2].set_xticks([])\n",
        "axs[2].set_yticks([])\n",
        "\n",
        "# Adjust the space between subplots\n",
        "fig.subplots_adjust(wspace=space_between)\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(os.path.join(data_dir, 'combined_heatmaps.jpg'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(os.path.join(data_dir, 'exp5_combined_heatmaps.pdf'), bbox_inches='tight', pad_inches=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ljnLMk7je8SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp4_hc'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "epochs = 10000\n",
        "lr = 0.001\n",
        "torch.manual_seed(42)\n",
        "grid_size_sub = 20\n",
        "sns.set_context('paper', font_scale=1.5)\n",
        "reds = sns.color_palette(\"deep\")[3]\n",
        "\n",
        "# Read all CSVs\n",
        "interventions_reg = torch.tensor(pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.flatten(), dtype=torch.float32)\n",
        "outcome_reg = torch.tensor(pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.flatten(), dtype=torch.float32)\n",
        "context_sub = torch.tensor(pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values, dtype=torch.float32)\n",
        "#noise_sub = torch.tensor(pd.read_csv(os.path.join(data_dir, 'noise_subregion.csv'), header=None).values, dtype=torch.float32)\n",
        "\n",
        "# Model\n",
        "class CausalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.inter_est = nn.Parameter(torch.randn(num_regions, num_subregions))\n",
        "        self.shift = nn.Parameter(torch.tensor(1.0))\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "        self.temp = torch.tensor(10.0)\n",
        "        self.noise_scale = 0.1\n",
        "\n",
        "    def preprocess_inter(self):\n",
        "        inter_pos = (self.inter_est + torch.randn_like(self.inter_est)*self.noise_scale) ** 2  # Ensure positivity\n",
        "        inter_sparse = torch.softmax(inter_pos / self.temp, dim=1)  # Softmax for differentiable near-one-hot sparsity\n",
        "        inter_constrained = inter_sparse * interventions_reg.unsqueeze(1)  # Multiply by known region sum\n",
        "        return inter_constrained\n",
        "\n",
        "    def forward(self):\n",
        "        inter_sub = self.preprocess_inter()\n",
        "        outcome_sub_pred = (self.shift - context_sub) * self.scale * inter_sub# + noise_sub\n",
        "        outcome_reg_pred = outcome_sub_pred.mean(dim=1)\n",
        "        return outcome_reg_pred\n",
        "\n",
        "model = CausalModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Train\n",
        "losses = []\n",
        "start_temp = 10.0\n",
        "end_temp = 2.0\n",
        "for epoch in range(epochs):\n",
        "    model.temp = torch.tensor(start_temp + (end_temp - start_temp) * (epoch / (epochs - 1)))\n",
        "    optimizer.zero_grad()\n",
        "    pred = model.forward()\n",
        "    loss = criterion(pred, outcome_reg)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "model.temp = 0.0001\n",
        "model.noise_scale = 0.0\n",
        "\n",
        "# Save loss curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses)\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curve.jpg'))\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curve.pdf'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Print final estimates\n",
        "print(f\"Final shift_param: {model.shift.item()}\")\n",
        "print(f\"Final scale_param: {model.scale.item()}\")\n",
        "\n",
        "# Save params to txt\n",
        "with open(os.path.join(data_dir, 'parameter_estimate.txt'), 'w') as f:\n",
        "    f.write(f\"shift_param: {model.shift.item()}\\n\")\n",
        "    f.write(f\"scale_param: {model.scale.item()}\\n\")\n",
        "\n",
        "# Save estimated interventions sub (processed)\n",
        "inter_est_final = model.preprocess_inter().detach().numpy()\n",
        "pd.DataFrame(inter_est_final).to_csv(os.path.join(data_dir, 'interventions_subregion_estimated.csv'), index=False, header=False)\n",
        "\n",
        "# Visualize estimated subregion\n",
        "def visualize_estimated_subregion():\n",
        "    data = inter_est_final\n",
        "    grid = np.zeros((grid_size_sub, grid_size_sub))\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            region_idx = i * 10 + j\n",
        "            sub_data = data[region_idx, :].reshape(2, 2)\n",
        "            grid[2*i:2*(i+1), 2*j:2*(j+1)] = sub_data\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    heatmap_cmap = sns.light_palette(reds, as_cmap=True)\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=False, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title('Estimated Interventions Subregion', fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, 'interventions_subregion_estimated.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, 'exp5_interventions_subregion_estimated_standard.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_estimated_subregion()"
      ],
      "metadata": {
        "id": "iQaHAfU6fLWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate true causal effect at subregion level\n",
        "true_outcome_sub = (shift_param - context_sub) * scale_param * interventions_sub\n",
        "\n",
        "# Load estimated interventions\n",
        "inter_est_final = pd.read_csv(os.path.join(data_dir, 'interventions_subregion_estimated.csv'), header=None).values\n",
        "\n",
        "# Calculate predicted outcome at subregion level using estimated interventions and learned parameters\n",
        "with open(os.path.join(data_dir, 'parameter_estimate.txt'), 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    estimated_shift = float(lines[0].split(': ')[1])\n",
        "    estimated_scale = float(lines[1].split(': ')[1])\n",
        "\n",
        "# Ensure context_sub is a numpy array for the calculation\n",
        "if isinstance(context_sub, torch.Tensor):\n",
        "    context_sub_np = context_sub.numpy()\n",
        "else:\n",
        "    context_sub_np = context_sub\n",
        "\n",
        "\n",
        "predicted_outcome_sub = (estimated_shift - context_sub_np) * estimated_scale * inter_est_final\n",
        "\n",
        "# Visualize true vs predicted outcome at subregion level\n",
        "def visualize_true_vs_predicted_subregion(true_data, predicted_data, num_regions=100, num_subregions=4):\n",
        "    grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "    subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "\n",
        "    true_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    predicted_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    difference_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "\n",
        "    for i in range(int(np.sqrt(num_regions))):\n",
        "        for j in range(int(np.sqrt(num_regions))):\n",
        "            region_idx = i * int(np.sqrt(num_regions)) + j\n",
        "            true_sub_data = true_data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            predicted_sub_data = predicted_data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            true_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = true_sub_data\n",
        "            predicted_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = predicted_sub_data\n",
        "            difference_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = true_sub_data - predicted_sub_data\n",
        "\n",
        "    # Color maps from seaborn deep palette (assuming these are available in the notebook's global scope)\n",
        "    colors = sns.color_palette(\"deep\")\n",
        "    blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue for Outcome\n",
        "    coolwarm_cmap = 'coolwarm' # Using coolwarm for difference as it was used before\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5)) # Adjusted figure size\n",
        "\n",
        "\n",
        "    # True Outcome (Blue)\n",
        "    sns.heatmap(true_grid, ax=axs[0], cmap=blue_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[0].set_title('True Outcome Subregion')\n",
        "    axs[0].set_xticks([])\n",
        "    axs[0].set_yticks([])\n",
        "\n",
        "    # Predicted Outcome (Blue)\n",
        "    sns.heatmap(predicted_grid, ax=axs[1], cmap=blue_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[1].set_title('Predicted Outcome Subregion')\n",
        "    axs[1].set_xticks([])\n",
        "    axs[1].set_yticks([])\n",
        "\n",
        "    # Difference (Coolwarm)\n",
        "    sns.heatmap(difference_grid, ax=axs[2], cmap=coolwarm_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[2].set_title('Difference (True - Predicted)')\n",
        "    axs[2].set_xticks([])\n",
        "    axs[2].set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, 'true_vs_predicted_outcome_subregion_styled.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, 'exp5_standard_true_vs_predicted_outcome_subregion_styled.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_true_vs_predicted_subregion(true_outcome_sub, predicted_outcome_sub, num_regions, num_subregions)\n",
        "\n",
        "# Calculate MSE for true vs predicted subregion outcome\n",
        "mse_subregion = np.mean((np.asarray(true_outcome_sub) - np.asarray(predicted_outcome_sub)) ** 2)\n",
        "print(f\"MSE between true and predicted subregion outcome: {mse_subregion}\")"
      ],
      "metadata": {
        "id": "zdaGNpMjfRCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define parameters\n",
        "data_dir = 'data_exp4_hc'\n",
        "num_regions = 100\n",
        "num_subregions = 4\n",
        "epochs = 10000\n",
        "lr = 0.001\n",
        "torch.manual_seed(42)\n",
        "grid_size_sub = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "sns.set_context('paper', font_scale=1.5)\n",
        "reds = sns.color_palette(\"deep\")[3]\n",
        "\n",
        "# Read all CSVs\n",
        "interventions_reg = torch.tensor(pd.read_csv(os.path.join(data_dir, 'interventions_region.csv'), header=None).values.flatten(), dtype=torch.float32)\n",
        "outcome_reg = torch.tensor(pd.read_csv(os.path.join(data_dir, 'outcome_region.csv'), header=None).values.flatten(), dtype=torch.float32)\n",
        "context_sub = torch.tensor(pd.read_csv(os.path.join(data_dir, 'context_subregion.csv'), header=None).values, dtype=torch.float32)\n",
        "# context_reg is not directly used in the preprocess_inter in this version, but kept for consistency if needed elsewhere\n",
        "context_reg = torch.mean(context_sub, dim=1)\n",
        "\n",
        "# Model\n",
        "class CausalModelCombined(nn.Module):\n",
        "    def __init__(self, num_regions, num_subregions):\n",
        "        super().__init__()\n",
        "        self.shift = nn.Parameter(torch.tensor(1.0))\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "        # Make tau a learnable parameter\n",
        "        self.log_tau = nn.Parameter(torch.tensor(np.log(1.0))) # Initialize log_tau to 0 (tau=1)\n",
        "        self.num_regions = num_regions\n",
        "        self.num_subregions = num_subregions\n",
        "\n",
        "\n",
        "    def preprocess_inter(self, context_sub, interventions_reg):\n",
        "        tau = torch.exp(self.log_tau)\n",
        "        # Apply softmax over subregion contexts using the learnable tau\n",
        "        inter_pred_raw = context_sub / tau\n",
        "        inter_pred_softmax = torch.softmax(inter_pred_raw, dim=1)\n",
        "\n",
        "        # Find the subregion with the highest predicted probability for each region\n",
        "        # and set its value to 1, and others to 0.\n",
        "        max_prob_indices = torch.argmax(inter_pred_softmax, dim=1)\n",
        "        inter_discrete = torch.zeros_like(inter_pred_softmax)\n",
        "        inter_discrete[torch.arange(self.num_regions), max_prob_indices] = 1.0\n",
        "\n",
        "        # Scale the discrete intervention by the total regional intervention amount\n",
        "        inter_constrained = inter_discrete * interventions_reg.unsqueeze(1)\n",
        "\n",
        "        return inter_constrained, inter_pred_softmax # Also return softmax probabilities for loss\n",
        "\n",
        "    def forward(self, context_sub, interventions_reg):\n",
        "        inter_sub, inter_probs = self.preprocess_inter(context_sub, interventions_reg)\n",
        "        outcome_sub_pred = (self.shift - context_sub) * self.scale * inter_sub# + noise_sub\n",
        "        outcome_reg_pred = outcome_sub_pred.mean(dim=1)\n",
        "        return outcome_reg_pred, inter_sub, inter_probs\n",
        "\n",
        "model_combined = CausalModelCombined(num_regions, num_subregions)\n",
        "optimizer_combined = optim.Adam(model_combined.parameters(), lr=lr)\n",
        "criterion_outcome = nn.MSELoss()\n",
        "criterion_inter = nn.NLLLoss()\n",
        "\n",
        "# Train\n",
        "losses_combined = []\n",
        "outcome_losses_combined = []\n",
        "inter_losses_combined = []\n",
        "# Load true subregion interventions for loss calculation\n",
        "true_interventions_sub = torch.tensor(pd.read_csv(os.path.join(data_dir, 'interventions_subregion.csv'), header=None).values, dtype=torch.float32)\n",
        "# Create target indices for NLLLoss\n",
        "true_intervention_indices = torch.argmax(true_interventions_sub, dim=1)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer_combined.zero_grad()\n",
        "    outcome_reg_pred, inter_sub_pred, inter_probs = model_combined.forward(context_sub, interventions_reg)\n",
        "\n",
        "    loss_outcome = criterion_outcome(outcome_reg_pred, outcome_reg)\n",
        "    # Use log_softmax for NLLLoss\n",
        "    loss_inter = criterion_inter(torch.log(inter_probs + 1e-9), true_intervention_indices)\n",
        "    loss = loss_outcome + loss_inter # Combine losses\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer_combined.step()\n",
        "    losses_combined.append(loss.item())\n",
        "    outcome_losses_combined.append(loss_outcome.item())\n",
        "    inter_losses_combined.append(loss_inter.item())\n",
        "\n",
        "\n",
        "# Save loss curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses_combined, label='Total Loss')\n",
        "plt.plot(outcome_losses_combined, label='Outcome Loss')\n",
        "plt.plot(inter_losses_combined, label='Intervention Prediction Loss (NLL)')\n",
        "plt.title('Combined Model Loss Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curves_combined_new.jpg'))\n",
        "plt.savefig(os.path.join(data_dir, 'loss_curves_combined_new.pdf'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# Print final estimates\n",
        "print(f\"Final shift_param (Combined): {model_combined.shift.item()}\")\n",
        "print(f\"Final scale_param (Combined): {model_combined.scale.item()}\")\n",
        "print(f\"Final estimated tau (Combined): {torch.exp(model_combined.log_tau).item()}\")\n",
        "\n",
        "# Save params to txt\n",
        "with open(os.path.join(data_dir, 'parameter_estimate_combined_new.txt'), 'w') as f:\n",
        "    f.write(f\"shift_param: {model_combined.shift.item()}\\n\")\n",
        "    f.write(f\"scale_param: {model_combined.scale.item()}\\n\")\n",
        "    f.write(f\"estimated_tau: {torch.exp(model_combined.log_tau).item()}\\n\")\n",
        "\n",
        "# Save estimated interventions sub (processed)\n",
        "inter_est_final_combined, _ = model_combined.preprocess_inter(context_sub, interventions_reg)\n",
        "inter_est_final_combined = inter_est_final_combined.detach().numpy()\n",
        "pd.DataFrame(inter_est_final_combined).to_csv(os.path.join(data_dir, 'interventions_subregion_estimated_combined_new.csv'), index=False, header=False)\n",
        "\n",
        "# Visualize estimated subregion\n",
        "def visualize_estimated_subregion(data, title, file_suffix):\n",
        "    grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "    subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "    grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    for i in range(int(np.sqrt(num_regions))):\n",
        "        for j in range(int(np.sqrt(num_regions))):\n",
        "            region_idx = i * int(np.sqrt(num_regions)) + j\n",
        "            sub_data = data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = sub_data\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    heatmap_cmap = sns.light_palette(reds, as_cmap=True)\n",
        "    sns.heatmap(grid, ax=ax, cmap=heatmap_cmap, square=True, cbar=True, cbar_kws={'label': 'Value', 'location': 'right', 'pad': 0.1})\n",
        "    ax.set_title(title, fontsize=18)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    plt.legend([], [], frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, f'interventions_subregion_estimated_{file_suffix}.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, f'exp_5_interventions_subregion_estimated_{file_suffix}.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_estimated_subregion(inter_est_final_combined, 'Estimated Interventions Subregion (Combined Model)', 'combined_new')"
      ],
      "metadata": {
        "id": "RHzEjmasfede"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate true causal effect at subregion level\n",
        "true_outcome_sub = (shift_param - context_sub) * scale_param * interventions_sub\n",
        "\n",
        "# Load estimated interventions\n",
        "inter_est_final = pd.read_csv(os.path.join(data_dir, 'interventions_subregion_estimated_combined_new.csv'), header=None).values\n",
        "\n",
        "# Calculate predicted outcome at subregion level using estimated interventions and learned parameters\n",
        "# Need to load learned parameters from the text file\n",
        "with open(os.path.join(data_dir, 'parameter_estimate_combined_new.txt'), 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    estimated_shift = float(lines[0].split(': ')[1])\n",
        "    estimated_scale = float(lines[1].split(': ')[1])\n",
        "\n",
        "# Ensure context_sub is a numpy array for the calculation\n",
        "if isinstance(context_sub, torch.Tensor):\n",
        "    context_sub_np = context_sub.numpy()\n",
        "else:\n",
        "    context_sub_np = context_sub\n",
        "\n",
        "\n",
        "predicted_outcome_sub = (estimated_shift - context_sub_np) * estimated_scale * inter_est_final\n",
        "\n",
        "# Visualize true vs predicted outcome at subregion level\n",
        "def visualize_true_vs_predicted_subregion(true_data, predicted_data, num_regions=100, num_subregions=4):\n",
        "    grid_size_sub_calc = int(np.sqrt(num_regions) * np.sqrt(num_subregions))\n",
        "    subregions_per_dim = int(np.sqrt(num_subregions))\n",
        "\n",
        "    true_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    predicted_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "    difference_grid = np.zeros((grid_size_sub_calc, grid_size_sub_calc))\n",
        "\n",
        "    for i in range(int(np.sqrt(num_regions))):\n",
        "        for j in range(int(np.sqrt(num_regions))):\n",
        "            region_idx = i * int(np.sqrt(num_regions)) + j\n",
        "            true_sub_data = true_data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            predicted_sub_data = predicted_data[region_idx, :].reshape(subregions_per_dim, subregions_per_dim)\n",
        "            true_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = true_sub_data\n",
        "            predicted_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = predicted_sub_data\n",
        "            difference_grid[i*subregions_per_dim:(i+1)*subregions_per_dim, j*subregions_per_dim:(j+1)*subregions_per_dim] = true_sub_data - predicted_sub_data\n",
        "\n",
        "    # Color maps from seaborn deep palette (assuming these are available in the notebook's global scope)\n",
        "    colors = sns.color_palette(\"deep\")\n",
        "    blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue for Outcome\n",
        "    coolwarm_cmap = 'coolwarm' # Using coolwarm for difference as it was used before\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5)) # Adjusted figure size\n",
        "\n",
        "\n",
        "    # True Outcome (Blue)\n",
        "    sns.heatmap(true_grid, ax=axs[0], cmap=blue_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[0].set_title('True Outcome Subregion')\n",
        "    axs[0].set_xticks([])\n",
        "    axs[0].set_yticks([])\n",
        "\n",
        "    # Predicted Outcome (Blue)\n",
        "    sns.heatmap(predicted_grid, ax=axs[1], cmap=blue_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[1].set_title('Predicted Outcome Subregion')\n",
        "    axs[1].set_xticks([])\n",
        "    axs[1].set_yticks([])\n",
        "\n",
        "    # Difference (Coolwarm)\n",
        "    sns.heatmap(difference_grid, ax=axs[2], cmap=coolwarm_cmap, square=True, cbar=False, linewidths=1) # Added linewidths\n",
        "    axs[2].set_title('Difference (True - Predicted)')\n",
        "    axs[2].set_xticks([])\n",
        "    axs[2].set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(data_dir, 'true_vs_predicted_outcome_subregion_styled.jpg'), dpi=300)\n",
        "    plt.savefig(os.path.join(data_dir, 'exp5_tau_true_vs_predicted_outcome_subregion_styled.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "visualize_true_vs_predicted_subregion(true_outcome_sub, predicted_outcome_sub, num_regions, num_subregions)\n",
        "\n",
        "# Calculate MSE for true vs predicted subregion outcome\n",
        "mse_subregion = np.mean((np.asarray(true_outcome_sub) - np.asarray(predicted_outcome_sub)) ** 2)\n",
        "print(f\"MSE between true and predicted subregion outcome: {mse_subregion}\")"
      ],
      "metadata": {
        "id": "DHbP0us2fg_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exp. 5\n",
        "Unknown Aggregation Functions: Learn both the causal effect of driving bans on air quality and the unknown aggregation rule that maps subregional outcomes to regional reports. The goal is to jointly recover the aggregation mechanism and the fine-scale causal effects."
      ],
      "metadata": {
        "id": "Jb-5lX6HgH2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp. 5 mean aggregation"
      ],
      "metadata": {
        "id": "sFwoEtDisHBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def soft_aggregate_np(values, temperature=1.0):\n",
        "    if temperature <= 0:\n",
        "        raise ValueError(\"Temperature must be positive\")\n",
        "\n",
        "    logits = values / temperature\n",
        "    logits = logits - np.max(logits)  # For numerical stability\n",
        "    weights = np.exp(logits)\n",
        "    weights /= weights.sum()\n",
        "\n",
        "    return np.sum(values * weights)\n"
      ],
      "metadata": {
        "id": "dDoFWS7Bf36C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------- Hyperparameters ----------------\n",
        "OUTFOLDER        = \"data_exp5_mean\"\n",
        "GRID_SIZE        = 10          # number of regions per side\n",
        "SUBREGION_SIZE   = 10          # subregions per region side  (=> 100100 grid)\n",
        "SPATIAL_VARIANCE = 3           # wiggle room for rich/poor counts\n",
        "OUTCOME_NOISE    = 0.02        # stddev of additive noise\n",
        "RICH_VOTE        = 0.80\n",
        "POOR_VOTE        = 0.40\n",
        "INTER_VOTE       = 0.50\n",
        "BASELINE_VOTE    = 0.50\n",
        "N_SIM_EXTRA      = 10\n",
        "RANDOM_SEED      = 42\n",
        "agg_temp         = 6\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "os.makedirs(OUTFOLDER, exist_ok=True)\n",
        "\n",
        "# ---------------- 1) 1010 intervention grid ----------------\n",
        "interventions = rng.integers(0, 2, size=(GRID_SIZE, GRID_SIZE))\n",
        "interventions[0, 0] = 1  # force interventions in the two topleft cells\n",
        "interventions[0, 1] = 1\n",
        "pd.DataFrame(interventions).to_csv(f\"{OUTFOLDER}/interventions.csv\",\n",
        "                                   header=False, index=False)\n",
        "\n",
        "# ---------------- 2) 100100 wealth grid ----------------\n",
        "wealth_hi = np.zeros((GRID_SIZE*SUBREGION_SIZE,\n",
        "                      GRID_SIZE*SUBREGION_SIZE), dtype=int)\n",
        "\n",
        "for i in range(GRID_SIZE):\n",
        "    for j in range(GRID_SIZE):\n",
        "        # pick counts for this 1010 block\n",
        "        n_inter = rng.integers(20, 101)        # 20100 intermediate\n",
        "        remaining = 100 - n_inter\n",
        "        wiggle = min(SPATIAL_VARIANCE, remaining//2)\n",
        "        n_rich = remaining//2 + rng.integers(-wiggle, wiggle+1)\n",
        "        n_rich = np.clip(n_rich, 0, remaining)\n",
        "        n_poor = remaining - n_rich\n",
        "\n",
        "        # special overrides for the two upperleft blocks\n",
        "        if (i, j) == (0, 0):\n",
        "            block_vals = np.full(100, 2)                      # all intermediate\n",
        "        elif (i, j) == (0, 1):\n",
        "            half = 50\n",
        "            block_vals = np.concatenate([np.ones(half,  int),   # 50 poor\n",
        "                                          np.full(half, 3, int)])  # 50 rich\n",
        "            rng.shuffle(block_vals)\n",
        "        else:\n",
        "            block_vals = np.concatenate([np.ones(n_poor, int),\n",
        "                                          np.full(n_inter, 2, int),\n",
        "                                          np.full(n_rich, 3, int)])\n",
        "            rng.shuffle(block_vals)\n",
        "\n",
        "        # place block into wealth_hi\n",
        "        r0, c0 = i*SUBREGION_SIZE, j*SUBREGION_SIZE\n",
        "        wealth_hi[r0:r0+SUBREGION_SIZE, c0:c0+SUBREGION_SIZE] = \\\n",
        "            block_vals.reshape(SUBREGION_SIZE, SUBREGION_SIZE)\n",
        "\n",
        "pd.DataFrame(wealth_hi).to_csv(f\"{OUTFOLDER}/wealth_high_res.csv\",\n",
        "                               header=False, index=False)\n",
        "\n",
        "# ---------------- 3) 1010 coarse (mean) wealth grid ---------\n",
        "wealth_lo = wealth_hi.reshape(GRID_SIZE, SUBREGION_SIZE,\n",
        "                              GRID_SIZE, SUBREGION_SIZE).mean(axis=(1, 3))\n",
        "pd.DataFrame(np.round(wealth_lo, 2)).to_csv(f\"{OUTFOLDER}/wealth_low_res.csv\",\n",
        "                                            header=False, index=False)\n",
        "\n",
        "# ---------------- 4) voting outcome ----------------\n",
        "subregion_noise = np.zeros_like(wealth_hi, dtype=float)  # NEW: store 100x100 noise\n",
        "\n",
        "def vote(sub_wealth, treated, i, j):\n",
        "    \"\"\"Return a single subregion vote in  fraction 01.\"\"\"\n",
        "    noise = rng.normal(0, OUTCOME_NOISE)\n",
        "    subregion_noise[i, j] = noise  # NEW: store sampled noise\n",
        "    if not treated:\n",
        "        return np.clip(BASELINE_VOTE + noise, 0, 1)\n",
        "    # treated case\n",
        "    if sub_wealth == 3:\n",
        "        base = RICH_VOTE\n",
        "    elif sub_wealth == 1:\n",
        "        base = POOR_VOTE\n",
        "    else:\n",
        "        base = INTER_VOTE\n",
        "    return np.clip(base + noise, 0, 1)\n",
        "\n",
        "outcome = np.zeros_like(interventions, dtype=float)\n",
        "\n",
        "for i in range(GRID_SIZE):\n",
        "    for j in range(GRID_SIZE):\n",
        "        r0, c0 = i*SUBREGION_SIZE, j*SUBREGION_SIZE\n",
        "        block_wealth = wealth_hi[r0:r0+SUBREGION_SIZE,\n",
        "                                 c0:c0+SUBREGION_SIZE]\n",
        "        block_treated = bool(interventions[i, j])\n",
        "        votes = [[vote(block_wealth[r, c], block_treated, r0 + r, c0 + c)\n",
        "                  for c in range(SUBREGION_SIZE)]\n",
        "                 for r in range(SUBREGION_SIZE)]\n",
        "        # outcome[i, j] = np.mean(votes) * 100.0        # percentage 0100\n",
        "        outcome[i, j] = soft_aggregate_np(np.array(votes), temperature=agg_temp) * 100.0\n",
        "\n",
        "# Save subregion noise\n",
        "pd.DataFrame(np.round(subregion_noise, 4)).to_csv(f\"{OUTFOLDER}/subregion_noise_gt.csv\",\n",
        "                                                  header=False, index=False)\n",
        "\n",
        "# Save aggregated region-level noise\n",
        "region_noise = subregion_noise.reshape(GRID_SIZE, SUBREGION_SIZE,\n",
        "                                       GRID_SIZE, SUBREGION_SIZE).sum(axis=(1, 3))\n",
        "pd.DataFrame(np.round(region_noise, 4)).to_csv(f\"{OUTFOLDER}/region_noise_gt.csv\",\n",
        "                                               header=False, index=False)\n",
        "\n",
        "# Save outcome as before\n",
        "pd.DataFrame(np.round(outcome, 2)).to_csv(f\"{OUTFOLDER}/outcome.csv\",\n",
        "                                          header=False, index=False)\n",
        "\n",
        "\n",
        "# ---------------- 5) counterfactual outcome (GT) ----------------\n",
        "# Invert top half (rows 0-4) of intervention matrix\n",
        "interventions_cf = interventions.copy()\n",
        "interventions_cf[:GRID_SIZE//2, :] = 1 - interventions_cf[:GRID_SIZE//2, :]\n",
        "\n",
        "pd.DataFrame(interventions_cf).to_csv(f\"{OUTFOLDER}/interventions_cf_gt.csv\",\n",
        "                                      header=False, index=False)\n",
        "\n",
        "# Compute counterfactual using same noise\n",
        "outcome_cf = np.zeros_like(interventions_cf, dtype=float)\n",
        "\n",
        "for i in range(GRID_SIZE):\n",
        "    for j in range(GRID_SIZE):\n",
        "        r0, c0 = i*SUBREGION_SIZE, j*SUBREGION_SIZE\n",
        "        block_wealth = wealth_hi[r0:r0+SUBREGION_SIZE, c0:c0+SUBREGION_SIZE]\n",
        "        block_treated = bool(interventions_cf[i, j])\n",
        "        votes = [[\n",
        "            # reuse stored noise instead of generating new one\n",
        "            np.clip(\n",
        "                (RICH_VOTE if w == 3 else POOR_VOTE if w == 1 else INTER_VOTE) + subregion_noise[r0 + r, c0 + c]\n",
        "                if block_treated else BASELINE_VOTE + subregion_noise[r0 + r, c0 + c],\n",
        "                0, 1\n",
        "            )\n",
        "            for c, w in enumerate(block_wealth[r])]\n",
        "            for r in range(SUBREGION_SIZE)]\n",
        "        # outcome_cf[i, j] = np.mean(votes) * 100.0\n",
        "        outcome_cf[i, j] = soft_aggregate_np(np.array(votes), temperature=agg_temp) * 100.0\n",
        "\n",
        "\n",
        "pd.DataFrame(np.round(outcome_cf, 2)).to_csv(f\"{OUTFOLDER}/outcome_cf_gt.csv\",\n",
        "                                             header=False, index=False)\n",
        "\n",
        "print(\" Synthetic data and counterfactuals written to:\", OUTFOLDER)"
      ],
      "metadata": {
        "id": "6QmNQnR1geOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set output folder\n",
        "OUTFOLDER = \"data_exp5_mean\"\n",
        "\n",
        "# Load wealth and noise\n",
        "wealth_hi = pd.read_csv(f\"{OUTFOLDER}/wealth_high_res.csv\", header=None).values\n",
        "subregion_noise = pd.read_csv(f\"{OUTFOLDER}/subregion_noise_gt.csv\", header=None).values\n",
        "\n",
        "# Define constants from data generation\n",
        "BASELINE_VOTE = 0.50\n",
        "RICH_VOTE = 0.80\n",
        "POOR_VOTE = 0.40\n",
        "INTER_VOTE = 0.50\n",
        "\n",
        "# Compute control outcome (no intervention) for each sub-cell\n",
        "control_outcome = np.clip(BASELINE_VOTE + subregion_noise, 0, 1) * 100.0\n",
        "\n",
        "# Compute treated outcome (with intervention) for each sub-cell\n",
        "treated_outcome = np.zeros_like(subregion_noise)\n",
        "for i in range(wealth_hi.shape[0]):\n",
        "    for j in range(wealth_hi.shape[1]):\n",
        "        w = wealth_hi[i, j]\n",
        "        base = RICH_VOTE if w == 3 else POOR_VOTE if w == 1 else INTER_VOTE\n",
        "        treated_outcome[i, j] = np.clip(base + subregion_noise[i, j], 0, 1) * 100.0\n",
        "\n",
        "# Compute causal effect: treated - control\n",
        "causal_effect = treated_outcome - control_outcome\n",
        "\n",
        "# Save to CSVs\n",
        "pd.DataFrame(np.round(control_outcome, 2)).to_csv(f\"{OUTFOLDER}/control_sub.csv\", header=False, index=False)\n",
        "pd.DataFrame(np.round(treated_outcome, 2)).to_csv(f\"{OUTFOLDER}/treated_sub.csv\", header=False, index=False)\n",
        "pd.DataFrame(np.round(causal_effect, 2)).to_csv(f\"{OUTFOLDER}/effect_sub.csv\", header=False, index=False)\n",
        "\n",
        "# Normalize for plotting (0-100 to 0-1 for outcomes, effect min-max to 0-1? but for consistency vmin vmax)\n",
        "control_norm = control_outcome #/ 100.0\n",
        "treated_norm = treated_outcome #/ 100.0\n",
        "effect_norm = causal_effect #/ 100.0  # since difference is -0.1 to 0.3 roughly\n",
        "\n",
        "# Color maps from seaborn deep palette\n",
        "colors = sns.color_palette(\"deep\")\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue for outcomes\n",
        "pink_cmap = sns.light_palette(colors[4], as_cmap=True)  # Pink for effect\n",
        "\n",
        "# Parameter to control space between subplots\n",
        "space_between = 0.2\n",
        "\n",
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Left: Control outcome in blue\n",
        "sns.heatmap(control_norm, ax=axs[0], cmap=blue_cmap, cbar=False, square=True, linewidths=0.0)#, vmin=0.0, vmax=1.0)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "\n",
        "# Center: Treated outcome in blue\n",
        "sns.heatmap(treated_norm, ax=axs[1], cmap=blue_cmap, cbar=False, square=True, linewidths=0.0)#, vmin=0.0, vmax=1.0)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "\n",
        "# Right: Causal effect in pink\n",
        "sns.heatmap(effect_norm, ax=axs[2], cmap=pink_cmap, cbar=False, square=True, linewidths=0.0)#, vmin=-0.2, vmax=0.4)  # Adjusted vmin/vmax for effect range\n",
        "axs[2].set_xticks([])\n",
        "axs[2].set_yticks([])\n",
        "\n",
        "\n",
        "# Adjust the space between subplots\n",
        "fig.subplots_adjust(wspace=space_between)\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(f'{OUTFOLDER}/sub_heatmaps.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(f'{OUTFOLDER}/sub_heatmaps.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g0YVxrwggmHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Set output folder\n",
        "OUTFOLDER = \"data_exp5_mean\"\n",
        "\n",
        "# Load the data\n",
        "interv_data = pd.read_csv(f\"{OUTFOLDER}/interventions.csv\", header=None).values.astype(int)\n",
        "outcome_data = pd.read_csv(f\"{OUTFOLDER}/outcome.csv\", header=None).values / 100.0\n",
        "context_data = pd.read_csv(f\"{OUTFOLDER}/wealth_high_res.csv\", header=None).values\n",
        "context_data_norm = (context_data - 1) / 2.0  # Normalize assuming values 1,2,3 to 0-1\n",
        "\n",
        "# Color maps from seaborn deep palette\n",
        "colors = sns.color_palette(\"deep\")\n",
        "red_cmap = sns.light_palette(colors[3], as_cmap=True)  # Red\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue\n",
        "green_cmap = sns.light_palette(colors[2], as_cmap=True)  # Green\n",
        "\n",
        "# Parameter to control space between subplots (as fraction of average axis width)\n",
        "space_between = 0.05  # Adjust this value as needed (e.g., 0.0 for no space, 0.5 for more space)\n",
        "\n",
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Left: Intervention in red\n",
        "sns.heatmap(interv_data, ax=axs[0], cmap=red_cmap, cbar=False, square=True, linewidths=2, vmin=0.0, vmax=1.0)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "# Center: Outcome in blue\n",
        "sns.heatmap(outcome_data, ax=axs[1], cmap=blue_cmap, cbar=False, square=True, linewidths=2, vmin=0.4, vmax=0.6)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "# Right: Context in green\n",
        "sns.heatmap(context_data_norm, ax=axs[2], cmap=green_cmap, cbar=False, square=True, linewidths=0, vmin=0.0, vmax=1.0)\n",
        "axs[2].set_xticks([])\n",
        "axs[2].set_yticks([])\n",
        "\n",
        "\n",
        "# Adjust the space between subplots\n",
        "fig.subplots_adjust(wspace=space_between)\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(f'{OUTFOLDER}/combined_heatmaps.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(f'{OUTFOLDER}/combined_heatmaps.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "czBm1n23gn-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_theme(style=\"white\")\n",
        "\n",
        "# -------------- parameters -------------\n",
        "OUTFOLDER = \"data_exp5_mean\"\n",
        "N_SIM_PER_ARM = 100 # 10 untreated + 10 treated (=20 total)\n",
        "RANDOM_SEED = 100\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "# ---------- load wealth & helper vote() ----------\n",
        "wealth_hi = pd.read_csv(f\"{OUTFOLDER}/wealth_high_res.csv\",\n",
        "                        header=None).values\n",
        "GRID_SIZE = 10\n",
        "SUBREGION_SIZE = 10\n",
        "\n",
        "def vote(sub_wealth, treated):\n",
        "    noise = rng.normal(0, OUTCOME_NOISE)\n",
        "    if not treated:\n",
        "        return np.clip(BASELINE_VOTE + noise, 0, 1)\n",
        "    base = RICH_VOTE if sub_wealth==3 else POOR_VOTE if sub_wealth==1 else INTER_VOTE\n",
        "    return np.clip(base + noise, 0, 1)\n",
        "\n",
        "def simulate_outcome(intervention_grid):\n",
        "    \"\"\"Return a 1010 matrix of region votes (0100%).\"\"\"\n",
        "    out = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "    for i in range(GRID_SIZE):\n",
        "        for j in range(GRID_SIZE):\n",
        "            r0, c0 = i*SUBREGION_SIZE, j*SUBREGION_SIZE\n",
        "            block_wealth = wealth_hi[r0:r0+SUBREGION_SIZE,\n",
        "                                     c0:c0+SUBREGION_SIZE]\n",
        "            treated = bool(intervention_grid[i, j])\n",
        "            votes = [[vote(block_wealth[r, c], treated)\n",
        "                     for c in range(SUBREGION_SIZE)]\n",
        "                     for r in range(SUBREGION_SIZE)]\n",
        "            out[i, j] = np.mean(votes)*100\n",
        "    return out\n",
        "\n",
        "# ---------- run simulations ----------\n",
        "base_interv = pd.read_csv(f\"{OUTFOLDER}/interventions.csv\",\n",
        "                          header=None).values\n",
        "global_diffs = []\n",
        "all_treated_runs = []\n",
        "all_control_runs = []\n",
        "\n",
        "for idx in range(N_SIM_PER_ARM):\n",
        "    # ---- control (all zeros) ----\n",
        "    interv_off = np.zeros_like(base_interv)\n",
        "    out_off = simulate_outcome(interv_off)\n",
        "    pd.DataFrame(np.round(out_off, 2)).to_csv(\n",
        "        f\"{OUTFOLDER}/outcome_treatment_off_{idx:02d}.csv\",\n",
        "        header=False, index=False)\n",
        "    all_control_runs.append(out_off)\n",
        "\n",
        "    # ---- treated (all ones) ----\n",
        "    interv_on = np.ones_like(base_interv)\n",
        "    out_on = simulate_outcome(interv_on)\n",
        "    pd.DataFrame(np.round(out_on, 2)).to_csv(\n",
        "        f\"{OUTFOLDER}/outcome_treatment_on_{idx:02d}.csv\",\n",
        "        header=False, index=False)\n",
        "    all_treated_runs.append(out_on)\n",
        "\n",
        "    # global ATE for this pair\n",
        "    global_diffs.append(out_on.mean() - out_off.mean())\n",
        "\n",
        "all_control_runs = np.stack(all_control_runs) # shape (10,10,10)\n",
        "all_treated_runs = np.stack(all_treated_runs)\n",
        "\n",
        "# ---------- compute regional ATE ----------\n",
        "ate_grid = all_treated_runs.mean(axis=0) - all_control_runs.mean(axis=0)\n",
        "pd.DataFrame(np.round(ate_grid, 2)).to_csv(f\"{OUTFOLDER}/ate.csv\",\n",
        "                                           header=False, index=False)\n",
        "\n",
        "# ---------- PLOT 1: heatmap ----------\n",
        "plt.figure(figsize=(6,6))\n",
        "ax = sns.heatmap(ate_grid, annot=np.round(ate_grid,2), fmt=\".2f\",\n",
        "                 cmap=\"RdBu_r\", center=0, cbar_kws={'label': 'ATE (%)'})\n",
        "ax.set_title(\"Average Treatment Effect per region\")\n",
        "ax.set_xticks([]); ax.set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTFOLDER}/ate_heatmap.jpg\", dpi=300)\n",
        "plt.savefig(f\"{OUTFOLDER}/ate_heatmap.pdf\")\n",
        "plt.show()\n",
        "\n",
        "# ---------- PLOT 1b: minimal heatmap ----------\n",
        "colors = sns.color_palette(\"deep\")\n",
        "cmap = sns.light_palette(colors[4], as_cmap=True)\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "sns.heatmap(ate_grid, ax=ax, cmap=cmap, cbar=False, square=True, linewidths=2, vmin=0.0, vmax=9.0)\n",
        "ax.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "fig.savefig(f\"{OUTFOLDER}/ate_heatmap_minimal.jpg\", dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(f\"{OUTFOLDER}/ate_heatmap_minimal.pdf\", bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "# ---------- PLOT 2: violin of global ATE ----------\n",
        "plt.figure(figsize=(4,6))\n",
        "sns.violinplot(y=global_diffs, color=sns.color_palette(\"deep\")[3],\n",
        "               inner=None, cut=0)\n",
        "sns.stripplot(y=global_diffs, color=sns.color_palette(\"deep\")[0],\n",
        "              size=8, alpha=0.7)\n",
        "plt.axhline(np.mean(global_diffs), ls=\"--\", lw=1,\n",
        "            label=f\"Mean ATE = {np.mean(global_diffs):.2f}%\")\n",
        "plt.ylabel(\"Global ATE (%)\")\n",
        "plt.title(\"Overall Average Treatment Effect\")\n",
        "plt.legend(frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTFOLDER}/ate_global_violin.jpg\", dpi=300)\n",
        "plt.savefig(f\"{OUTFOLDER}/ate_global_violin.pdf\")\n",
        "plt.show()\n",
        "\n",
        "print(\" ATE analysis complete  results stored in\", OUTFOLDER)"
      ],
      "metadata": {
        "id": "fG3eg4MOhVJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_aggregate(values, temperature=1.0):\n",
        "    if isinstance(temperature, torch.Tensor):\n",
        "        if (temperature <= 0).any():\n",
        "            raise ValueError(\"Temperature must be positive\")\n",
        "    elif temperature <= 0:\n",
        "        raise ValueError(\"Temperature must be positive\")\n",
        "\n",
        "    logits = values / temperature\n",
        "    logits = logits - torch.amax(logits, dim=-1, keepdim=True)  # numerical stability\n",
        "    weights = torch.softmax(logits, dim=-1)  # (..., K)\n",
        "    return (values * weights).sum(dim=-1)     # (...,)"
      ],
      "metadata": {
        "id": "4Y1hVMlQhmvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "OUTFOLDER = \"data_exp5_mean\"\n",
        "N_EPOCHS = 1000\n",
        "LR = 0.01\n",
        "SEED = 42\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "LOGLOG_FLAG = True  # Whether to plot losses in log-log scale\n",
        "\n",
        "# ------------------ Set seeds ------------------\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ------------------ Load data ------------------\n",
        "interv_np  = pd.read_csv(f\"{OUTFOLDER}/interventions.csv\",   header=None).values  # 0/1\n",
        "wealth_hi  = pd.read_csv(f\"{OUTFOLDER}/wealth_high_res.csv\", header=None).values  # 100100 wealth classes\n",
        "outcome_np = pd.read_csv(f\"{OUTFOLDER}/outcome.csv\",         header=None).values  # region outcomes %\n",
        "\n",
        "# Convert to tensor\n",
        "outcome_t = torch.tensor(outcome_np, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "# ------------------ Known Constants ------------------\n",
        "BASELINE_VOTE = 0.50\n",
        "RICH_VOTE     = 0.80\n",
        "INTER_VOTE    = 0.50\n",
        "POOR_VOTE     = 0.40\n",
        "\n",
        "# ------------------ Initialize Parameters ------------------\n",
        "params = torch.nn.Parameter(torch.full((6,), 0.5, dtype=torch.float32, device=DEVICE))  # 6 s\n",
        "log_tau = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float32, device=DEVICE))     # log temperature\n",
        "opt = torch.optim.Adam([params, log_tau], lr=LR)\n",
        "\n",
        "loss_hist = []\n",
        "ce_loss_hist = []\n",
        "temp_hist = []\n",
        "\n",
        "# ------------------ Training Loop ------------------\n",
        "for epoch in range(N_EPOCHS):\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # Compute predicted outcomes at subregion level\n",
        "    wealth_hi_t = torch.tensor(wealth_hi, dtype=torch.float32, device=DEVICE)\n",
        "    wealth_hi_reshaped = wealth_hi_t.reshape(10, 10, 10, 10).permute(0, 2, 1, 3).reshape(10, 10, 100)\n",
        "    treated_mask_t = torch.tensor(interv_np, dtype=torch.float32, device=DEVICE) == 1\n",
        "    treated_mask_expanded = treated_mask_t.unsqueeze(-1).expand(-1, -1, 100)\n",
        "\n",
        "    params_expanded = params.unsqueeze(0).unsqueeze(0).expand(10, 10, -1)\n",
        "\n",
        "    predicted_sub_outcomes = torch.where(\n",
        "        treated_mask_expanded,\n",
        "        torch.where(\n",
        "            wealth_hi_reshaped == 3, params_expanded[:, :, 5].unsqueeze(-1),\n",
        "            torch.where(wealth_hi_reshaped == 1, params_expanded[:, :, 3].unsqueeze(-1), params_expanded[:, :, 4].unsqueeze(-1))\n",
        "        ),\n",
        "        torch.where(\n",
        "            wealth_hi_reshaped == 3, params_expanded[:, :, 2].unsqueeze(-1),\n",
        "            torch.where(wealth_hi_reshaped == 1, params_expanded[:, :, 0].unsqueeze(-1), params_expanded[:, :, 1].unsqueeze(-1))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Soft aggregation\n",
        "    temperature = torch.exp(log_tau)\n",
        "    mean_pred = soft_aggregate(predicted_sub_outcomes, temperature=temperature) * 100  # %\n",
        "\n",
        "    # Compute region-level MSE loss\n",
        "    loss = torch.mean((mean_pred - outcome_t) ** 2)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Log losses and temperature\n",
        "    loss_hist.append(loss.item())\n",
        "    temp_hist.append(temperature.item())\n",
        "\n",
        "    # Compute causal effect MSE loss (against true values)\n",
        "    with torch.no_grad():\n",
        "        est_causal_effect = (params[3:] - params[:3])  # estimated 1 - 0\n",
        "        true_causal_effect = torch.tensor([\n",
        "            (POOR_VOTE - BASELINE_VOTE),\n",
        "            (INTER_VOTE - BASELINE_VOTE),\n",
        "            (RICH_VOTE  - BASELINE_VOTE)\n",
        "        ], device=DEVICE)\n",
        "\n",
        "        ce_mse = torch.mean((est_causal_effect - true_causal_effect) ** 2).item()\n",
        "        ce_loss_hist.append(ce_mse)\n",
        "\n",
        "# ------------------ Save Loss Logs ------------------\n",
        "loss_df = pd.DataFrame({\n",
        "    'region_loss': loss_hist,\n",
        "    'causal_effect_loss': ce_loss_hist,\n",
        "    'temperature': temp_hist\n",
        "})\n",
        "loss_df.to_csv(f\"{OUTFOLDER}/estimation_loss.csv\", index_label=\"epoch\")\n",
        "\n",
        "# ------------------ Plot Loss Curves ------------------\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "colors = sns.color_palette(\"deep\")\n",
        "epochs = np.arange(1, N_EPOCHS + 1)\n",
        "\n",
        "# Region Loss\n",
        "ax1.set_xlabel('Epoch', fontsize=16)\n",
        "ax1.set_ylabel('Training Loss', fontsize=16, color=colors[0])\n",
        "if LOGLOG_FLAG:\n",
        "    ax1.loglog(epochs, loss_hist, color=colors[0], lw=2.5)\n",
        "else:\n",
        "    ax1.plot(epochs, loss_hist, color=colors[0], lw=2)\n",
        "ax1.tick_params(axis='y', labelcolor=colors[0])\n",
        "\n",
        "# Causal Effect Loss\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Causal Effect MSE', fontsize=16, color=colors[3])\n",
        "if LOGLOG_FLAG:\n",
        "    ax2.loglog(epochs, ce_loss_hist, color=colors[3], ls='--', lw=2.5)\n",
        "else:\n",
        "    ax2.plot(epochs, ce_loss_hist, color=colors[3], ls='--', lw=2)\n",
        "ax2.tick_params(axis='y', labelcolor=colors[3])\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(f\"{OUTFOLDER}/estimation_loss.jpg\", dpi=300)\n",
        "fig.savefig(f\"{OUTFOLDER}/estimation_loss.pdf\")\n",
        "plt.show()\n",
        "\n",
        "# ------------------ Save Parameters ------------------\n",
        "param_names = [\"mu_poor_0\", \"mu_inter_0\", \"mu_rich_0\",\n",
        "               \"mu_poor_1\", \"mu_inter_1\", \"mu_rich_1\", \"temperature\"]\n",
        "\n",
        "values = np.append(params.detach().cpu().numpy(), temperature.item())\n",
        "results_df = pd.DataFrame({'parameter': param_names, 'value': values})\n",
        "results_df.to_csv(f\"{OUTFOLDER}/results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "2vmgslWb3pwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "data_dir = 'data_exp5_mean'\n",
        "loglog_flag = True  # Set to True for log-log plot, False for linear plot\n",
        "\n",
        "# Load loss history from CSV\n",
        "loss_data = pd.read_csv(os.path.join(data_dir, 'estimation_loss.csv'))\n",
        "loss_hist = loss_data['region_loss'].values\n",
        "ce_loss_hist = loss_data['causal_effect_loss'].values\n",
        "epochs = np.arange(1, len(loss_hist) + 1)\n",
        "\n",
        "# Plotting\n",
        "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "colors = sns.color_palette(\"deep\")\n",
        "\n",
        "ax1.set_xlabel('Epoch', fontsize=20)\n",
        "ax1.set_ylabel('Training Loss', fontsize=20, color=colors[0])\n",
        "if loglog_flag:\n",
        "    ax1.loglog(epochs, loss_hist, color=colors[0], alpha=0.9, lw=3)\n",
        "else:\n",
        "    ax1.plot(epochs, loss_hist, color=colors[0], alpha=0.7, lw=2)\n",
        "ax1.tick_params(axis='both', which='both', length=0, labelsize=18, colors=colors[0])\n",
        "ax1.set_title('MSE Curves', fontsize=22)\n",
        "ax1.spines['top'].set_visible(False)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('MSE Causal Effect', fontsize=20, color=colors[3])\n",
        "if loglog_flag:\n",
        "    ax2.loglog(epochs, ce_loss_hist, color=colors[3], alpha=0.9, ls='--', lw=5)\n",
        "else:\n",
        "    ax2.plot(epochs, ce_loss_hist, color=colors[3], alpha=0.7, ls='--', lw=2)\n",
        "ax2.tick_params(axis='y', which='both', length=0, labelsize=18, colors=colors[3])\n",
        "ax2.spines['top'].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(os.path.join(data_dir, 'estimation_loss.jpg'), dpi=300)\n",
        "fig.savefig(os.path.join(data_dir, 'estimation_loss.pdf'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ahb3IsXriF1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Set output folder\n",
        "OUTFOLDER = \"data_exp5_mean\"\n",
        "\n",
        "# Load data generated in previous cells\n",
        "wealth_hi = pd.read_csv(f\"{OUTFOLDER}/wealth_high_res.csv\", header=None).values\n",
        "subregion_noise = pd.read_csv(f\"{OUTFOLDER}/subregion_noise_gt.csv\", header=None).values # True noise\n",
        "\n",
        "params = results_df[\"value\"].values[:6] # Exclude temperature\n",
        "learned_temp = results_df[\"value\"].values[6] # Learned temperature\n",
        "\n",
        "\n",
        "# Define constants from data generation\n",
        "BASELINE_VOTE = 0.50\n",
        "RICH_VOTE = 0.80\n",
        "POOR_VOTE = 0.40\n",
        "INTER_VOTE = 0.50\n",
        "\n",
        "# ---------------- Compute True Causal Effect (Subregion Level) ----------------\n",
        "# Compute control outcome (no intervention) for each sub-cell using true noise\n",
        "control_outcome_true = np.clip(BASELINE_VOTE + subregion_noise, 0, 1) * 100.0\n",
        "\n",
        "# Compute treated outcome (with intervention) for each sub-cell using true noise\n",
        "treated_outcome_true = np.zeros_like(subregion_noise)\n",
        "for i in range(wealth_hi.shape[0]):\n",
        "    for j in range(wealth_hi.shape[1]):\n",
        "        w = wealth_hi[i, j]\n",
        "        base = RICH_VOTE if w == 3 else POOR_VOTE if w == 1 else INTER_VOTE\n",
        "        treated_outcome_true[i, j] = np.clip(base + subregion_noise[i, j], 0, 1) * 100.0\n",
        "\n",
        "# Compute true causal effect: treated - control\n",
        "causal_effect_true = treated_outcome_true - control_outcome_true\n",
        "\n",
        "# ---------------- Compute Estimated Causal Effect (Subregion Level) ----------------\n",
        "estimated_control_outcome_base = np.zeros_like(wealth_hi, dtype=float)\n",
        "estimated_treated_outcome_base = np.zeros_like(wealth_hi, dtype=float)\n",
        "\n",
        "for i in range(wealth_hi.shape[0]):\n",
        "    for j in range(wealth_hi.shape[1]):\n",
        "        w = wealth_hi[i, j]\n",
        "        # Untreated (control)\n",
        "        if w == 1: # poor\n",
        "            estimated_control_outcome_base[i, j] = params[0]\n",
        "        elif w == 2: # intermediate\n",
        "            estimated_control_outcome_base[i, j] = params[1]\n",
        "        elif w == 3: # rich\n",
        "            estimated_control_outcome_base[i, j] = params[2]\n",
        "\n",
        "        # Treated\n",
        "        if w == 1: # poor\n",
        "             estimated_treated_outcome_base[i, j] = params[3]\n",
        "        elif w == 2: # intermediate\n",
        "             estimated_treated_outcome_base[i, j] = params[4]\n",
        "        elif w == 3: # rich\n",
        "             estimated_treated_outcome_base[i, j] = params[5]\n",
        "\n",
        "\n",
        "estimated_causal_effect = (estimated_treated_outcome_base - estimated_control_outcome_base) * 100.0 # Convert to percentage\n",
        "\n",
        "# Define shared colormap and range\n",
        "colors = sns.color_palette(\"deep\")\n",
        "pink_cmap = sns.light_palette(colors[4], as_cmap=True)\n",
        "vmin = np.min([causal_effect_true, estimated_causal_effect])\n",
        "vmax = np.max([causal_effect_true, estimated_causal_effect])\n",
        "\n",
        "# Create figure and axes with space for the colorbar\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.05})\n",
        "\n",
        "# Plot true causal effect\n",
        "sns.heatmap(causal_effect_true, ax=axs[0], cmap=pink_cmap, cbar=False, square=True,\n",
        "            vmin=vmin, vmax=vmax, linewidths=0)\n",
        "axs[0].set_title(\"True Causal Effect (Subregion)\")\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "# Plot estimated causal effect\n",
        "sns.heatmap(estimated_causal_effect, ax=axs[1], cmap=pink_cmap, cbar=False, square=True,\n",
        "            vmin=vmin, vmax=vmax, linewidths=0)\n",
        "axs[1].set_title(\"Estimated Causal Effect (Subregion)\")\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "# Add a single shared vertical colorbar to the right\n",
        "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
        "sm = mpl.cm.ScalarMappable(cmap=pink_cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "\n",
        "# Add colorbar to the right of both plots\n",
        "cbar = fig.colorbar(sm, ax=axs, orientation='vertical', fraction=0.046, pad=0.04)\n",
        "cbar.set_label(\"Causal Effect (%)\")\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(f'{OUTFOLDER}/subregion_causal_effects_heatmaps.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(f'{OUTFOLDER}/subregion_causal_effects_heatmaps.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "print(\" Subregion causal effect plots generated.\")\n"
      ],
      "metadata": {
        "id": "JFmkrFbNiQ-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exp. 5 max aggregation"
      ],
      "metadata": {
        "id": "ptzGuDGKsv86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------- Hyperparameters ----------------\n",
        "OUTFOLDER        = \"data_exp5_max\"\n",
        "GRID_SIZE        = 10          # number of regions per side\n",
        "SUBREGION_SIZE   = 10          # subregions per region side\n",
        "SPATIAL_VARIANCE = 3           # wiggle room for rich/poor counts\n",
        "OUTCOME_NOISE    = 0.02        # stddev of additive noise\n",
        "RICH_VOTE        = 0.80\n",
        "POOR_VOTE        = 0.40\n",
        "INTER_VOTE       = 0.50\n",
        "BASELINE_VOTE    = 0.50\n",
        "N_SIM_EXTRA      = 10\n",
        "RANDOM_SEED      = 42\n",
        "agg_temp         = 0.1\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "os.makedirs(OUTFOLDER, exist_ok=True)\n",
        "\n",
        "# ---------------- 1) 1010 intervention grid ----------------\n",
        "interventions = rng.integers(0, 2, size=(GRID_SIZE, GRID_SIZE))\n",
        "interventions[0, 0] = 1  # force interventions in the two topleft cells\n",
        "interventions[0, 1] = 1\n",
        "pd.DataFrame(interventions).to_csv(f\"{OUTFOLDER}/interventions.csv\",\n",
        "                                   header=False, index=False)\n",
        "\n",
        "# ---------------- 2) 100100 wealth grid ----------------\n",
        "wealth_hi = np.zeros((GRID_SIZE*SUBREGION_SIZE,\n",
        "                      GRID_SIZE*SUBREGION_SIZE), dtype=int)\n",
        "\n",
        "for i in range(GRID_SIZE):\n",
        "    for j in range(GRID_SIZE):\n",
        "        # pick counts for this 1010 block\n",
        "        n_inter = rng.integers(20, 101)        # 20100 intermediate\n",
        "        remaining = 100 - n_inter\n",
        "        wiggle = min(SPATIAL_VARIANCE, remaining//2)\n",
        "        n_rich = remaining//2 + rng.integers(-wiggle, wiggle+1)\n",
        "        n_rich = np.clip(n_rich, 0, remaining)\n",
        "        n_poor = remaining - n_rich\n",
        "\n",
        "        # special overrides for the two upperleft blocks\n",
        "        if (i, j) == (0, 0):\n",
        "            block_vals = np.full(100, 2)                      # all intermediate\n",
        "        elif (i, j) == (0, 1):\n",
        "            half = 50\n",
        "            block_vals = np.concatenate([np.ones(half,  int),   # 50 poor\n",
        "                                          np.full(half, 3, int)])  # 50 rich\n",
        "            rng.shuffle(block_vals)\n",
        "        else:\n",
        "            block_vals = np.concatenate([np.ones(n_poor, int),\n",
        "                                          np.full(n_inter, 2, int),\n",
        "                                          np.full(n_rich, 3, int)])\n",
        "            rng.shuffle(block_vals)\n",
        "\n",
        "        # place block into wealth_hi\n",
        "        r0, c0 = i*SUBREGION_SIZE, j*SUBREGION_SIZE\n",
        "        wealth_hi[r0:r0+SUBREGION_SIZE, c0:c0+SUBREGION_SIZE] = \\\n",
        "            block_vals.reshape(SUBREGION_SIZE, SUBREGION_SIZE)\n",
        "\n",
        "pd.DataFrame(wealth_hi).to_csv(f\"{OUTFOLDER}/wealth_high_res.csv\",\n",
        "                               header=False, index=False)\n",
        "\n",
        "# ---------------- 3) 1010 coarse (mean) wealth grid ---------\n",
        "wealth_lo = wealth_hi.reshape(GRID_SIZE, SUBREGION_SIZE,\n",
        "                              GRID_SIZE, SUBREGION_SIZE).mean(axis=(1, 3))\n",
        "pd.DataFrame(np.round(wealth_lo, 2)).to_csv(f\"{OUTFOLDER}/wealth_low_res.csv\",\n",
        "                                            header=False, index=False)\n",
        "\n",
        "# ---------------- 4) voting outcome ----------------\n",
        "subregion_noise = np.zeros_like(wealth_hi, dtype=float)  # NEW: store 100x100 noise\n",
        "\n",
        "def vote(sub_wealth, treated, i, j):\n",
        "    \"\"\"Return a single subregion vote in  fraction 01.\"\"\"\n",
        "    noise = rng.normal(0, OUTCOME_NOISE)\n",
        "    subregion_noise[i, j] = noise  # NEW: store sampled noise\n",
        "    if not treated:\n",
        "        return np.clip(BASELINE_VOTE + noise, 0, 1)\n",
        "    # treated case\n",
        "    if sub_wealth == 3:\n",
        "        base = RICH_VOTE\n",
        "    elif sub_wealth == 1:\n",
        "        base = POOR_VOTE\n",
        "    else:\n",
        "        base = INTER_VOTE\n",
        "    return np.clip(base + noise, 0, 1)\n",
        "\n",
        "outcome = np.zeros_like(interventions, dtype=float)\n",
        "\n",
        "for i in range(GRID_SIZE):\n",
        "    for j in range(GRID_SIZE):\n",
        "        r0, c0 = i*SUBREGION_SIZE, j*SUBREGION_SIZE\n",
        "        block_wealth = wealth_hi[r0:r0+SUBREGION_SIZE,\n",
        "                                 c0:c0+SUBREGION_SIZE]\n",
        "        block_treated = bool(interventions[i, j])\n",
        "        votes = [[vote(block_wealth[r, c], block_treated, r0 + r, c0 + c)\n",
        "                  for c in range(SUBREGION_SIZE)]\n",
        "                 for r in range(SUBREGION_SIZE)]\n",
        "        # outcome[i, j] = np.mean(votes) * 100.0        # percentage 0100\n",
        "        outcome[i, j] = soft_aggregate_np(np.array(votes), temperature=agg_temp) * 100.0\n",
        "\n",
        "# Save subregion noise\n",
        "pd.DataFrame(np.round(subregion_noise, 4)).to_csv(f\"{OUTFOLDER}/subregion_noise_gt.csv\",\n",
        "                                                  header=False, index=False)\n",
        "\n",
        "# Save aggregated region-level noise\n",
        "region_noise = subregion_noise.reshape(GRID_SIZE, SUBREGION_SIZE,\n",
        "                                       GRID_SIZE, SUBREGION_SIZE).sum(axis=(1, 3))\n",
        "pd.DataFrame(np.round(region_noise, 4)).to_csv(f\"{OUTFOLDER}/region_noise_gt.csv\",\n",
        "                                               header=False, index=False)\n",
        "\n",
        "# Save outcome as before\n",
        "pd.DataFrame(np.round(outcome, 2)).to_csv(f\"{OUTFOLDER}/outcome.csv\",\n",
        "                                          header=False, index=False)\n",
        "\n",
        "\n",
        "# ---------------- 5) counterfactual outcome (GT) ----------------\n",
        "# Invert top half (rows 0-4) of intervention matrix\n",
        "interventions_cf = interventions.copy()\n",
        "interventions_cf[:GRID_SIZE//2, :] = 1 - interventions_cf[:GRID_SIZE//2, :]\n",
        "\n",
        "pd.DataFrame(interventions_cf).to_csv(f\"{OUTFOLDER}/interventions_cf_gt.csv\",\n",
        "                                      header=False, index=False)\n",
        "\n",
        "# Compute counterfactual using same noise\n",
        "outcome_cf = np.zeros_like(interventions_cf, dtype=float)\n",
        "\n",
        "for i in range(GRID_SIZE):\n",
        "    for j in range(GRID_SIZE):\n",
        "        r0, c0 = i*SUBREGION_SIZE, j*SUBREGION_SIZE\n",
        "        block_wealth = wealth_hi[r0:r0+SUBREGION_SIZE, c0:c0+SUBREGION_SIZE]\n",
        "        block_treated = bool(interventions_cf[i, j])\n",
        "        votes = [[\n",
        "            # reuse stored noise instead of generating new one\n",
        "            np.clip(\n",
        "                (RICH_VOTE if w == 3 else POOR_VOTE if w == 1 else INTER_VOTE) + subregion_noise[r0 + r, c0 + c]\n",
        "                if block_treated else BASELINE_VOTE + subregion_noise[r0 + r, c0 + c],\n",
        "                0, 1\n",
        "            )\n",
        "            for c, w in enumerate(block_wealth[r])]\n",
        "            for r in range(SUBREGION_SIZE)]\n",
        "        outcome_cf[i, j] = soft_aggregate_np(np.array(votes), temperature=agg_temp) * 100.0\n",
        "\n",
        "\n",
        "pd.DataFrame(np.round(outcome_cf, 2)).to_csv(f\"{OUTFOLDER}/outcome_cf_gt.csv\",\n",
        "                                             header=False, index=False)\n",
        "\n",
        "print(\" Synthetic data and counterfactuals written to:\", OUTFOLDER)"
      ],
      "metadata": {
        "id": "kdEiZR_vi_KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set output folder\n",
        "OUTFOLDER = \"data_exp5_max\"\n",
        "\n",
        "# Load wealth and noise\n",
        "wealth_hi = pd.read_csv(f\"{OUTFOLDER}/wealth_high_res.csv\", header=None).values\n",
        "subregion_noise = pd.read_csv(f\"{OUTFOLDER}/subregion_noise_gt.csv\", header=None).values\n",
        "\n",
        "# Define constants from data generation\n",
        "BASELINE_VOTE = 0.50\n",
        "RICH_VOTE = 0.80\n",
        "POOR_VOTE = 0.40\n",
        "INTER_VOTE = 0.50\n",
        "\n",
        "# Compute control outcome (no intervention) for each sub-cell\n",
        "control_outcome = np.clip(BASELINE_VOTE + subregion_noise, 0, 1) * 100.0\n",
        "\n",
        "# Compute treated outcome (with intervention) for each sub-cell\n",
        "treated_outcome = np.zeros_like(subregion_noise)\n",
        "for i in range(wealth_hi.shape[0]):\n",
        "    for j in range(wealth_hi.shape[1]):\n",
        "        w = wealth_hi[i, j]\n",
        "        base = RICH_VOTE if w == 3 else POOR_VOTE if w == 1 else INTER_VOTE\n",
        "        treated_outcome[i, j] = np.clip(base + subregion_noise[i, j], 0, 1) * 100.0\n",
        "\n",
        "# Compute causal effect: treated - control\n",
        "causal_effect = treated_outcome - control_outcome\n",
        "\n",
        "# Save to CSVs\n",
        "pd.DataFrame(np.round(control_outcome, 2)).to_csv(f\"{OUTFOLDER}/control_sub.csv\", header=False, index=False)\n",
        "pd.DataFrame(np.round(treated_outcome, 2)).to_csv(f\"{OUTFOLDER}/treated_sub.csv\", header=False, index=False)\n",
        "pd.DataFrame(np.round(causal_effect, 2)).to_csv(f\"{OUTFOLDER}/effect_sub.csv\", header=False, index=False)\n",
        "\n",
        "# Normalize for plotting (0-100 to 0-1 for outcomes, effect min-max to 0-1? but for consistency vmin vmax)\n",
        "control_norm = control_outcome #/ 100.0\n",
        "treated_norm = treated_outcome #/ 100.0\n",
        "effect_norm = causal_effect #/ 100.0  # since difference is -0.1 to 0.3 roughly\n",
        "\n",
        "# Color maps from seaborn deep palette\n",
        "colors = sns.color_palette(\"deep\")\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue for outcomes\n",
        "pink_cmap = sns.light_palette(colors[4], as_cmap=True)  # Pink for effect\n",
        "\n",
        "# Parameter to control space between subplots\n",
        "space_between = 0.2\n",
        "\n",
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Left: Control outcome in blue\n",
        "sns.heatmap(control_norm, ax=axs[0], cmap=blue_cmap, cbar=False, square=True, linewidths=0.0)#, vmin=0.0, vmax=1.0)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "\n",
        "# Center: Treated outcome in blue\n",
        "sns.heatmap(treated_norm, ax=axs[1], cmap=blue_cmap, cbar=False, square=True, linewidths=0.0)#, vmin=0.0, vmax=1.0)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "\n",
        "# Right: Causal effect in pink\n",
        "sns.heatmap(effect_norm, ax=axs[2], cmap=pink_cmap, cbar=False, square=True, linewidths=0.0)#, vmin=-0.2, vmax=0.4)  # Adjusted vmin/vmax for effect range\n",
        "axs[2].set_xticks([])\n",
        "axs[2].set_yticks([])\n",
        "\n",
        "\n",
        "# Adjust the space between subplots\n",
        "fig.subplots_adjust(wspace=space_between)\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(f'{OUTFOLDER}/sub_heatmaps.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(f'{OUTFOLDER}/sub_heatmaps.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t-IUC5TNmd3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Set output folder\n",
        "OUTFOLDER = \"data_exp5_max\"\n",
        "\n",
        "# Load the data\n",
        "interv_data = pd.read_csv(f\"{OUTFOLDER}/interventions.csv\", header=None).values.astype(int)\n",
        "outcome_data = pd.read_csv(f\"{OUTFOLDER}/outcome.csv\", header=None).values / 100.0\n",
        "context_data = pd.read_csv(f\"{OUTFOLDER}/wealth_high_res.csv\", header=None).values\n",
        "context_data_norm = (context_data - 1) / 2.0  # Normalize assuming values 1,2,3 to 0-1\n",
        "\n",
        "# Color maps from seaborn deep palette\n",
        "colors = sns.color_palette(\"deep\")\n",
        "red_cmap = sns.light_palette(colors[3], as_cmap=True)  # Red\n",
        "blue_cmap = sns.light_palette(colors[0], as_cmap=True)  # Blue\n",
        "green_cmap = sns.light_palette(colors[2], as_cmap=True)  # Green\n",
        "\n",
        "# Parameter to control space between subplots (as fraction of average axis width)\n",
        "space_between = 0.05  # Adjust this value as needed (e.g., 0.0 for no space, 0.5 for more space)\n",
        "\n",
        "# Create a single figure with three subplots side by side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Left: Intervention in red\n",
        "sns.heatmap(interv_data, ax=axs[0], cmap=red_cmap, cbar=False, square=True, linewidths=2, vmin=0.0, vmax=1.0)\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "\n",
        "sns.heatmap(outcome_data, ax=axs[1], cmap=blue_cmap, cbar=False, square=True, linewidths=2, vmin=0.4, vmax=0.6)\n",
        "axs[1].set_xticks([])\n",
        "axs[1].set_yticks([])\n",
        "\n",
        "sns.heatmap(context_data_norm, ax=axs[2], cmap=green_cmap, cbar=False, square=True, linewidths=0, vmin=0.0, vmax=1.0)\n",
        "axs[2].set_xticks([])\n",
        "axs[2].set_yticks([])\n",
        "\n",
        "# Adjust the space between subplots\n",
        "fig.subplots_adjust(wspace=space_between)\n",
        "\n",
        "# Save and show\n",
        "fig.savefig(f'{OUTFOLDER}/combined_heatmaps.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "fig.savefig(f'{OUTFOLDER}/combined_heatmaps.pdf', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NHGb-jHNnAwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_aggregate(values, temperature=1.0):\n",
        "    if isinstance(temperature, torch.Tensor):\n",
        "        if (temperature <= 0).any():\n",
        "            raise ValueError(\"Temperature must be positive\")\n",
        "    elif temperature <= 0:\n",
        "        raise ValueError(\"Temperature must be positive\")\n",
        "\n",
        "    logits = values / temperature\n",
        "    logits = logits - torch.amax(logits, dim=-1, keepdim=True)  # numerical stability\n",
        "    weights = torch.softmax(logits, dim=-1)  # (..., K)\n",
        "    return (values * weights).sum(dim=-1)     # (...,)"
      ],
      "metadata": {
        "id": "3qylc9GCs9W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "OUTFOLDER = \"data_exp5_max\"\n",
        "N_EPOCHS = 1000\n",
        "LR = 0.01\n",
        "SEED = 42\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "LOGLOG_FLAG = True  # Whether to plot losses in log-log scale\n",
        "\n",
        "# ------------------ Set seeds ------------------\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ------------------ Load data ------------------\n",
        "interv_np  = pd.read_csv(f\"{OUTFOLDER}/interventions.csv\",   header=None).values  # 0/1\n",
        "wealth_hi  = pd.read_csv(f\"{OUTFOLDER}/wealth_high_res.csv\", header=None).values  # 100100 wealth classes\n",
        "outcome_np = pd.read_csv(f\"{OUTFOLDER}/outcome.csv\",         header=None).values  # region outcomes %\n",
        "\n",
        "# Convert to tensor\n",
        "outcome_t = torch.tensor(outcome_np, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "# ------------------ Known Constants ------------------\n",
        "BASELINE_VOTE = 0.50\n",
        "RICH_VOTE     = 0.80\n",
        "INTER_VOTE    = 0.50\n",
        "POOR_VOTE     = 0.40\n",
        "\n",
        "# ------------------ Initialize Parameters ------------------\n",
        "params = torch.nn.Parameter(torch.full((6,), 0.5, dtype=torch.float32, device=DEVICE))  # 6 s\n",
        "log_tau = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float32, device=DEVICE))     # log temperature\n",
        "opt = torch.optim.Adam([params, log_tau], lr=LR)\n",
        "\n",
        "loss_hist = []\n",
        "ce_loss_hist = []\n",
        "temp_hist = []\n",
        "\n",
        "# ------------------ Training Loop ------------------\n",
        "for epoch in range(N_EPOCHS):\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # Compute predicted outcomes at subregion level\n",
        "    wealth_hi_t = torch.tensor(wealth_hi, dtype=torch.float32, device=DEVICE)\n",
        "    wealth_hi_reshaped = wealth_hi_t.reshape(10, 10, 10, 10).permute(0, 2, 1, 3).reshape(10, 10, 100)\n",
        "    treated_mask_t = torch.tensor(interv_np, dtype=torch.float32, device=DEVICE) == 1\n",
        "    treated_mask_expanded = treated_mask_t.unsqueeze(-1).expand(-1, -1, 100)\n",
        "\n",
        "    params_expanded = params.unsqueeze(0).unsqueeze(0).expand(10, 10, -1)\n",
        "\n",
        "    predicted_sub_outcomes = torch.where(\n",
        "        treated_mask_expanded,\n",
        "        torch.where(\n",
        "            wealth_hi_reshaped == 3, params_expanded[:, :, 5].unsqueeze(-1),\n",
        "            torch.where(wealth_hi_reshaped == 1, params_expanded[:, :, 3].unsqueeze(-1), params_expanded[:, :, 4].unsqueeze(-1))\n",
        "        ),\n",
        "        torch.where(\n",
        "            wealth_hi_reshaped == 3, params_expanded[:, :, 2].unsqueeze(-1),\n",
        "            torch.where(wealth_hi_reshaped == 1, params_expanded[:, :, 0].unsqueeze(-1), params_expanded[:, :, 1].unsqueeze(-1))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Soft aggregation\n",
        "    temperature = torch.exp(log_tau)\n",
        "    mean_pred = soft_aggregate(predicted_sub_outcomes, temperature=temperature) * 100  # %\n",
        "\n",
        "    # Compute region-level MSE loss\n",
        "    loss = torch.mean((mean_pred - outcome_t) ** 2)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Log losses and temperature\n",
        "    loss_hist.append(loss.item())\n",
        "    temp_hist.append(temperature.item())\n",
        "\n",
        "    # Compute causal effect MSE loss (against true values)\n",
        "    with torch.no_grad():\n",
        "        est_causal_effect = (params[3:] - params[:3])  # estimated 1 - 0\n",
        "        true_causal_effect = torch.tensor([\n",
        "            (POOR_VOTE - BASELINE_VOTE),\n",
        "            (INTER_VOTE - BASELINE_VOTE),\n",
        "            (RICH_VOTE  - BASELINE_VOTE)\n",
        "        ], device=DEVICE)\n",
        "\n",
        "        ce_mse = torch.mean((est_causal_effect - true_causal_effect) ** 2).item()\n",
        "        ce_loss_hist.append(ce_mse)\n",
        "\n",
        "# ------------------ Save Loss Logs ------------------\n",
        "loss_df = pd.DataFrame({\n",
        "    'region_loss': loss_hist,\n",
        "    'causal_effect_loss': ce_loss_hist,\n",
        "    'temperature': temp_hist\n",
        "})\n",
        "loss_df.to_csv(f\"{OUTFOLDER}/estimation_loss.csv\", index_label=\"epoch\")\n",
        "\n",
        "# ------------------ Plot Loss Curves ------------------\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "colors = sns.color_palette(\"deep\")\n",
        "epochs = np.arange(1, N_EPOCHS + 1)\n",
        "\n",
        "# Region Loss\n",
        "ax1.set_xlabel('Epoch', fontsize=16)\n",
        "ax1.set_ylabel('Training Loss', fontsize=16, color=colors[0])\n",
        "if LOGLOG_FLAG:\n",
        "    ax1.loglog(epochs, loss_hist, color=colors[0], lw=2.5)\n",
        "else:\n",
        "    ax1.plot(epochs, loss_hist, color=colors[0], lw=2)\n",
        "ax1.tick_params(axis='y', labelcolor=colors[0])\n",
        "\n",
        "# Causal Effect Loss\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Causal Effect MSE', fontsize=16, color=colors[3])\n",
        "if LOGLOG_FLAG:\n",
        "    ax2.loglog(epochs, ce_loss_hist, color=colors[3], ls='--', lw=2.5)\n",
        "else:\n",
        "    ax2.plot(epochs, ce_loss_hist, color=colors[3], ls='--', lw=2)\n",
        "ax2.tick_params(axis='y', labelcolor=colors[3])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(f\"{OUTFOLDER}/estimation_loss.jpg\", dpi=300)\n",
        "fig.savefig(f\"{OUTFOLDER}/estimation_loss.pdf\")\n",
        "plt.show()\n",
        "\n",
        "# ------------------ Save Parameters ------------------\n",
        "param_names = [\"mu_poor_0\", \"mu_inter_0\", \"mu_rich_0\",\n",
        "               \"mu_poor_1\", \"mu_inter_1\", \"mu_rich_1\", \"temperature\"]\n",
        "\n",
        "values = np.append(params.detach().cpu().numpy(), temperature.item())\n",
        "results_df = pd.DataFrame({'parameter': param_names, 'value': values})\n",
        "results_df.to_csv(f\"{OUTFOLDER}/results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "wJt238qr4HYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "data_dir = 'data_exp5_max'\n",
        "loglog_flag = True\n",
        "\n",
        "# Load loss history from CSV\n",
        "loss_data = pd.read_csv(os.path.join(data_dir, 'estimation_loss.csv'))\n",
        "loss_hist = loss_data['region_loss'].values\n",
        "ce_loss_hist = loss_data['causal_effect_loss'].values\n",
        "epochs = np.arange(1, len(loss_hist) + 1)\n",
        "\n",
        "# Plotting\n",
        "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "colors = sns.color_palette(\"deep\")\n",
        "\n",
        "ax1.set_xlabel('Epoch', fontsize=20)\n",
        "ax1.set_ylabel('Training Loss', fontsize=20, color=colors[0])\n",
        "if loglog_flag:\n",
        "    ax1.loglog(epochs, loss_hist, color=colors[0], alpha=0.9, lw=3)\n",
        "else:\n",
        "    ax1.plot(epochs, loss_hist, color=colors[0], alpha=0.7, lw=2)\n",
        "ax1.tick_params(axis='both', which='both', length=0, labelsize=18, colors=colors[0])\n",
        "ax1.set_title('MSE Curves', fontsize=22)\n",
        "ax1.spines['top'].set_visible(False)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('MSE Causal Effect', fontsize=20, color=colors[3])\n",
        "if loglog_flag:\n",
        "    ax2.loglog(epochs, ce_loss_hist, color=colors[3], alpha=0.9, ls='--', lw=5)\n",
        "else:\n",
        "    ax2.plot(epochs, ce_loss_hist, color=colors[3], alpha=0.7, ls='--', lw=2)\n",
        "ax2.tick_params(axis='y', which='both', length=0, labelsize=18, colors=colors[3])\n",
        "ax2.spines['top'].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(os.path.join(data_dir, 'estimation_loss.jpg'), dpi=300)\n",
        "fig.savefig(os.path.join(data_dir, 'estimation_loss.pdf'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JRHD5zYLtMgv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}